<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta http-equiv="Content-Style-Type" content="text/css" />
  <meta name="generator" content="pandoc" />
  <meta name="author" content="Ryan Reece" />
  <meta name="date" content="Mon May 25, 2020" />
  <title>Philosophy of statistics - Ryan&#8217;s Outline of Philosophy</title>
  <meta name="description" content="Ryan Reece&#8217;s outline of philosophy" />
  <link rel="stylesheet" type="text/css" href="templates/markdown-memo-alt-blue.css"/>
  <link rel="icon" type="image/png" href="img/markdown-favicon-196x196.png" />

<!--- Google Analytics Tracking stuff  -->
<script type="text/javascript">
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-55199032-1', 'auto');
  ga('send', 'pageview');
</script>

<!--- MathJax stuff -->
<script type="text/javascript" src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
</script>

<!--- toggle_more function  -->
<script type="text/javascript">
    //<![CDATA[
    function toggle_more(objID) {
        if (!document.getElementById) return;
        var ob = document.getElementById(objID).style;
        ob.display = (ob.display == 'block')?'none':'block';
        var ob2 = document.getElementById('link:'+objID);
        ob2.className = (ob2.className == 'open') ? 'closed' : 'open';
    }
    //]]>
</script>

</head>

<body>

<div id="site_header">
    <a href="./">Ryan&#8217;s Outline of Philosophy</a>
</div>

<!--
<div id="beforebody">
</div>
-->

<div class="nav">
<ul>
    <li> <a href="./">&#8634;&nbsp;Contents</a> </li>
    <li> <a href="#site_footer">&#8615;&nbsp;Bottom</a> </li>
    <li> <a href="./scientific-method.html">&#8612;&nbsp;Previous</a> </li>
    <li> <a href="./scientific-realism.html">&#8614;&nbsp;Next</a> </li>
</ul>
</div>

<div id="mainbody">
<div id="pagecontainer">

<h1 id="philosophy-of-statistics">Philosophy of statistics</h1>
<p>Statistics are <em>way</em> important in addressing the problem of induction.</p>
<h3 id="contents" class="unnumbered">Contents</h3>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#problem-of-induction">Problem of induction</a></li>
<li><a href="statistics.html#probability-and-its-related-concepts">Probability and its related concepts</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#probability">Probability</a></li>
<li><a href="statistics.html#expectation-and-variance">Expectation and variance</a></li>
<li><a href="statistics.html#uncertainty">Uncertainty</a></li>
<li><a href="statistics.html#bayes-theorem">Bayes&#8217; theorem</a></li>
<li><a href="statistics.html#likelihood-and-frequentist-vs-bayesian-probability">Likelihood and frequentist vs bayesian probability</a></li>
</ol></li>
<li><a href="statistics.html#foundations-of-statistics">Foundations of statistics</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#early-investigators">Early investigators</a></li>
<li><a href="statistics.html#foundations-of-modern-statistics">Foundations of modern statistics</a></li>
<li><a href="statistics.html#pedagogy">Pedagogy</a></li>
</ol></li>
<li><a href="statistics.html#statistical-models">Statistical models</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#parametric-models">Parametric models</a></li>
<li><a href="statistics.html#canonical-distributions">Canonical distributions</a></li>
<li><a href="statistics.html#mixture-models">Mixture models</a></li>
</ol></li>
<li><a href="statistics.html#point-estimation-and-confidence-intervals">Point estimation and confidence intervals</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#inverse-problems">Inverse problems</a></li>
<li><a href="statistics.html#bias-and-variance">Bias and variance</a></li>
<li><a href="statistics.html#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li><a href="statistics.html#variance-of-mles">Variance of MLEs</a></li>
<li><a href="statistics.html#bayesian-credibility-intervals">Bayesian credibility intervals</a></li>
</ol></li>
<li><a href="statistics.html#statistical-hypothesis-testing">Statistical hypothesis testing</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#null-hypothesis-significance-testing">Null hypothesis significance testing</a></li>
<li><a href="statistics.html#neyman-pearson-theory">Neyman-Pearson theory</a></li>
<li><a href="statistics.html#p-values-and-significance"><span class="math inline">(p)</span>-values and significance</a></li>
<li><a href="statistics.html#students-t-test">Student&#8217;s <span class="math inline">(t)</span>-test</a></li>
<li><a href="statistics.html#cls-method">CLs method</a></li>
<li><a href="statistics.html#asymptotics">Asymptotics</a></li>
<li><a href="statistics.html#frequentist-vs-bayesian-decision-theory">Frequentist vs bayesian decision theory</a></li>
<li><a href="statistics.html#examples">Examples</a></li>
</ol></li>
<li><a href="statistics.html#systematic-uncertainties">Systematic uncertainties</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#examples-of-poor-estimates-of-systematic-uncertanties">Examples of poor estimates of systematic uncertanties</a></li>
</ol></li>
<li><a href="statistics.html#exploratory-data-analysis">Exploratory data analysis</a></li>
<li><a href="statistics.html#likelihood-principle">Likelihood principle</a></li>
<li><a href="statistics.html#statistics-wars">&#8220;Statistics Wars&#8221;</a></li>
<li><a href="statistics.html#replication-crisis">Replication crisis</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#introduction">Introduction</a></li>
<li><a href="statistics.html#p-value-controversy">P-value controversy</a></li>
</ol></li>
<li><a href="statistics.html#information-geometry">Information geometry</a></li>
<li><a href="statistics.html#machine-learning">Machine learning</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#classical">Classical</a></li>
<li><a href="statistics.html#deep-learning">Deep learning</a></li>
<li><a href="statistics.html#computer-vision">Computer Vision</a></li>
<li><a href="statistics.html#natural-language-processing">Natural language processing</a></li>
<li><a href="statistics.html#reinforcement-learning">Reinforcement learning</a></li>
</ol></li>
<li><a href="statistics.html#theoretical-machine-learning">Theoretical machine learning</a></li>
<li><a href="statistics.html#automation">Automation</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#surrogate-models">Surrogate models</a></li>
<li><a href="statistics.html#automl">AutoML</a></li>
<li><a href="statistics.html#autoscience">AutoScience</a></li>
</ol></li>
<li><a href="statistics.html#implications-for-the-realism-debate">Implications for the realism debate</a></li>
<li><a href="statistics.html#my-thoughts">My thoughts</a></li>
<li><a href="statistics.html#annotated-bibliography">Annotated bibliography</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#mayo-d.g.-1996.-error-and-the-growth-of-experimental-knowledge.">Mayo, D.G. (1996). <em>Error and the Growth of Experimental Knowledge</em>.</a></li>
<li><a href="statistics.html#cowan-g.-1998.-statistical-data-analysis.">Cowan, G. (1998). <em>Statistical Data Analysis</em>.</a></li>
<li><a href="statistics.html#james-f.-2006.-statistical-methods-in-experimental-physics.">James, F. (2006). <em>Statistical Methods in Experimental Physics</em>.</a></li>
<li><a href="statistics.html#cowan-g.-et-al.-2011.-asymptotic-formulae-for-likelihood-based-tests-of-new-physics.">Cowan, G. <em>et al.</em> (2011). Asymptotic formulae for likelihood-based tests of new physics.</a></li>
<li><a href="statistics.html#atlas-collaboration.-2012.-combined-search-for-the-standard-model-higgs-boson.">ATLAS Collaboration. (2012). Combined search for the Standard Model Higgs boson.</a></li>
<li><a href="statistics.html#cranmer-k-2015.-practical-statistics-for-the-lhc.">Cranmer, K (2015). Practical statistics for the LHC.</a></li>
<li><a href="statistics.html#more-articles-to-do">More articles to do</a></li>
</ol></li>
<li><a href="statistics.html#links-and-encyclopedia-articles">Links and encyclopedia articles</a>
<ol style="list-style-type: decimal">
<li><a href="statistics.html#sep">SEP</a></li>
<li><a href="statistics.html#iep">IEP</a></li>
<li><a href="statistics.html#scholarpedia">Scholarpedia</a></li>
<li><a href="statistics.html#wikipedia">Wikipedia</a></li>
<li><a href="statistics.html#others">Others</a></li>
</ol></li>
<li><a href="statistics.html#references">References</a></li>
</ol>
<h2 id="problem-of-induction">Problem of induction</h2>
<p>See the <a href="scientific-method.html#induction">Outline of the scientific method</a></p>
<h2 id="probability-and-its-related-concepts">Probability and its related concepts</h2>
<h3 id="probability">Probability</h3>
<p>Probability is of epistemic interest, being in some sense a measure of inductive confidence.</p>
<ul>
<li>Kolmogorov</li>
<li>Carnap: &#8220;Probability as a guide in life&#8221;<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a></li>
</ul>
<h3 id="expectation-and-variance">Expectation and variance</h3>
<p>Expectation:</p>
<p><span class="math display">\[ \mathrm{E}(y) \equiv \int dx \: P(x) \: y(x) \label{eq:expectation} \]</span></p>
<p>Expectation values can be approximated with a partial sum over some data or monte carol sample:</p>
<p><span class="math display">\[ \mathrm{E}(y) \approx \frac{1}{n} \sum_s^n y(x_s) \label{eq:expectation_sum} \]</span></p>
<p>The variance of a random variable, <span class="math inline">\(y\)</span>, is defined as</p>
<span class="math display">\[\begin{align}
    \mathrm{Var}(y) &amp;\equiv \mathrm{E}((y - \mathrm{E}(y))^2) \nonumber \\
    &amp;= \mathrm{E}(y^2 - 2 \: y \: \mathrm{E}(y) + \mathrm{E}(y)^2) \nonumber \\
    &amp;= \mathrm{E}(y^2) - 2 \: \mathrm{E}(y) \: \mathrm{E}(y) + \mathrm{E}(y)^2 \nonumber \\
    &amp;= \mathrm{E}(y^2) - \mathrm{E}(y)^2 \label{eq:variance}
\end{align}\]</span>
<p>The covariance matrix, <span class="math inline">\(\boldsymbol{V}\)</span>, of random variables <span class="math inline">\(x_i\)</span> is</p>
<span class="math display">\[\begin{align}
    V_{ij} &amp;= \mathrm{Cov}(x_i, x_j) \equiv \mathrm{E}[(x_i - \mathrm{E}(x_i)) \: (x_j - \mathrm{E}(x_j))] \nonumber \\
           &amp;= \mathrm{E}(x_i \: x_{j} - \mu_i \: x_j - x_i \: \mu_j + \mu_i \: \mu_j ) \nonumber \\
           &amp;= \mathrm{E}(x_i \: x_{j}) - \mu_i \: \mu_j \label{eq:covariance_matrix_indexed}
\end{align}\]</span>
<span class="math display">\[\begin{equation}
\boldsymbol{V} = 
\begin{pmatrix}
    \mathrm{Var}(x_1) &amp; \mathrm{Cov}(x_1, x_2) &amp; \cdots &amp; \mathrm{Cov}(x_1, x_n) \\
    \mathrm{Cov}(x_2, x_1) &amp; \mathrm{Var}(x_2) &amp; \cdots &amp; \mathrm{Cov}(x_2, x_n) \\
    \vdots &amp; \vdots &amp; \ddots &amp;  \vdots \\
    \mathrm{Cov}(x_n, x_1) &amp; \mathrm{Cov}(x_n, x_2) &amp; \cdots &amp; \mathrm{Var}(x_n)
\end{pmatrix}
\label{eq:covariance_matrix_array}
\end{equation}\]</span>
<p>Diagonal elements of the covariance matrix are the variances of each variable.</p>
<p><span class="math display">\[ \mathrm{Cov}(x_i, x_i) = \mathrm{Var}(x_i) \]</span></p>
<p>Off-diagonal elements of a covariance matrix measure how related two variables are, linearly. Covariance can be normalized to give the correlation coefficient between variables:</p>
<p><span class="math display">\[ \mathrm{Cor}(x_i, x_j) \equiv \frac{ \mathrm{Cov}(x_i, x_j) }{ \sqrt{ \mathrm{Var}(x_i) \: \mathrm{Var}(x_j)  }  } \label{eq:correlation_matrix} \]</span></p>
<p>which is bounded: <span class="math inline">\(-1 \leq \mathrm{Cor}(x_i, x_j) \leq 1\)</span>.</p>
<p>The covariance of two random vectors is given by</p>
<p><span class="math display">\[ \boldsymbol{V} = \mathrm{Cov}(\vec{x}, \vec{y}) = \mathrm{E}(\vec{x} \: \vec{y}^{\top}) - \vec{\mu}_x \: \vec{\mu}_{y}^{\top} \label{eq:covariance_matrix_vectors} \]</span></p>
<h3 id="uncertainty">Uncertainty</h3>
<ul>
<li>Propagation of error</li>
<li>Quantiles</li>
<li>Practice of standard error for uncertainty quantification.</li>
</ul>
<h3 id="bayes-theorem">Bayes&#8217; theorem</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes, Thomas</a> (1701-1761)</li>
<li>Bayes&#8217; theorem</li>
</ul>
<p><span class="math display">\[ P(A|B) = P(B|A) \: P(A) \: / \: P(B) \label{eq:bayes_theorem} \]</span></p>
<ul>
<li>Extended version of Bayes theorem</li>
<li>Example of conditioning with medical diagnostics</li>
</ul>
<h3 id="likelihood-and-frequentist-vs-bayesian-probability">Likelihood and frequentist vs bayesian probability</h3>
<ul>
<li>frequentist vs bayesian probability</li>
<li>Weisberg: <a href="https://jonathanweisberg.org/vip/two-schools.html">&#8220;Two Schools&#8221;</a><a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a></li>
</ul>
<p><span class="math display">\[ P(H|D) = P(D|H) \: P(H) \: / \: P(D) \label{eq:bayes_theorem_hd} \]</span></p>
<ul>
<li>Likelihood</li>
</ul>
<p><span class="math display">\[ L(\theta) = P(D|\theta) \label{eq:likelihood_def_x} \]</span></p>
<ul>
<li>Cranmer</li>
</ul>
<blockquote>
<p>Bayes&#8217;s theorem is a theorem, so there&#8217;s no debating it. It is not the case that Frequentists dispute whether Bayes&#8217;s theorem is true. The debate is whether the necessary probabilities exist in the first place. If one can define the joint probability <span class="math inline">\(P (A, B)\)</span> in a frequentist way, then a Frequentist is perfectly happy using Bayes theorem. Thus, the debate starts at the very definition of probability.<a href="#fn3" class="footnoteRef" id="fnref3"><sup>3</sup></a></p>
</blockquote>
<ul>
<li>Carnap
<ul>
<li>&#8220;The two concepts of probability&#8221;<a href="#fn4" class="footnoteRef" id="fnref4"><sup>4</sup></a></li>
</ul></li>
<li>Royall
<ul>
<li>&#8220;What do these data say?&#8221;<a href="#fn5" class="footnoteRef" id="fnref5"><sup>5</sup></a></li>
</ul></li>
<li>We will return to the frequentist vs bayesian debate in the section on the <a href="#statistics-wars">&#8220;Statistics Wars&#8221;</a>.</li>
</ul>
<h2 id="foundations-of-statistics">Foundations of statistics</h2>
<h3 id="early-investigators">Early investigators</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/John_Graunt">Graunt, John</a> (1620-1674)</li>
<li><a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Bernoulli, Jacob</a> (1655-1705)
<ul>
<li><em>Ars Conjectandi</em> (1713, posthumous)</li>
<li>First modern phrasing of the problem of parameter estimation<a href="#fn6" class="footnoteRef" id="fnref6"><sup>6</sup></a></li>
<li>See Hacking<a href="#fn7" class="footnoteRef" id="fnref7"><sup>7</sup></a></li>
<li>Early vision of decision theory:</li>
</ul></li>
</ul>
<blockquote>
<p>The art of measuring, as precisely as possible, probabilities of things, with the goal that we would be able always to choose or follow in our judgments and actions that course, which will have been determined to be better, more satisfactory, safer or more advantageous.<a href="#fn8" class="footnoteRef" id="fnref8"><sup>8</sup></a></p>
</blockquote>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes, Thomas</a> (1701-1761)</li>
<li><a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Laplace, Pierre-Simon</a> (1749-1827)
<ul>
<li>rule of succssion, bayesian</li>
</ul></li>
<li><a href="http://en.wikipedia.org/wiki/Gauss">Gauss, Carl Friedrich</a> (1777-1855)</li>
<li><a href="https://en.wikipedia.org/wiki/John_Stuart_Mill">Mill, John Stuart</a> (1806-1873)</li>
<li><a href="https://en.wikipedia.org/wiki/John_Venn">Venn, John</a> (1834-1923)
<ul>
<li><em>The Logic of Chance</em> (1866)<a href="#fn9" class="footnoteRef" id="fnref9"><sup>9</sup></a></li>
</ul></li>
</ul>
<h3 id="foundations-of-modern-statistics">Foundations of modern statistics</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Charles_Sanders_Peirce">Peirce, Charles Sanders</a> (1839-1914)
<ul>
<li>Formulated modern statistics in &#8220;Illustrations of the Logic of Science&#8221;, a series published in <em>Popular Science Monthly</em> (1877-1878), and also &#8220;A Theory of Probable Inference&#8221; in <em>Studies in Logic</em> (1883).<a href="#fn10" class="footnoteRef" id="fnref10"><sup>10</sup></a></li>
<li>With a repeated measures design, introduced blinded, controlled randomized experiments (before Fisher).</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Pearson, Karl</a> (1857-1936)</li>
<li><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Fisher, Ronald</a> (1890-1972)
<ul>
<li>Fisher significance of the null hypothesis (<span class="math inline">\(p\)</span>-values)
<ul>
<li>On an absolute criterion for fitting frequency curves.<a href="#fn11" class="footnoteRef" id="fnref11"><sup>11</sup></a></li>
<li>Frequency distribution of the values of the correlation coefficient in samples of indefinitely large population.<a href="#fn12" class="footnoteRef" id="fnref12"><sup>12</sup></a></li>
<li>On the &#8220;probable error&#8221; of a coefficient of correlation deduced from a small sample<a href="#fn13" class="footnoteRef" id="fnref13"><sup>13</sup></a> - definition of <em>likelihood</em></li>
<li><em>The Lady Tasting Tea</em><a href="#fn14" class="footnoteRef" id="fnref14"><sup>14</sup></a></li>
</ul></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Neyman, Jerzy</a> (1894-1981)
<ul>
<li>biography by Reid<a href="#fn15" class="footnoteRef" id="fnref15"><sup>15</sup></a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Egon_Pearson">Pearson, Egon</a> (1895-1980)
<ul>
<li>Neyman-Pearson confidence intervals with fixed error probabilities (also <span class="math inline">\(p\)</span>-values but considering two hypotheses involves two types of errors)</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Jeffreys, Harold</a> (1891-1989)
<ul>
<li>objective (non-informative) Jeffreys priors</li>
</ul></li>
</ul>
<h3 id="pedagogy">Pedagogy</h3>
<ul>
<li>Kendall<a href="#fn16" class="footnoteRef" id="fnref16"><sup>16</sup></a></li>
<li>James<a href="#fn17" class="footnoteRef" id="fnref17"><sup>17</sup></a></li>
<li>Cousins<a href="#fn18" class="footnoteRef" id="fnref18"><sup>18</sup></a></li>
<li>Cowan<a href="#fn19" class="footnoteRef" id="fnref19"><sup>19</sup></a></li>
<li>Lista<a href="#fn20" class="footnoteRef" id="fnref20"><sup>20</sup></a></li>
<li>Cranmer<a href="#fn21" class="footnoteRef" id="fnref21"><sup>21</sup></a></li>
<li>Weisberg<a href="#fn22" class="footnoteRef" id="fnref22"><sup>22</sup></a></li>
</ul>
<h2 id="statistical-models">Statistical models</h2>
<h3 id="parametric-models">Parametric models</h3>
<ul>
<li>Data: <span class="math inline">\(x_i\)</span></li>
<li>Parameters: <span class="math inline">\(\theta_j\)</span></li>
<li>Model: <span class="math inline">\(f(\vec{x} ; \vec{\theta})\)</span></li>
</ul>
<h3 id="canonical-distributions">Canonical distributions</h3>
<ul>
<li>Bernoulli distribution</li>
<li>Binomial distribution</li>
<li>Poisson distribution</li>
<li>Normal/Gaussian distribution</li>
</ul>
<p><span class="math display">\[ N(x \,|\, \mu, \sigma^2) = \frac{1}{\sqrt{2\,\pi\:\sigma^2}} \: \exp\left[\frac{-(x-\mu)^2}{2\,\sigma^2}\right] \label{eq:gaussian} \]</span></p>
<p>and in <span class="math inline">\(k\)</span> dimensions:</p>
<p><span class="math display">\[ N(\vec{x} \,|\, \vec{\mu}, \boldsymbol{\Sigma}) = (2 \pi)^{-k/2}\:\left|\boldsymbol{\Sigma}\right|^{-1/2} \: \exp\left[\frac{-1}{2}\:(\vec{x}-\vec{\mu})^{\top}\:\boldsymbol{\Sigma}^{-1}\:(\vec{x}-\vec{\mu})\right] \label{eq:gaussian_k_dim} \]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix (defined in eq.&#160;<span class="math inline">\(\eqref{eq:covariance_matrix_indexed}\)</span>) of the distribution.</p>
<ul>
<li>Central limit theorem</li>
<li><span class="math inline">\(\chi^2\)</span> distribution</li>
<li>Univariate distribution relationships<a href="#fn23" class="footnoteRef" id="fnref23"><sup>23</sup></a></li>
<li>TODO: add fig</li>
</ul>
<h3 id="mixture-models">Mixture models</h3>
<ul>
<li>Gaussian mixture models (GMM)</li>
<li>Marked poisson
<ul>
<li><a href="https://scikit-hep.org/pyhf/intro.html">pyhf model description</a></li>
<li>HistFactory<a href="#fn24" class="footnoteRef" id="fnref24"><sup>24</sup></a></li>
</ul></li>
</ul>
<h2 id="point-estimation-and-confidence-intervals">Point estimation and confidence intervals</h2>
<h3 id="inverse-problems">Inverse problems</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Inverse_problem">Inverse problem</a></li>
<li>estimators</li>
<li>regression</li>
<li>Accuracy vs precision<a href="#fn25" class="footnoteRef" id="fnref25"><sup>25</sup></a></li>
</ul>
<h3 id="bias-and-variance">Bias and variance</h3>
<p>The bias of an estimator, <span class="math inline">\(\hat\theta\)</span>, is defined as</p>
<p><span class="math display">\[ \mathrm{Bias}(\hat{\theta}) \equiv \mathrm{E}(\hat{\theta} - \theta) = \int dx \: P(x|\theta) \: (\hat{\theta} - \theta) \label{eq:bias} \]</span></p>
<p>The mean squared error (MSE) of an estimator has a similar formula to variance (eq.&#160;<span class="math inline">\(\eqref{eq:variance}\)</span>) except that instead of quantifying the square of the difference of the estimator and its expected value, the MSE uses the square of the difference of the estimator and the true parameter:</p>
<p><span class="math display">\[ \mathrm{MSE}(\hat{\theta}) \equiv \mathrm{E}((\hat{\theta} - \theta)^2) \label{eq:mse} \]</span></p>
<p>The MSE of an estimator can be related to its bias and its variance by the following proof:</p>
<span class="math display">\[\begin{align}
    \mathrm{MSE}(\hat{\theta}) &amp;= \mathrm{E}(\hat{\theta}^2 - 2 \: \hat{\theta} \: \theta + \theta^2) \nonumber \\
    &amp;= \mathrm{E}(\hat{\theta}^2) - 2 \: \mathrm{E}(\hat{\theta}) \: \theta + \theta^2
\end{align}\]</span>
<p>noting that</p>
<p><span class="math display">\[ \mathrm{Var}(\hat{\theta}) = \mathrm{E}(\hat{\theta}^2) - \mathrm{E}(\hat{\theta})^2 \]</span></p>
<p>and</p>
<span class="math display">\[\begin{align}
    \mathrm{Bias}(\hat{\theta})^2 &amp;= \mathrm{E}(\hat{\theta} - \theta)^2 \nonumber \\
    &amp;= \mathrm{E}(\hat{\theta})^2 - 2 \: \mathrm{E}(\hat{\theta}) \: \theta + \theta^2
\end{align}\]</span>
<p>we see that MSE is equivalent to</p>
<p><span class="math display">\[ \mathrm{MSE}(\hat{\theta}) = \mathrm{Var}(\hat{\theta}) + \mathrm{Bias}(\hat{\theta})^2 \label{eq:mse_variance_bias} \]</span></p>
<p>For an unbiased estimator, the MSE is the variance of the estimator.</p>
<p>TODO:</p>
<ul>
<li>Note the new deep learning view. See <a href="#deep-learning">Deep learning</a>.</li>
</ul>
<h3 id="maximum-likelihood-estimation">Maximum likelihood estimation</h3>
<p>A maximum likelihood estimator (MLE) was first used by Fisher<a href="#fn26" class="footnoteRef" id="fnref26"><sup>26</sup></a></p>
<p><span class="math display">\[\hat{\theta} \equiv \underset{\theta}{\mathrm{argmax}} \: \mathrm{log} \: L(\theta) \label{eq:mle} \]</span></p>
<p>Maximizing <span class="math inline">\(\mathrm{log} \: L(\theta)\)</span> is equivalent to maximizing <span class="math inline">\(L(\theta)\)</span>, and the former is more convenient because for data that are independent and identically distributed (<em>i.i.d.</em>) the joint likelihood can be factored into a product of individual measurements:</p>
<p><span class="math display">\[ L(\theta) = \prod_i L(\theta|x_i) = \prod_i P(x_i|\theta) \]</span></p>
<p>and taking the log of the product makes it a sum:</p>
<p><span class="math display">\[ \mathrm{log} \: L(\theta) = \sum_i \mathrm{log} \: L(\theta|x_i) = \sum_i \mathrm{log} \: P(x_i|\theta) \]</span></p>
<p>Maximizing <span class="math inline">\(\mathrm{log} \: L(\theta)\)</span> is also equivalent to minimizing <span class="math inline">\(-\mathrm{log} \: L(\theta)\)</span>, the negative-log-likelihoo (NLL). For distributions that are <em>i.i.d.</em>,</p>
<p><span class="math display">\[ L = \prod_i L_i \]</span></p>
<p><span class="math display">\[ - \log L = - \sum_i \log L_i \]</span></p>
<p><span class="math display">\[ \Rightarrow \mathrm{NLL} = \sum_i \mathrm{NLL}_i \]</span></p>
<p>TODO:</p>
<ul>
<li>Least squares from MLE of gaussian models: <span class="math inline">\(\chi^2\)</span></li>
<li>Ordinary Least Squares (OLS)</li>
<li>interpretation</li>
</ul>
<h3 id="variance-of-mles">Variance of MLEs</h3>
<ul>
<li>Taylor expansion of a likelihood near its maximum</li>
<li>Analytic variance of gaussian likelihoods: <span class="math inline">\(\chi^2\)</span></li>
<li>Cram&#233;r-Rao bound<a href="#fn27" class="footnoteRef" id="fnref27"><sup>27</sup></a>
<ul>
<li>for unbiased and efficient estimators</li>
<li>proof in Rice<a href="#fn28" class="footnoteRef" id="fnref28"><sup>28</sup></a></li>
</ul></li>
<li>Variance of MLEs by the method of <span class="math inline">\(\Delta\chi^2\)</span> or <span class="math inline">\(\Delta{}L\)</span>
<ul>
<li>Invariance of likelihoods to reparametrization</li>
</ul></li>
<li>Uncertainty of measuring an efficiency
<ul>
<li>Precision vs recall for classification</li>
<li>Classification and logistic regression</li>
</ul></li>
<li>TODO:
<ul>
<li>Explain if/why the Neyman construction would be needed for point estimation.</li>
</ul></li>
</ul>
<h3 id="bayesian-credibility-intervals">Bayesian credibility intervals</h3>
<ul>
<li>defined, MAP</li>
<li>prior sensitivity
<ul>
<li>Betancourt, M. (2018). <a href="https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb">Towards a principled Bayesian workflow</a> - ipynb</li>
</ul></li>
<li>not invariant to reparametrization in general</li>
<li>Jeffreys priors are</li>
<li>Examples:
<ul>
<li>Some sample mean</li>
<li>Measuring efficiency</li>
<li>Bayesian lighthouse</li>
<li>Some HEP fit</li>
</ul></li>
</ul>
<h2 id="statistical-hypothesis-testing">Statistical hypothesis testing</h2>
<h3 id="null-hypothesis-significance-testing">Null hypothesis significance testing</h3>
<ul>
<li>Null hypothesis significance testing (NHST)</li>
<li>goodness of fit</li>
<li>Fisher</li>
</ul>
<p>Fisher:</p>
<blockquote>
<p>[T]he null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation.<a href="#fn29" class="footnoteRef" id="fnref29"><sup>29</sup></a></p>
</blockquote>
<h3 id="neyman-pearson-theory">Neyman-Pearson theory</h3>
<ul>
<li>probes an alternative hypothesis<a href="#fn30" class="footnoteRef" id="fnref30"><sup>30</sup></a></li>
<li>Type-1 and type-2 errors</li>
<li>Power and confidence</li>
<li>Neyman-Pearson lemma<a href="#fn31" class="footnoteRef" id="fnref31"><sup>31</sup></a></li>
<li>Neyman construction</li>
</ul>
<p>Neyman-Pearson test statistic:</p>
<p><span class="math display">\[ q_\mathrm{NP} = - 2 \ln \frac{L(H_0)}{L(H_1)} \label{eq:qnp-test-stat} \]</span></p>
<p>Background-only Neyman-Pearson test statistic:</p>
<p><span class="math display">\[ q_\mathrm{0} = - 2 \ln \frac{L(b)}{L(\mu\,s + b)} \label{eq:q0-test-stat} \]</span></p>
<div class="figure">
<img src="img/ROC-explainer.png" alt="Figure 1: TODO: ROC explainer. (Wikimedia, 2015)." id="fig:ROC-explainer" />
<p class="caption">Figure 1: TODO: ROC explainer. (<a href="https://commons.wikimedia.org/wiki/File:ROC_curves.svg">Wikimedia</a>, 2015).</p>
</div>
<h3 id="p-values-and-significance"><span class="math inline">\(p\)</span>-values and significance</h3>
<ul>
<li><span class="math inline">\(p\)</span>-values and significance<a href="#fn32" class="footnoteRef" id="fnref32"><sup>32</sup></a></li>
<li>Coverage</li>
<li>Fisherian vs Neyman-Pearson <span class="math inline">\(p\)</span>-values</li>
<li>Flip-flopping and Feldman-Cousins confidence intervals<a href="#fn33" class="footnoteRef" id="fnref33"><sup>33</sup></a></li>
</ul>
<h3 id="students-t-test">Student&#8217;s <span class="math inline">\(t\)</span>-test</h3>
<ul>
<li>ANOVA</li>
</ul>
<h3 id="cls-method">CLs method</h3>
<ul>
<li>Conservative coverage; used in particle physics</li>
<li>Junk<a href="#fn34" class="footnoteRef" id="fnref34"><sup>34</sup></a></li>
<li>Read<a href="#fn35" class="footnoteRef" id="fnref35"><sup>35</sup></a></li>
<li>ATLAS<a href="#fn36" class="footnoteRef" id="fnref36"><sup>36</sup></a></li>
</ul>
<h3 id="asymptotics">Asymptotics</h3>
<ul>
<li>Wilks<a href="#fn37" class="footnoteRef" id="fnref37"><sup>37</sup></a></li>
<li>Pearson <span class="math inline">\(\chi^2\)</span>-test</li>
<li>Wald<a href="#fn38" class="footnoteRef" id="fnref38"><sup>38</sup></a></li>
<li>Cowan <em>et al</em>.<a href="#fn39" class="footnoteRef" id="fnref39"><sup>39</sup></a></li>
</ul>
<h3 id="frequentist-vs-bayesian-decision-theory">Frequentist vs bayesian decision theory</h3>
<ul>
<li>Frequentist vs bayesian decision theory<a href="#fn40" class="footnoteRef" id="fnref40"><sup>40</sup></a></li>
</ul>
<h3 id="examples">Examples</h3>
<ul>
<li>Difference of two means: <span class="math inline">\(t\)</span>-test</li>
<li>A/B-testing</li>
<li>New physics</li>
</ul>
<h2 id="systematic-uncertainties">Systematic uncertainties</h2>
<ul>
<li>Class-1, class-2, and class-3 systematic uncertanties (good, bad, ugly), Classification by Pekka Sinervo (PhyStat2003)<a href="#fn41" class="footnoteRef" id="fnref41"><sup>41</sup></a></li>
<li>Not to be confused with type-1 and type-2 errors in Neyman-Pearson theory</li>
<li>Profiling and the profile likelihood
<ul>
<li>Importance of Wald and Cowan <em>et al</em>.</li>
</ul></li>
</ul>
<div class="figure">
<img src="img/systematic-uncertainties-sinervo.png" alt="Figure 2: Classification of measurement uncertainties (philosophy-in-figures.tumblr.com, 2016)." id="fig:systematic-uncertainties-sinervo" />
<p class="caption">Figure 2: Classification of measurement uncertainties (<a href="http://philosophy-in-figures.tumblr.com/post/150371555016/classification-of-measurement-uncertainties">philosophy-in-figures.tumblr.com</a>, 2016).</p>
</div>
<h3 id="examples-of-poor-estimates-of-systematic-uncertanties">Examples of poor estimates of systematic uncertanties</h3>
<ul>
<li>CDF <span class="math inline">\(Wjj\)</span> bump
<ul>
<li>Phys.Rev.Lett.106:171801 (2011) / <a href="https://arxiv.org/abs/1104.0699">arxiv:1104.0699</a></li>
<li><a href="https://www-cdf.fnal.gov/physics/ewk/2011/wjj/7_3.html">Invariant mass distribution of jet pairs produced in association with a <span class="math inline">\(W\)</span> boson in <span class="math inline">\(p\bar{p}\)</span> collisions at <span class="math inline">\(\sqrt{s}\)</span> = 1.96 TeV</a></li>
<li>Dorigo, T. (2011). <a href="https://www.science20.com/quantum_diaries_survivor/blog/jet_energy_scale_explanation_cdf_signal-77886">The jet energy scale as an explanation of the CDF signal</a>.</li>
</ul></li>
</ul>
<div class="figure">
<img src="img/AnimatedDijet.gif" alt="Figure 3: Demonstration of sensitivity to the jet energy scale for an alleged excess in Wjj by Tommaso Dorigo (2011)." id="fig:AnimatedDijet" />
<p class="caption">Figure 3: Demonstration of sensitivity to the jet energy scale for an alleged excess in <span class="math inline">\(Wjj\)</span> by <a href="https://www.science20.com/quantum_diaries_survivor/blog/jet_energy_scale_explanation_cdf_signal-77886">Tommaso Dorigo (2011)</a>.</p>
</div>
<ul>
<li>Faster-than-light neutrinos.</li>
</ul>
<h2 id="exploratory-data-analysis">Exploratory data analysis</h2>
<ul>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory data analysis</a></li>
<li><em>Exploratory Data Analysis</em> (1977)<a href="#fn42" class="footnoteRef" id="fnref42"><sup>42</sup></a></li>
</ul></li>
<li>Look-Elsewhere Effect (LEE)
<ul>
<li>AKA File-drawer effect</li>
</ul></li>
<li>Stopping rules
<ul>
<li>validation dataset</li>
<li>statistical issues, violates the likelihood principle</li>
</ul></li>
<li>&#8220;Data science&#8221;
<ul>
<li>Data collection, quality, analysis, archival, and reinterpretation</li>
<li>RECAST</li>
</ul></li>
</ul>
<h2 id="likelihood-principle">Likelihood principle</h2>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Likelihood_principle">Likelihood principle</a></li>
<li>The likelihood principle is the proposition that, given a statistical model and a data sample, all the evidence relevant to model parameters is contained in the likelihood function.</li>
<li>The history of likelihood<a href="#fn43" class="footnoteRef" id="fnref43"><sup>43</sup></a>
<ul>
<li>Allan Birnbaum proved that the likelihood principle follows from two more primitive and seemingly reasonable principles, the <a href="https://en.wikipedia.org/wiki/Conditionality_principle">conditionality principle</a> and the <a href="https://en.wikipedia.org/wiki/Sufficient_statistic">sufficiency principle</a>.<a href="#fn44" class="footnoteRef" id="fnref44"><sup>44</sup></a></li>
<li>Hacking identified the &#8220;law of likelihood&#8221;.<a href="#fn45" class="footnoteRef" id="fnref45"><sup>45</sup></a></li>
</ul></li>
<li>Berger &amp; Wolpert. (1988). <em>The Likelihood Principle</em><a href="#fn46" class="footnoteRef" id="fnref46"><sup>46</sup></a></li>
</ul>
<p>O&#8217;Hagan:</p>
<blockquote>
<p>The first key argument in favour of the Bayesian approach can be called the axiomatic argument. We can formulate systems of axioms of good inference, and under some persuasive axiom systems it can be proved that Bayesian inference is a consequence of adopting any of these systems&#8230; If one adopts two principles known as ancillarity and sufficiency principles, then under some statement of these principles it follows that one must adopt another known as the likelihood principle. Bayesian inference conforms to the likelihood principle whereas classical inference does not. Classical procedures regularly violate the likelihood principle or one or more of the other axioms of good inference. There are no such arguments in favour of classical inference.<a href="#fn47" class="footnoteRef" id="fnref47"><sup>47</sup></a></p>
</blockquote>
<ul>
<li>Criticisms:
<ul>
<li>Evans<a href="#fn48" class="footnoteRef" id="fnref48"><sup>48</sup></a></li>
<li>Mayo<a href="#fn49" class="footnoteRef" id="fnref49"><sup>49</sup></a></li>
<li>Mayo: <a href="https://errorstatistics.com/2019/04/04/excursion-1-tour-ii-error-probing-tools-versus-logics-of-evidence-excerpt/">The Law of Likelihood and Error Statistics</a></li>
</ul></li>
<li>Gandenberger
<ul>
<li>&#8220;A new proof of the likelihood principle&#8221;<a href="#fn50" class="footnoteRef" id="fnref50"><sup>50</sup></a></li>
<li>Thesis: <a href="http://d-scholarship.pitt.edu/24634/"><em>Two Principles of Evidence and Their Implications for the Philosophy of Scientific Method</em></a> (2015)</li>
<li><a href="http://gandenberger.org/research/">gandenberger.org/research</a></li>
<li><a href="http://gandenberger.org/2014/04/28/do-frequentist-methods-violate-lp/">Do frequentist methods violate the likelihood principle?</a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Likelihoodist_statistics">Likelihoodist statistics</a></li>
</ul>
<p>Mayo:</p>
<blockquote>
<p>Likelihoods are vital to all statistical accounts, but they are often misunderstood because the data are fixed and the hypothesis varies. Likelihoods of hypotheses should not be confused with their probabilities. &#8230; [T]he same phenomenon may be perfectly predicted or explained by two rival theories; so both theories are equally likely on the data, even though they cannot both be true.<a href="#fn51" class="footnoteRef" id="fnref51"><sup>51</sup></a></p>
</blockquote>
<h2 id="statistics-wars">&#8220;Statistics Wars&#8221;</h2>
<ul>
<li>Carnap
<ul>
<li>Sznajder on the alleged evolution of Carnap&#8217;s views of inductive logic<a href="#fn52" class="footnoteRef" id="fnref52"><sup>52</sup></a></li>
</ul></li>
<li>David Cox</li>
<li>Ian Hacking
<ul>
<li><em>Logic of Statistical Inference</em><a href="#fn53" class="footnoteRef" id="fnref53"><sup>53</sup></a></li>
</ul></li>
<li>Richard Royall
<ul>
<li><em>Statistical Evidence: A likelihood paradigm</em><a href="#fn54" class="footnoteRef" id="fnref54"><sup>54</sup></a></li>
</ul></li>
<li>Jim Berger
<ul>
<li>&#8220;Could Fisher, Jeffreys, and Neyman have agreed on testing?&#8221;<a href="#fn55" class="footnoteRef" id="fnref55"><sup>55</sup></a></li>
</ul></li>
<li>Deborah Mayo
<ul>
<li>&#8220;In defense of the Neyman-Pearson theory of confidence intervals&#8221;<a href="#fn56" class="footnoteRef" id="fnref56"><sup>56</sup></a></li>
<li>Concept of &#8220;Learning from error&#8221; in <em>Error and the Growth of Experimental Knowledge</em><a href="#fn57" class="footnoteRef" id="fnref57"><sup>57</sup></a></li>
<li>&#8220;Error statistics&#8221;<a href="#fn58" class="footnoteRef" id="fnref58"><sup>58</sup></a></li>
<li><em>Statistical Inference as Severe Testing</em><a href="#fn59" class="footnoteRef" id="fnref59"><sup>59</sup></a></li>
<li><a href="https://blog.apaonline.org/2019/03/07/interview-with-deborah-mayo/">Statistics Wars: Interview with Deborah Mayo</a> - APA blog</li>
<li><a href="https://ndpr.nd.edu/news/statistical-inference-as-severe-testing-how-to-get-beyond-the-statistics-wars/">Review of <em>SIST</em> by Prasanta S. Bandyopadhyay</a></li>
</ul></li>
<li>Andrew Gelman
<ul>
<li>Beyond subjective and objective in statistics<a href="#fn60" class="footnoteRef" id="fnref60"><sup>60</sup></a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2014/09/05/confirmationist-falsificationist-paradigms-science/">Confirmationist and falsificationist paradigms of science</a> - Sept. 5, 2014</li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/03/20/retire-statistical-significance-the-discussion/">Retire Statistical Significance: The discussion</a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/09/11/exchange-with-deborah-mayo-on-abandoning-statistical-significance/">Exchange with Deborah Mayo on abandoning statistical significance</a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/04/12/several-reviews-of-deborah-mayos-new-book-statistical-inference-as-severe-testing-how-to-get-beyond-the-statistics-wars/">Several reviews of <em>SIST</em></a></li>
</ul></li>
<li>Larry Wasserman
<ul>
<li><a href="https://normaldeviate.wordpress.com/2012/07/28/statistical-principles/">Statistical Principles?</a></li>
</ul></li>
<li>Kevin Murphy
<ul>
<li>Pathologies of frequentist statistics<a href="#fn61" class="footnoteRef" id="fnref61"><sup>61</sup></a></li>
</ul></li>
<li>Greg Gandenberger
<ul>
<li><a href="http://gandenberger.org/2014/07/21/intro-to-statistical-methods/">An introduction to likelihoodist, bayesian, and frequentist methods (1/3)</a></li>
<li>As Neyman and Pearson put it in their original presentation of the frequentist approach, &#8220;without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behavior with regard to them, in the following which we insure that, in the long run of experience, we shall not too often be wrong&#8221; (1933, 291).</li>
<li><a href="http://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/">An introduction to likelihoodist, bayesian, and frequentist methods (2/3)</a></li>
<li><a href="http://gandenberger.org/2014/08/26/intro-to-statistical-methods-3/">An introduction to likelihoodist, bayesian, and frequentist methods (3/3)</a></li>
<li><a href="http://gandenberger.org/2013/10/21/against-likelihoodist-methods/">An argument against likelihoodist methods as genuine alternatives to bayesian and frequentist methods</a></li>
<li>&#8220;Why I am not a likelihoodist&#8221;<a href="#fn62" class="footnoteRef" id="fnref62"><sup>62</sup></a></li>
</ul></li>
</ul>
<div class="figure">
<img src="img/gandenberger-thesis-venn-diagram.png" alt="Figure 4: The major virtues and vices of Bayesian, frequentist, and likelihoodist approaches to statistical inference (gandenberger.org/research/, 2015)." id="fig:gandenberger-thesis-venn-diagram" />
<p class="caption">Figure 4: The major virtues and vices of Bayesian, frequentist, and likelihoodist approaches to statistical inference (<a href="http://gandenberger.org/research/">gandenberger.org/research/</a>, 2015).</p>
</div>
<h2 id="replication-crisis">Replication crisis</h2>
<h3 id="introduction">Introduction</h3>
<ul>
<li>&#8220;Why most published research findings are false&#8221;<a href="#fn63" class="footnoteRef" id="fnref63"><sup>63</sup></a></li>
</ul>
<h3 id="p-value-controversy">P-value controversy</h3>
<ul>
<li>ASA statement on <span class="math inline">\(p\)</span>-values<a href="#fn64" class="footnoteRef" id="fnref64"><sup>64</sup></a></li>
<li><a href="http://www.nature.com/news/big-names-in-statistics-want-to-shake-up-much-maligned-p-value-1.22375">Big names in statistics want to shake up much-maligned P value</a><a href="#fn65" class="footnoteRef" id="fnref65"><sup>65</sup></a></li>
<li><a href="https://hiphination.org/episodes/episode-7-hackademics-ii-the-hackers/">Hi-Phi Nation, episode 7</a></li>
<li>Fisher:</li>
</ul>
<blockquote>
<p>[N]o isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon; for the &#8220;one chance in a million&#8221; will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us. In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.<a href="#fn66" class="footnoteRef" id="fnref66"><sup>66</sup></a></p>
</blockquote>
<ul>
<li>Relationship to the LEE</li>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Uncomfortable_science">Uncomfortable science</a></li>
</ul></li>
<li>Mayo
<ul>
<li><a href="https://errorstatistics.com/2019/12/13/les-stats-cest-moi-we-take-that-step-here-adopt-our-fav-word-or-phil-stat/">&#8220;Les stats, c&#8217;est moi: We take that step here!&#8221;</a></li>
</ul></li>
<li>Vox: <a href="https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005">What a nerdy debate about p-values shows about science&#8211;and how to fix it</a></li>
<li>Karen Kafadar: <a href="https://magazine.amstat.org/blog/2019/12/01/kk_dec2019/">The Year in Review &#8230; And More to Come</a></li>
</ul>
<h2 id="information-geometry">Information geometry</h2>
<ul>
<li>&#8220;A gentle introduction to information geometry&#8221;<a href="#fn67" class="footnoteRef" id="fnref67"><sup>67</sup></a></li>
<li>&#8220;An elementary introduction to information geometry&#8221;<a href="#fn68" class="footnoteRef" id="fnref68"><sup>68</sup></a></li>
<li><em>Information Geometry and Its Applications</em><a href="#fn69" class="footnoteRef" id="fnref69"><sup>69</sup></a></li>
<li>&#8220;A geometric formulation of Occam&#8217;s razor for inference of parametric distributions&#8221;<a href="#fn70" class="footnoteRef" id="fnref70"><sup>70</sup></a></li>
<li>&#8220;Statistical inference, Occam&#8217;s Razor and statistical mechanics on the space of probability distributions&#8221;<a href="#fn71" class="footnoteRef" id="fnref71"><sup>71</sup></a></li>
<li><em>Geometric Modeling in Probability and Statistics</em><a href="#fn72" class="footnoteRef" id="fnref72"><sup>72</sup></a></li>
<li>Cranmer</li>
<li>&#8220;Geometric understanding of deep learning&#8221;<a href="#fn73" class="footnoteRef" id="fnref73"><sup>73</sup></a></li>
<li>Cohen<a href="#fn74" class="footnoteRef" id="fnref74"><sup>74</sup></a></li>
</ul>
<h2 id="machine-learning">Machine learning</h2>
<h3 id="classical">Classical</h3>
<ul>
<li>classification vs regression</li>
<li>supervised and unsupervised learning
<ul>
<li>classification = supervised; clustering = unsupervised</li>
</ul></li>
<li>Hastie, Tibshirani, &amp; Friedman<a href="#fn75" class="footnoteRef" id="fnref75"><sup>75</sup></a></li>
<li><em>Information Theory, Inference, and Learning</em><a href="#fn76" class="footnoteRef" id="fnref76"><sup>76</sup></a></li>
<li>VC-dimension<a href="#fn77" class="footnoteRef" id="fnref77"><sup>77</sup></a></li>
</ul>
<h3 id="deep-learning">Deep learning</h3>
<ul>
<li>backpropagation<a href="#fn78" class="footnoteRef" id="fnref78"><sup>78</sup></a></li>
<li>&#8220;Review: Deep learning&#8221;<a href="#fn79" class="footnoteRef" id="fnref79"><sup>79</sup></a></li>
<li>Lower to higher level representations<a href="#fn80" class="footnoteRef" id="fnref80"><sup>80</sup></a></li>
<li><em>Deep Learning</em><a href="#fn81" class="footnoteRef" id="fnref81"><sup>81</sup></a></li>
<li>&#8220;The explanation game: A formal framework for interpretable machine learning&#8221;<a href="#fn82" class="footnoteRef" id="fnref82"><sup>82</sup></a></li>
<li>Deep double descent
<ul>
<li>Bias and variance trade-off. See <a href="#bias-and-variance">Bias and variance</a>.</li>
<li>MSE and model capacity</li>
<li>Note the new deep learning view<a href="#fn83" class="footnoteRef" id="fnref83"><sup>83</sup></a></li>
<li>Deep double descent<a href="#fn84" class="footnoteRef" id="fnref84"><sup>84</sup></a></li>
<li><a href="https://openai.com/blog/deep-double-descent/">Deep Double Descent</a>. <em>OpenAI Blog</em>.</li>
<li>Hubinger, E. (2019). <a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">Understanding &#8220;Deep Double Descent&#8221;.</a> <em>LessWrong</em>.</li>
</ul></li>
</ul>
<div class="figure">
<img src="img/bengio-raw-to-higher-rep.png" alt="Figure 5: Raw input image is transformed into gradually higher levels of representation." id="fig:bengio-raw-to-higher-rep" />
<p class="caption">Figure 5: Raw input image is transformed into gradually higher levels of representation.<a href="#fn85" class="footnoteRef" id="fnref85"><sup>85</sup></a></p>
</div>
<h3 id="computer-vision">Computer Vision</h3>
<p>Computer Vision (CV)</p>
<ul>
<li>Fukushima: neocognitron</li>
<li>LeCun: OCR with backpropagation<a href="#fn86" class="footnoteRef" id="fnref86"><sup>86</sup></a></li>
<li>LeCun: LeNet-5<a href="#fn87" class="footnoteRef" id="fnref87"><sup>87</sup></a></li>
<li>Ciresan: MCDNN</li>
<li>Krizhevsky, Sutskever, and Hinton: AlexNet<a href="#fn88" class="footnoteRef" id="fnref88"><sup>88</sup></a></li>
</ul>
<h3 id="natural-language-processing">Natural language processing</h3>
<p>Natural language processing (NLP)</p>
<ul>
<li>RNNs and LSTMs
<ul>
<li>Hochreiter, S. &amp; Schmidhuber, J. (1997). Long short-term memory.<a href="#fn89" class="footnoteRef" id="fnref89"><sup>89</sup></a></li>
</ul></li>
<li>Sutskever seq2seq<a href="#fn90" class="footnoteRef" id="fnref90"><sup>90</sup></a></li>
<li>Review by Stahlberg<a href="#fn91" class="footnoteRef" id="fnref91"><sup>91</sup></a></li>
<li>Rationalism and empiricism in artificial intellegence: A survey of 25 years of evaluation [in NLP].<a href="#fn92" class="footnoteRef" id="fnref92"><sup>92</sup></a></li>
</ul>
<h3 id="reinforcement-learning">Reinforcement learning</h3>
<p>Reinforcement learning (RL)</p>
<h2 id="theoretical-machine-learning">Theoretical machine learning</h2>
<ul>
<li>No free lunch theorem</li>
<li>Relationship to statistical mechanics<a href="#fn93" class="footnoteRef" id="fnref93"><sup>93</sup></a></li>
<li>Relationship to gauge theory</li>
</ul>
<h2 id="automation">Automation</h2>
<h3 id="surrogate-models">Surrogate models</h3>
<ul>
<li>&#8220;The frontier of simulation-based inference&#8221;<a href="#fn94" class="footnoteRef" id="fnref94"><sup>94</sup></a></li>
</ul>
<h3 id="automl">AutoML</h3>
<ul>
<li>Neural Architecture Search (NAS)</li>
<li>AutoML frameworks</li>
</ul>
<h3 id="autoscience">AutoScience</h3>
<ul>
<li>Automated discovery</li>
<li>&#8220;Learning new physics from a machine&#8221;<a href="#fn95" class="footnoteRef" id="fnref95"><sup>95</sup></a></li>
<li>&#8220;Big data and extreme-scale computing: Pathways to Convergence-Toward a shaping strategy for a future software and data ecosystem for scientific inquiry.&#8221;<a href="#fn96" class="footnoteRef" id="fnref96"><sup>96</sup></a>
<ul>
<li>Note that this description of abduction is missing that it is normative (i.e. &#8220;best-fit&#8221;).</li>
</ul></li>
<li>&#8220;The End of Theory: The data deluge makes the scientific method obsolete.&#8221;<a href="#fn97" class="footnoteRef" id="fnref97"><sup>97</sup></a></li>
</ul>
<div class="figure">
<img src="img/BDEC-scientific-method.png" alt="Figure 6: The inference cycle for the process of scientific inquiry. The three distinct forms of inference (abduction, deduction, and induction) facilitate an all-encompassing vision, enabling HPC and HDA to converge in a rational and structured manner. HPC: high- performance computing; HDA: high-end data analysis." id="fig:BDEC-scientific-method" />
<p class="caption">Figure 6: The inference cycle for the process of scientific inquiry. The three distinct forms of inference (abduction, deduction, and induction) facilitate an all-encompassing vision, enabling HPC and HDA to converge in a rational and structured manner. HPC: high- performance computing; HDA: high-end data analysis.<a href="#fn98" class="footnoteRef" id="fnref98"><sup>98</sup></a></p>
</div>
<h2 id="implications-for-the-realism-debate">Implications for the realism debate</h2>
<ul>
<li>Korb<a href="#fn99" class="footnoteRef" id="fnref99"><sup>99</sup></a></li>
<li>Williamson<a href="#fn100" class="footnoteRef" id="fnref100"><sup>100</sup></a></li>
<li>Bensusan<a href="#fn101" class="footnoteRef" id="fnref101"><sup>101</sup></a></li>
<li>Note that NLP has implications to the philosophy of language and realism
<ul>
<li><a href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/">NLP word representations and the Wittgenstein philosophy of language</a>.<a href="#fn102" class="footnoteRef" id="fnref102"><sup>102</sup></a></li>
</ul></li>
<li>See the <a href="scientific-realism.html">Outline on scientific realism</a></li>
</ul>
<h2 id="my-thoughts">My thoughts</h2>
<p>My docs:</p>
<ul>
<li><a href="http://rreece.github.io/publications/pdf/2009.Reece.Derivation-of-the-Cramer-Rao-Bound.pdf">Derivation of the Cram&#233;r-Rao Bound</a></li>
</ul>
<p>My talks:</p>
<ul>
<li><a href="http://rreece.github.io/talks/pdf/2009-09-29-RReece-Likelihood-functions-for-SUSY.pdf">Likelihood functions for supersymmetric observables</a></li>
<li><a href="http://rreece.github.io/talks/pdf/2018-02-16-RReece-statistics-workshop-insight.pdf">Primer on statistics: MLE, confidence intervals, and hypothesis testing</a></li>
</ul>
<p>Lorem ipsum dolor sit amet, consectetur adipisicing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.</p>
<h2 id="annotated-bibliography">Annotated bibliography</h2>
<h3 id="mayo-d.g.-1996.-error-and-the-growth-of-experimental-knowledge.">Mayo, D.G. (1996). <em>Error and the Growth of Experimental Knowledge</em>.</h3>
<ul>
<li><span class="citation">Mayo (1996)</span></li>
</ul>
<h4 id="my-thoughts-1">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cowan-g.-1998.-statistical-data-analysis.">Cowan, G. (1998). <em>Statistical Data Analysis</em>.</h3>
<ul>
<li><span class="citation">Cowan (1998)</span> and <span class="citation">Cowan (2016)</span></li>
</ul>
<h4 id="my-thoughts-2">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="james-f.-2006.-statistical-methods-in-experimental-physics.">James, F. (2006). <em>Statistical Methods in Experimental Physics</em>.</h3>
<ul>
<li><span class="citation">James (2006)</span></li>
</ul>
<h4 id="my-thoughts-3">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cowan-g.-et-al.-2011.-asymptotic-formulae-for-likelihood-based-tests-of-new-physics.">Cowan, G. <em>et al.</em> (2011). Asymptotic formulae for likelihood-based tests of new physics.</h3>
<ul>
<li><span class="citation">Cowan et al. (2011)</span></li>
<li>Glen Cowan, Kyle Cranmer, Eilam Gross, Ofer Vitells</li>
</ul>
<h4 id="my-thoughts-4">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="atlas-collaboration.-2012.-combined-search-for-the-standard-model-higgs-boson.">ATLAS Collaboration. (2012). Combined search for the Standard Model Higgs boson.</h3>
<ul>
<li><span class="citation">ATLAS Collaboration (2012)</span></li>
<li><a href="http://arxiv.org/abs/1207.0319">arxiv:1207.0319</a></li>
</ul>
<h4 id="my-thoughts-5">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cranmer-k-2015.-practical-statistics-for-the-lhc.">Cranmer, K (2015). Practical statistics for the LHC.</h3>
<ul>
<li><span class="citation">Cranmer (2015)</span></li>
</ul>
<h4 id="my-thoughts-6">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="more-articles-to-do">More articles to do</h3>
<ul>
<li><em>All of Statistics</em><a href="#fn103" class="footnoteRef" id="fnref103"><sup>103</sup></a></li>
<li><em>The Foundations of Statistics</em><a href="#fn104" class="footnoteRef" id="fnref104"><sup>104</sup></a></li>
</ul>
<h2 id="links-and-encyclopedia-articles">Links and encyclopedia articles</h2>
<div class="clickmore">
<a id="link:encyclopedia_articles" class="closed" onclick="toggle_more('encyclopedia_articles')"> Click to show links </a>
</div>
<div id="encyclopedia_articles" class="more">
<h3 id="sep">SEP</h3>
<ul>
<li><a href="http://plato.stanford.edu/entries/abduction/">Abduction</a></li>
<li><a href="http://plato.stanford.edu/entries/knowledge-analysis/">Analysis of Knowledge</a></li>
<li><a href="https://plato.stanford.edu/entries/bayes-theorem/">Bayes&#8217; Theorem</a></li>
<li><a href="https://plato.stanford.edu/entries/epistemology-bayesian/">Bayesian Epistemology</a></li>
<li><a href="https://plato.stanford.edu/entries/carnap/">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="https://plato.stanford.edu/entries/causal-models/">Causal Models</a></li>
<li><a href="http://plato.stanford.edu/entries/causation-process/">Causal Processes</a></li>
<li><a href="https://plato.stanford.edu/entries/dutch-book/">Dutch Book Arguments</a></li>
<li><a href="http://plato.stanford.edu/entries/epistemology/">Epistemology</a></li>
<li><a href="http://plato.stanford.edu/entries/justep-foundational/">Foundationalist Theories of Epistemic Justification</a></li>
<li><a href="http://plato.stanford.edu/entries/hume/">Hume, David (1711-1776)</a></li>
<li><a href="http://plato.stanford.edu/entries/identity-indiscernible/">Identity of Indiscernibles</a></li>
<li><a href="http://plato.stanford.edu/entries/induction-problem/">Induction, The problem of</a></li>
<li><a href="https://plato.stanford.edu/entries/logic-probability/">Logic and Probability</a></li>
<li><a href="http://plato.stanford.edu/entries/epistemology-naturalized/">Naturalized epistemology</a></li>
<li><a href="https://plato.stanford.edu/entries/peirce/">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="http://plato.stanford.edu/entries/popper/">Popper, Karl (1902-1994)</a></li>
<li><a href="http://plato.stanford.edu/entries/sufficient-reason/">Principle of Sufficient Reason</a></li>
<li><a href="https://plato.stanford.edu/entries/probability-interpret/">Probability, Interpretations of</a></li>
<li><a href="https://plato.stanford.edu/entries/causation-probabilistic/">Probabilistic Causation</a></li>
<li><a href="https://plato.stanford.edu/entries/reichenbach/">Reichenbach, Hans (1891-1953)</a></li>
<li><a href="http://plato.stanford.edu/entries/scientific-explanation/">Scientific Explanation</a></li>
<li><a href="http://plato.stanford.edu/entries/statistics/">Statistics, Philosophy of</a></li>
</ul>
<h3 id="iep">IEP</h3>
<ul>
<li><a href="http://www.iep.utm.edu/carnap/">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="http://www.iep.utm.edu/epistemo/">Epistemology</a></li>
<li><a href="http://www.iep.utm.edu/hempel/">Hempel, Carl Gustav (1905-1997)</a></li>
<li><a href="http://www.iep.utm.edu/hume-cau/">Hume, David (1711-1776)</a></li>
<li><a href="http://www.iep.utm.edu/naturali/">Naturalism</a></li>
<li><a href="http://www.iep.utm.edu/nat-epis/">Naturalistic Epistemology</a></li>
<li><a href="https://www.iep.utm.edu/peircebi/">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="http://www.iep.utm.edu/red-ism/">Reductionism</a></li>
<li><a href="http://www.iep.utm.edu/safety-c/">Safety Condition for Knowledge, The</a></li>
<li><a href="http://www.iep.utm.edu/simplici/">Simplicity in the philosophy of science</a></li>
<li><a href="http://www.iep.utm.edu/ockham/">William of Ockham (1280-1349)</a></li>
</ul>
<h3 id="scholarpedia">Scholarpedia</h3>
<ul>
<li><a href="http://www.scholarpedia.org/article/Algorithmic_probability">Algorithmic probability</a></li>
<li><a href="http://www.scholarpedia.org/article/Universal_search">Universal search</a></li>
</ul>
<h3 id="wikipedia">Wikipedia</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Abductive_reasoning">Abductive reasoning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Algorithmic_information_theory">Algorithmic information theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Algorithmic_probability">Algorithmic probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/Analysis_of_variance">Analysis of variance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem">Aumann&#8217;s agreement theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes, Thomas (1701-1761)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Bernoulli, Jacob (1655-1705)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Allan_Birnbaum">Birnbaum, Allan (1923-1976)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a></li>
<li><a href="http://en.wikipedia.org/wiki/Rudolf_Carnap">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Confidence_interval">Confidence interval</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cosmic_variance">Cosmic variance</a></li>
<li><a href="http://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cram&#233;r-Rao bound</a></li>
<li><a href="https://en.wikipedia.org/wiki/Harald_Cram%C3%A9r">Cram&#233;r, Harald (1893-1985)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Data_science">Data science</a></li>
<li><a href="https://en.wikipedia.org/wiki/Decision_theory">Decision theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deductive-nomological_model">Deductive-nomological model</a></li>
<li><a href="http://en.wikipedia.org/wiki/Empiricism">Empircism</a></li>
<li><a href="http://en.wikipedia.org/wiki/Epistemology">Epistemology</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory data analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Fisher, Ronald (1890-1962)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Frequentist_inference">Frequentist inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Foundations_of_statistics">Foundations of statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/Gauss">Gauss, Carl Friedrich (1777-1855)</a></li>
<li><a href="https://en.wikipedia.org/wiki/German_tank_problem">German tank problem</a></li>
<li><a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset">Gosset, William Sealy (1876-1937)</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Graunt">Graunt, John (1620-1674)</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_probability">History of probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_statistics">History of statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/David_Hume">Hume, David (1711-1776)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Problem_of_induction">Induction, The problem of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inductive_reasoning">Inductive reasoning</a></li>
<li><a href="http://en.wikipedia.org/wiki/Interval_estimation">Interval estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inverse_probability">Inverse probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inverse_problem">Inverse problem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Alexey_Ivakhnenko">Ivakhnenko, Alexey (1913-2007)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Richard_Jeffrey">Jeffrey, Richard (1926-2002)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Jeffreys, Harold (1891-1989)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys prior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Kolmogorov, Andrey (1903-1987)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lady_tasting_tea">Lady tasting tea</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Laplace, Pierre-Simon (1749-1827)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Likelihood_principle">Likelihood principle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Likelihoodist_statistics">Likelihoodist statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_important_publications_in_statistics">List of important publications in statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum likelihood estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Stuart_Mill">Mill, John Stuart (1806-1873)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Misuse_of_p-values">Misuse of p-values</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Neyman, Jerzy (1894-1981)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neyman_construction">Neyman construction</a></li>
<li><a href="http://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma">Neyman-Pearson lemma</a></li>
<li><a href="http://en.wikipedia.org/wiki/William_of_Ockham">Ockham, William of (1287-1347)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Egon_Pearson">Pearson, Egon (1895-1980)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Pearson, Karl (1857-1936)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Charles_Sanders_Peirce">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson">Poisson, Sim&#233;on Denis (1781-1840)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Popper">Popper, Karl (1902-1994)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principle_of_sufficient_reason">Principle of sufficient reason</a></li>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and recall</a></li>
<li><a href="https://en.wikipedia.org/wiki/P-value">P-value</a></li>
<li><a href="https://en.wikipedia.org/wiki/C._R._Rao">Rao, C.R. (b. 1920)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Replication_crisis">Replication crisis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Leonard_Jimmie_Savage">Savage, Leonard Jimmie (1917-1971)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ray_Solomonoff">Solomonoff, Ray (1926-2009)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">Solomonoff&#8217;s theory of inductive inference</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_hypothesis_testing">Statistical hypothesis testing</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_inference">Statistical inference</a></li>
<li><a href="http://en.wikipedia.org/wiki/Sensitivity_and_specificity">Statistical sensitivity and specificity</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_significance">Statistical significance</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistics">Statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/Founders_of_statistics">Statistics, Founders of</a></li>
<li><a href="http://en.wikipedia.org/wiki/History_of_statistics">Statistics, History of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematical_statistics">Statistics, Mathematical</a></li>
<li><a href="http://en.wikipedia.org/wiki/Outline_of_statistics">Statistics, Outline of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Philosophy_of_statistics">Statistics, Philosophy of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Student%27s_t-test">Student&#8217;s t-test</a></li>
<li><a href="http://en.wikipedia.org/wiki/Systematic_error">Systematic error</a></li>
<li><a href="https://en.wikipedia.org/wiki/Trial_and_error">Trial and error</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Type_I_and_type_II_errors">Type-I and type-II errors</a></li>
<li><a href="https://en.wikipedia.org/wiki/Uncomfortable_science">Uncomfortable science</a></li>
<li><a href="http://en.wikipedia.org/wiki/Uniformitarianism">Uniformitarianism</a></li>
<li><a href="http://en.wikipedia.org/wiki/List_of_unsolved_problems_in_statistics">Unsolved problems in statistics, List of</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Venn">Venn, John (1834-1923)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Samuel_S._Wilks">Wilks, S.S. (1906-1964)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wilks%27_theorem">Wilks&#8217;s theorem</a></li>
</ul>
<h3 id="others">Others</h3>
<ul>
<li><a href="http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html">Deep Learning: Our Miraculous Year 1990-1991</a> - Schmidhuber</li>
<li><a href="http://errorstatistics.com/">errorstatistics.com</a> - Deborah Mayo&#8217;s blog</li>
<li><a href="http://statprob.com/encyclopedia/JohnGRAUNT.html">Graunt, John (1620-1674)</a> - statprob.com</li>
<li><a href="http://simplystatistics.org/2016/08/24/replication-crisis/">Peng, R. (2016). A Simple Explanation for the Replication Crisis in Science.</a> - simplystatistics.org</li>
<li><a href="https://stats.stackexchange.com/questions/240138/why-is-binary-classification-not-a-hypothesis-test">Why is binary classification not a hypothesis test?</a> - stackexchange.com</li>
<li><a href="https://stats.stackexchange.com/questions/40856/if-the-likelihood-principle-clashes-with-frequentist-probability-then-do-we-disc">If the likelihood principle clashes with frequentist probability then do we discard one of them?</a> - stackexchange.com</li>
<li><a href="https://stephens999.github.io/fiveMinuteStats/wilks.html">Wilks&#8217;s theorem</a> - fiveMinuteStats</li>
</ul>
</div>
<!-- REFERENCES -->
<h2 id="references" class="unnumbered">References</h2>
<p>   </p>
<div id="refs" class="references">
<div id="ref-Aldrich_1997_RAFisher_and_the_making_of_maximum_likelihood">
<p>Aldrich, J. (1997). R. A. Fisher and the making of maximum likelihood 1912-1922. <em>Statistical Science</em>, <em>12</em>, 162&#8211;176.</p>
</div>
<div id="ref-Amari_2016_Information_Geometry_and_Its_Applications">
<p>Amari, S. (2016). <em>Information Geometry and Its Applications</em>. Springer Japan.</p>
</div>
<div id="ref-Anderson_2008_The_End_of_Theory_The_data_deluge_makes">
<p>Anderson, C. (2008). The End of Theory: The data deluge makes the scientific method obsolete. <em>Wired</em>. June 23, 2008. <a href="https://www.wired.com/2008/06/pb-theory/" class="uri">https://www.wired.com/2008/06/pb-theory/</a></p>
</div>
<div id="ref-Asch_2018_Big_data_and_extreme_scale_computing_Pathways">
<p>Asch, M. et al. (2018). Big data and extreme-scale computing: Pathways to Convergence-Toward a shaping strategy for a future software and data ecosystem for scientific inquiry. <em>The International Journal of High Performance Computing Applications</em>, <em>32</em>, 435&#8211;479.</p>
</div>
<div id="ref-ATLAS_2012_Combined_search_for_the_Standard_Model_Higgs_boson">
<p>ATLAS Collaboration. (2012). Combined search for the Standard Model Higgs boson in <span class="math inline">\(pp\)</span> collisions at <span class="math inline">\(\sqrt{s}\)</span> = 7 TeV with the ATLAS detector. <em>Physical Review D</em>, <em>86</em>, 032003. <a href="https://arxiv.org/abs/1207.0319" class="uri">https://arxiv.org/abs/1207.0319</a></p>
</div>
<div id="ref-ATLAS_2011_The_CLs_method_information_for_conference">
<p>ATLAS Statistics Forum. (2011). The CLs method: information for conference speakers. <a href="http://www.pp.rhul.ac.uk/~cowan/stat/cls/CLsInfo.pdf" class="uri">http://www.pp.rhul.ac.uk/~cowan/stat/cls/CLsInfo.pdf</a></p>
</div>
<div id="ref-Bahri_2020_Statistical_mechanics_of_deep_learning">
<p>Bahri, Y. et al. (2020). Statistical mechanics of deep learning. <em>Annual Review of Condensed Matter Physics</em>, <em>11</em>, 501&#8211;528.</p>
</div>
<div id="ref-Balasubramanian_1996_A_geometric_formulation_of_Occams_razor">
<p>Balasubramanian, V. (1996a). A geometric formulation of Occam&#8217;s razor for inference of parametric distributions. <a href="https://arxiv.org/abs/adap-org/9601001" class="uri">https://arxiv.org/abs/adap-org/9601001</a></p>
</div>
<div id="ref-Balasubramanian_1996_Statistical_inference_Occams_razor">
<p>&#8212;&#8212;&#8212;. (1996b). Statistical inference, Occam&#8217;s razor and statistical mechanics on the space of probability distributions. <a href="https://arxiv.org/abs/cond-mat/9601030" class="uri">https://arxiv.org/abs/cond-mat/9601030</a></p>
</div>
<div id="ref-Belkin_2019_Reconciling_modern_machine_learning_practice">
<p>Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S. (2019). Reconciling modern machine-learning practice and the classical bias-variance trade-off. <em>Proceedings of the National Academy of Sciences</em>, <em>116</em>, 15849&#8211;15854. <a href="https://arxiv.org/abs/1812.11118" class="uri">https://arxiv.org/abs/1812.11118</a></p>
</div>
<div id="ref-Bengio_2009_Learning_deep_architectures_for_AI">
<p>Bengio, Y. (2009). Learning deep architectures for AI. <em>Foundations and Trends in Machine Learning</em>, <em>2</em>, 1&#8211;127.</p>
</div>
<div id="ref-Benjamin_2017_Redefine_statistical_significance">
<p>Benjamin, D.J. et al. (2017). Redefine statistical significance. <em>PsyArXiv</em>. July 22, 2017. <a href="https://psyarxiv.com/mky9j/" class="uri">https://psyarxiv.com/mky9j/</a></p>
</div>
<div id="ref-Bensusan_2000_Is_machine_learning_experimental_philosophy">
<p>Bensusan, H. (2000). Is machine learning experimental philosophy of science? In <em>ECAI2000 Workshop notes on scientific Reasoning in Artificial Intelligence and the Philosophy of Science</em> (pp. 9&#8211;14).</p>
</div>
<div id="ref-Berger_2003_Could_Fisher_Jeffreys_and_Neyman_have_agreed_on">
<p>Berger, J. O. (2003). Could Fisher, Jeffreys and Neyman have agreed on testing? <em>Statistical Science</em>, <em>18</em>, 1&#8211;32.</p>
</div>
<div id="ref-Berger_1988_The_Likelihood_Principle">
<p>Berger, J. O. &amp; Wolpert, R. L. (1988). <em>The Likelihood Principle</em> (2nd ed.). Haywood, CA: The Institute of Mathematical Statistics.</p>
</div>
<div id="ref-Birnbaum_1962_On_the_foundations_of_statistical_inference">
<p>Birnbaum, A. (1962). On the foundations of statistical inference. <em>Journal of the American Statistical Association</em>, <em>57</em>, 269&#8211;326.</p>
</div>
<div id="ref-Calin_2014_Geometric_Modeling_in_Probability_and_Statistics">
<p>Calin, O. &amp; Udriste, C. (2014). <em>Geometric Modeling in Probability and Statistics</em>. Springer Switzerland.</p>
</div>
<div id="ref-Carnap_1945_The_two_concepts_of_probability">
<p>Carnap, R. (1945). The two concepts of probability. <em>Philosophy and Phenomenological Research</em>, <em>5</em>, 513&#8211;32.</p>
</div>
<div id="ref-Carnap_1947_Probability_as_a_guide_in_life">
<p>&#8212;&#8212;&#8212;. (1947). Probability as a guide in life. <em>Journal of Philosophy</em>, <em>44</em>, 141&#8211;48.</p>
</div>
<div id="ref-Church_2019_A_survey_of_25_years_of_evaluation">
<p>Church, K. W. &amp; Hestness, J. (2019). A survey of 25 years of evaluation. <em>Natural Language Engineering</em>, <em>25</em>, 753&#8211;767.</p>
</div>
<div id="ref-Cohen_2016_Group_equivariant_convolutional_networks">
<p>Cohen, T. &amp; Welling, M. (2016). Group equivariant convolutional networks. <em>Proceedings of International Conference on Machine Learning</em>, <em>2016</em>, 2990&#8211;9. <a href="http://proceedings.mlr.press/v48/cohenc16.pdf" class="uri">http://proceedings.mlr.press/v48/cohenc16.pdf</a></p>
</div>
<div id="ref-Cousins_2018_Lectures_on_statistics_in_theory_Prelude">
<p>Cousins, R. D. (2018). Lectures on statistics in theory: Prelude to statistics in practice. <a href="https://arxiv.org/abs/1807.05996" class="uri">https://arxiv.org/abs/1807.05996</a></p>
</div>
<div id="ref-Cowan_1998_Statistical_Data_Analysis">
<p>Cowan, G. (1998). <em>Statistical Data Analysis</em>. Clarendon Press.</p>
</div>
<div id="ref-Cowan_2012_Discovery_sensitivity_for_a_counting_experiment">
<p>&#8212;&#8212;&#8212;. (2012). Discovery sensitivity for a counting experiment with background uncertainty. <a href="https://www.pp.rhul.ac.uk/~cowan/stat/notes/medsigNote.pdf" class="uri">https://www.pp.rhul.ac.uk/~cowan/stat/notes/medsigNote.pdf</a></p>
</div>
<div id="ref-Cowan_2016_StatisticsIn_CPatrignani_et_alParticle_Data">
<p>&#8212;&#8212;&#8212;. (2016). Statistics. In C. Patrignani et al. (Particle Data Group), <em>Chinese Physics C</em>, <em>40</em>, 100001. <a href="http://pdg.lbl.gov/2016/reviews/rpp2016-rev-statistics.pdf" class="uri">http://pdg.lbl.gov/2016/reviews/rpp2016-rev-statistics.pdf</a></p>
</div>
<div id="ref-Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">
<p>Cowan, G., Cranmer, K., Gross, E., &amp; Vitells, O. (2011). Asymptotic formulae for likelihood-based tests of new physics. <em>European Physical Journal C</em>, <em>71</em>, 1544. <a href="https://arxiv.org/abs/1007.1727" class="uri">https://arxiv.org/abs/1007.1727</a></p>
</div>
<div id="ref-Cramer_1946_A_contribution_to_the_theory_of_statistical">
<p>Cram&#233;r, H. (1946). A contribution to the theory of statistical estimation. <em>Skandinavisk Aktuarietidskrift</em>, <em>29</em>, 85&#8211;94.</p>
</div>
<div id="ref-Cranmer_2015_Practical_statistics_for_the_LHC">
<p>Cranmer, K. (2015). Practical statistics for the LHC. <a href="https://arxiv.org/abs/1503.07622" class="uri">https://arxiv.org/abs/1503.07622</a></p>
</div>
<div id="ref-Cranmer_2019_The_frontier_of_simulation_based_inference">
<p>Cranmer, K., Brehmer, J., &amp; Louppe, G. (2019). The frontier of simulation-based inference. <a href="https://arxiv.org/abs/1911.01429" class="uri">https://arxiv.org/abs/1911.01429</a></p>
</div>
<div id="ref-Cranmer_2012_HistFactory_A_tool_for_creating_statistical">
<p>Cranmer, K. et al. (2012). HistFactory: A tool for creating statistical models for use with RooFit and RooStats. Technical Report: CERN-OPEN-2012-016. <a href="http://inspirehep.net/record/1236448/" class="uri">http://inspirehep.net/record/1236448/</a></p>
</div>
<div id="ref-DAgnolo_2019_Learning_New_Physics_from_a_Machine">
<p>D&#8217;Agnolo, R. T. &amp; Wulzer, A. (2019). Learning New Physics from a Machine. <em>Physical Review D</em>, <em>99</em>, 015014. <a href="https://arxiv.org/abs/1806.02350" class="uri">https://arxiv.org/abs/1806.02350</a></p>
</div>
<div id="ref-Edwards_1974_The_history_of_likelihood">
<p>Edwards, A. W. F. (1974). The history of likelihood. <em>International Statistical Review</em>, <em>42</em>, 9&#8211;15.</p>
</div>
<div id="ref-Evans_2013_What_does_the_proof_of_Birnbaums_theorem_prove">
<p>Evans, M. (2013). What does the proof of Birnbaum&#8217;s theorem prove? <a href="https://arxiv.org/abs/1302.5468" class="uri">https://arxiv.org/abs/1302.5468</a></p>
</div>
<div id="ref-Feldman_1998_A_unified_approach_to_the_classical_statistical">
<p>Feldman, G. J. &amp; Cousins, R. D. (1998). A unified approach to the classical statistical analysis of small signals. <em>Physical Review D</em>, <em>57</em>, 3873. <a href="https://arxiv.org/abs/physics/9711021" class="uri">https://arxiv.org/abs/physics/9711021</a></p>
</div>
<div id="ref-Fisher_1912_On_an_absolute_criterion_for_fitting_frequency">
<p>Fisher, R. A. (1912). On an absolute criterion for fitting frequency curves. <em>Statistical Science</em>, <em>12</em>, 39&#8211;41.</p>
</div>
<div id="ref-Fisher_1915_Frequency_distribution_of_the_values">
<p>&#8212;&#8212;&#8212;. (1915). Frequency distribution of the values of the correlation coefficient in samples of indefinitely large population. <em>Biometrika</em>, <em>10</em>, 507&#8211;521.</p>
</div>
<div id="ref-Fisher_1921_On_the_probable_error_of_a_coefficient">
<p>&#8212;&#8212;&#8212;. (1921). On the &#8220;probable error&#8221; of a coefficient of correlation deduced from a small sample. <em>Metron</em>, <em>1</em>, 1&#8211;32.</p>
</div>
<div id="ref-Fisher_1935_The_Design_of_Experiments">
<p>&#8212;&#8212;&#8212;. (1935). <em>The Design of Experiments</em>. Hafner.</p>
</div>
<div id="ref-Frechet_1943_Sur_lextension_de_certaines_evaluations">
<p>Fr&#233;chet, M. (1943). Sur l&#8217;extension de certaines &#233;valuations statistiques au cas de petits &#233;chantillons. <em>Revue de L&#8217;Institut International de Statistique</em>, <em>11</em>, 182&#8211;205.</p>
</div>
<div id="ref-Gandenberger_2015_A_new_proof_of_the_likelihood_principle">
<p>Gandenberger, G. (2015). A new proof of the likelihood principle, <em>British Journal for the Philosophy of Science</em>, <em>66</em>, 475&#8211;503.</p>
</div>
<div id="ref-Gandenberger_2016_Why_I_am_not_a_likelihoodist">
<p>&#8212;&#8212;&#8212;. (2016). Why I am not a likelihoodist. <em>Philosopher&#8217;s Imprint</em>, <em>16</em>, 1&#8211;22.</p>
</div>
<div id="ref-Gelman_2017_Beyond_subjective_and_objective_in_statistics">
<p>Gelman, A. &amp; Hennig, C. (2017). Beyond subjective and objective in statistics. <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em>180</em>, 967&#8211;1033.</p>
</div>
<div id="ref-Goodfellow_2016_Deep_Learning">
<p>Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em>Deep Learning</em>. MIT Press.</p>
</div>
<div id="ref-Goodman_1999_Toward_evidence_based_medical_statistics_1_The_P">
<p>Goodman, S. N. (1999). Toward evidence-based medical statistics 1: The P value fallacy. <em>Annals of Internal Medicine</em>, <em>130</em>, 995&#8211;1004.</p>
</div>
<div id="ref-Hacking_1965_Logic_of_Statistical_Inference">
<p>Hacking, I. (1965). <em>Logic of Statistical Inference</em>. Cambridge University Press.</p>
</div>
<div id="ref-Hacking_1971_Jacques_Bernoullis_Art_of_conjecturing">
<p>&#8212;&#8212;&#8212;. (1971). Jacques Bernoulli&#8217;s Art of conjecturing. <em>The British Journal for the Philosophy of Science</em>, <em>22</em>, 209&#8211;229.</p>
</div>
<div id="ref-Hastie_2009_The_Elements_of_Statistical_Learning_Data_Mining">
<p>Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em>The Elements of Statistical Learning: Data Mining, Inference, and Prediction</em> (2nd ed.). Springer.</p>
</div>
<div id="ref-Hochreiter_1997_Long_short_term_memory">
<p>Hochreiter, S. &amp; Schmidhuber, J. (1997). Long short-term memory. *Neural Computation, 9(8), 1735&#8211;1780.</p>
</div>
<div id="ref-Ioannidis_2005_Why_most_published_research_findings_are_false">
<p>Ioannidis, J. P. (2005). Why most published research findings are false. <em>PLos Medicine</em>, <em>2</em>, 696&#8211;701.</p>
</div>
<div id="ref-James_2006_Statistical_Methods_in_Experimental_Particle">
<p>James, F. (2006). <em>Statistical Methods in Experimental Particle Physics</em>. World Scientific.</p>
</div>
<div id="ref-Junk_1999_Confidence_level_computation_for_combining">
<p>Junk, T. (1999). Confidence level computation for combining searches with small statistics. <em>Nuclear Instruments and Methods in Physics Research Section A</em>, <em>434</em>.</p>
</div>
<div id="ref-Korb_2001_Machine_learning_as_philosophy_of_science">
<p>Korb, K. B. (2001). Machine learning as philosophy of science. In <em>Proceedings of the ECML-PKDD-01 Workshop on Machine Learning as Experimental Philosophy of Science</em>. Freiburg.</p>
</div>
<div id="ref-Krizhevsky_2012_ImageNet_classification_with_deep_convolutional">
<p>Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). ImageNet classification with deep convolutional neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>2012</em>, 1097&#8211;1105. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" class="uri">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a></p>
</div>
<div id="ref-LeCun_2015_Deep_learning">
<p>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <em>Nature</em>, <em>521</em>, 436&#8211;44.</p>
</div>
<div id="ref-LeCun_1998_Gradient_based_learning_applied_to_document">
<p>LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). Gradient-based learning applied to document recognition. <em>Proceedings of the IEEE</em>, <em>86</em>, 2278&#8211;2324. <a href="http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf" class="uri">http://www.dengfanxin.cn/wp-content/uploads/2016/03/1998Lecun.pdf</a></p>
</div>
<div id="ref-LeCun_1989_Backpropagation_applied_to_handwritten_zip_code">
<p>LeCun, Y. et al. (1989). Backpropagation applied to handwritten zip code recognition. <em>Neural Computation</em>, <em>1</em>, 541&#8211;551. <a href="http://www.ics.uci.edu/~welling/teaching/273ASpring09/lecun-89e.pdf" class="uri">http://www.ics.uci.edu/~welling/teaching/273ASpring09/lecun-89e.pdf</a></p>
</div>
<div id="ref-Leemis_2008_Univariate_distribution_relationships">
<p>Leemis, L. M. &amp; McQueston, J. T. (2008). Univariate distribution relationships. <em>The American Statistician</em>, <em>62</em>, 45&#8211;53. <a href="http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf" class="uri">http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf</a></p>
</div>
<div id="ref-Lei_2018_Geometric_understanding_of_deep_learning">
<p>Lei, N., Luo, Z., Yau, S., &amp; Gu, D. X. (2018). Geometric understanding of deep learning. <a href="https://arxiv.org/abs/1805.10451" class="uri">https://arxiv.org/abs/1805.10451</a></p>
</div>
<div id="ref-Lista_2016_Statistical_Methods_for_Data_Analysis_in_Particle">
<p>Lista, L. (2016). <em>Statistical Methods for Data Analysis in Particle Physics</em>. Springer. <a href="http://foswiki.oris.mephi.ru/pub/Main/Literature/st_methods_for_data_analysis_in_particle_ph.pdf" class="uri">http://foswiki.oris.mephi.ru/pub/Main/Literature/st_methods_for_data_analysis_in_particle_ph.pdf</a></p>
</div>
<div id="ref-MacKay_2003_Information_Theory_Inference_and_Learning">
<p>MacKay, D. J. C. (2003). <em>Information Theory, Inference, and Learning Algorithms</em>. Cambridge University Press.</p>
</div>
<div id="ref-Mayo_1981_In_defense_of_the_Neyman_Pearson_theory">
<p>Mayo, D. G. (1981). In defense of the Neyman-Pearson theory of confidence intervals. <em>Philosophy of Science</em>, <em>48</em>, 269&#8211;280.</p>
</div>
<div id="ref-Mayo_1996_Error_and_the_Growth_of_Experimental_Knowledge">
<p>&#8212;&#8212;&#8212;. (1996). <em>Error and the Growth of Experimental Knowledge</em>. Chicago University Press.</p>
</div>
<div id="ref-Mayo_2014_On_the_Birnbaum_Argument_for_the_Strong_Likelihood">
<p>&#8212;&#8212;&#8212;. (2014). On the Birnbaum Argument for the Strong Likelihood Principle, <em>Statistical Science</em>, <em>29</em>, 227&#8211;266.</p>
</div>
<div id="ref-Mayo_2018_Statistical_Inference_as_Severe_Testing_How">
<p>&#8212;&#8212;&#8212;. (2018). <em>Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</em>. Cambridge University Press.</p>
</div>
<div id="ref-Mayo_2011_Error_statistics">
<p>Mayo, D. G. &amp; Spanos, A. (2011). Error statistics. In <em>Philosophy of Statistics</em> (pp. 153&#8211;198). North-Holland.</p>
</div>
<div id="ref-Murphy_2012_Machine_Learning_A_probabilistic_perspective">
<p>Murphy, K. P. (2012). <em>Machine Learning: A probabilistic perspective</em>. MIT Press.</p>
</div>
<div id="ref-Nakkiran_2019_Deep_double_descent_Where_bigger_models_and_more">
<p>Nakkiran, P. et al. (2019). Deep double descent: Where bigger models and more data hurt. <a href="https://arxiv.org/abs/1912.02292" class="uri">https://arxiv.org/abs/1912.02292</a></p>
</div>
<div id="ref-Neyman_1933_On_the_problem_of_the_most_efficient_tests">
<p>Neyman, J. &amp; Pearson, E. S. (1933). On the problem of the most efficient tests of statistical hypotheses. <em>Philosophical Transactions of the Royal Society A</em>, <em>231</em>, 289&#8211;337.</p>
</div>
<div id="ref-Nielsen_2018_An_elementary_introduction_to_information_geometry">
<p>Nielsen, F. (2018). An elementary introduction to information geometry. <a href="https://arxiv.org/abs/1808.08271" class="uri">https://arxiv.org/abs/1808.08271</a></p>
</div>
<div id="ref-OHagan_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2B">
<p>O&#8217;Hagan, A. (2010). <em>Kendall&#8217;s Advanced Theory of Statistics, Vol 2B: Bayesian Inference</em>. Wiley.</p>
</div>
<div id="ref-Peirce_1883_Studies_in_Logic">
<p>Peirce, C. S. (1883). <em>Studies in Logic</em>. Boston: Little, Brown, and Co.</p>
</div>
<div id="ref-Perone_2018_NLP_word_representations_and_the_Wittgenstein">
<p>Perone, C. S. (2018). NLP word representations and the Wittgenstein philosophy of language. <a href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/" class="uri">http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/</a></p>
</div>
<div id="ref-Rao_1945_Information_and_the_accuracy_attainable">
<p>Rao, C. R. (1945). Information and the accuracy attainable in the estimation of statistical parameters. <em>Bulletin of the Calcutta Mathematical Society</em>, <em>37</em>, 81&#8211;91.</p>
</div>
<div id="ref-Rao_1947_Minimum_variance_and_the_estimation_of_several">
<p>&#8212;&#8212;&#8212;. (1947). Minimum variance and the estimation of several parameters. In <em>Mathematical Proceedings of the Cambridge Philosophical Society</em>. 43, 280&#8211;283. Cambridge University Press.</p>
</div>
<div id="ref-Read_2002_Presentation_of_search_results_the_CLs_technique">
<p>Read, A. L. (2002). Presentation of search results: the CLs technique. <em>Journal of Physics G: Nuclear and Particle Physics</em>, <em>28</em>, 2693. <a href="https://indico.cern.ch/event/398949/attachments/799330/1095613/The_CLs_Technique.pdf" class="uri">https://indico.cern.ch/event/398949/attachments/799330/1095613/The_CLs_Technique.pdf</a></p>
</div>
<div id="ref-Reid_1998_Neyman">
<p>Reid, C. (1998). <em>Neyman</em>. Springer-Verlag.</p>
</div>
<div id="ref-Rice_2007_Mathematical_Statistics_and_Data_Analysis">
<p>Rice, J. A. (2007). <em>Mathematical Statistics and Data Analysis</em> (3rd ed.). Thomson.</p>
</div>
<div id="ref-Royall_1997_Statistical_Evidence_A_likelihood_paradigm">
<p>Royall, R. (1997). <em>Statistical Evidence: A likelihood paradigm</em>. CRC Press.</p>
</div>
<div id="ref-Rumelhart_1986_Learning_representations_by_back_propagating">
<p>Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). Learning representations by back-propagating errors. <em>Nature</em>, <em>323</em>, 533&#8211;536.</p>
</div>
<div id="ref-Salsburg_2001_The_Lady_Tasting_Tea">
<p>Salsburg, D. (2001). <em>The Lady Tasting Tea</em>. Holt.</p>
</div>
<div id="ref-Savage_1954_The_Foundations_of_Statistics">
<p>Savage, L. J. (1954). <em>The Foundations of Statistics</em>. John Wiley &amp; Sons.</p>
</div>
<div id="ref-Sinervo_2002_Signal_significance_in_particle_physics">
<p>Sinervo, P. (2002). Signal significance in particle physics. In M. Whalley &amp; L. Lyons (Eds.), <em>Proceedings of the Conference on Advanced Statistical Techniques in Particle Physics</em>. Durham, UK: Institute of Particle Physics Phenomenology. <a href="https://arxiv.org/abs/hep-ex/0208005v1" class="uri">https://arxiv.org/abs/hep-ex/0208005v1</a></p>
</div>
<div id="ref-Sinervo_2003_Definition_and_treatment_of_systematic">
<p>&#8212;&#8212;&#8212;. (2003). Definition and treatment of systematic uncertainties in high energy physics and astrophysics. In Lyons L., Mount R., &amp; R. Reitmeyer (Eds.), <em>Proceedings of the Conference on Statistical Problems in Particle Physics, Astrophysics, and Cosmology (PhyStat2003)</em> (pp. 122&#8211;129). Stanford Linear Accelerator Center. <a href="https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf" class="uri">https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf</a></p>
</div>
<div id="ref-Smith_2019_A_gentle_introduction_to_information_geometry">
<p>Smith, L. (2019). A gentle introduction to information geometry. September 27, 2019. <a href="http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html" class="uri">http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html</a></p>
</div>
<div id="ref-Stahlberg_2019_Neural_machine_translation_A_review">
<p>Stahlberg, F. (2019). Neural machine translation: A review. <a href="https://arxiv.org/abs/1912.02047" class="uri">https://arxiv.org/abs/1912.02047</a></p>
</div>
<div id="ref-Stuart_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2A">
<p>Stuart, A., Ord, K., &amp; Arnold, S. (2010). <em>Kendall&#8217;s Advanced Theory of Statistics, Vol 2A: Classical Inference and the Linear Model</em>. Wiley.</p>
</div>
<div id="ref-Sutskever_2014_Sequence_to_sequence_learning_with_neural">
<p>Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). Sequence to sequence learning with neural networks. <em>Advances in Neural Information Processing Systems</em>, <em>2014</em>, 3104&#8211;3112. <a href="https://arxiv.org/abs/1409.3215" class="uri">https://arxiv.org/abs/1409.3215</a></p>
</div>
<div id="ref-Sznajder_2018_Inductive_logic_as_explication_The_evolution">
<p>Sznajder, M. (2018). Inductive logic as explication: The evolution of Carnap&#8217;s notion of logical probability. <em>The Monist</em>, <em>101</em>, 417&#8211;440.</p>
</div>
<div id="ref-Tukey_1977_Exploratory_Data_Analysis">
<p>Tukey, J. W. (1977). <em>Exploratory Data Analysis</em>. Pearson.</p>
</div>
<div id="ref-Vapnik_1994_Measuring_the_VC_dimension_of_a_learning_machine">
<p>Vapnik, V., Levin, E., &amp; LeCun, Y. (1994). Measuring the VC-dimension of a learning machine. <em>Neural Computation</em>, <em>6</em>, 851&#8211;876.</p>
</div>
<div id="ref-Venn_1888_The_Logic_of_Chance">
<p>Venn, J. (1888). <em>The Logic of Chance</em>. London: MacMillan and Co. (Originally published in 1866).</p>
</div>
<div id="ref-Wald_1943_Tests_of_statistical_hypotheses_concerning_several">
<p>Wald, A. (1943). Tests of statistical hypotheses concerning several parameters when the number of observations is large. <em>Transactions of the American Mathematical Society</em>, <em>54</em>, 426&#8211;482.</p>
</div>
<div id="ref-Wasserman_2003_All_of_Statistics_A_Concise_Course_in_Statistical">
<p>Wasserman, L. (2003). <em>All of Statistics: A Concise Course in Statistical Inference</em>. Springer.</p>
</div>
<div id="ref-Wasserstein_2016_The_ASAs_statement_on_p_values_Context_process">
<p>Wasserstein, R. L. &amp; Lazar, N. A. (2016). The ASA&#8217;s statement on p-values: Context, process, and purpose. <em>American Statistician</em>, <em>70</em>, 129&#8211;133.</p>
</div>
<div id="ref-Watson_2019_The_explanation_game_A_formal_framework">
<p>Watson, D. &amp; Floridi, L. (2019). The explanation game: A formal framework for interpretable machine learning. SSRN 3509737. <a href="https://ssrn.com/abstract=3509737" class="uri">https://ssrn.com/abstract=3509737</a></p>
</div>
<div id="ref-Weisberg_2019_Odds__Ends_Introducing_Probability__Decision">
<p>Weisberg, J. (2019). <em>Odds &amp; Ends: Introducing Probability &amp; Decision with a Visual Emphasis</em>. <a href="https://jonathanweisberg.org/vip/" class="uri">https://jonathanweisberg.org/vip/</a></p>
</div>
<div id="ref-Wilks_1938_The_large_sample_distribution_of_the_likelihood">
<p>Wilks, S. S. (1938). The large-sample distribution of the likelihood ratio for testing composite hypotheses. <em>The Annals of Mathematical Statistics</em>, <em>9</em>, 60&#8211;62.</p>
</div>
<div id="ref-Williamson_2009_The_philosophy_of_science_and_its_relation">
<p>Williamson, J. (2009). The philosophy of science and its relation to machine learning. In <em>Scientific Data Mining and Knowledge Discovery</em> (pp. 77&#8211;89). Springer, Berlin, Heidelberg.</p>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p><span class="citation">Carnap (1947)</span>.<a href="#fnref1">&#8617;</a></p></li>
<li id="fn2"><p><span class="citation">Weisberg (2019)</span>, ch.&#160;15.<a href="#fnref2">&#8617;</a></p></li>
<li id="fn3"><p><span class="citation">Cranmer (2015)</span>, p.&#160;6.<a href="#fnref3">&#8617;</a></p></li>
<li id="fn4"><p><span class="citation">Carnap (1945)</span>.<a href="#fnref4">&#8617;</a></p></li>
<li id="fn5"><p><span class="citation">Royall (1997)</span>, p.&#160;171&#8211;2.<a href="#fnref5">&#8617;</a></p></li>
<li id="fn6"><p><span class="citation">Edwards (1974)</span>, p.&#160;9.<a href="#fnref6">&#8617;</a></p></li>
<li id="fn7"><p><span class="citation">Hacking (1971)</span>.<a href="#fnref7">&#8617;</a></p></li>
<li id="fn8"><p>Bernoulli, J. (1713). <em>Ars Conjectandi</em>, Chapter II, Part IV, defining the art of conjecture [<a href="https://en.wikiquote.org/wiki/Jacob_Bernoulli">wikiquote</a>].<a href="#fnref8">&#8617;</a></p></li>
<li id="fn9"><p><span class="citation">Venn (1888)</span>.<a href="#fnref9">&#8617;</a></p></li>
<li id="fn10"><p><span class="citation">Peirce (1883)</span>, p.&#160;126&#8211;181.<a href="#fnref10">&#8617;</a></p></li>
<li id="fn11"><p><span class="citation">Fisher (1912)</span>.<a href="#fnref11">&#8617;</a></p></li>
<li id="fn12"><p><span class="citation">Fisher (1915)</span>.<a href="#fnref12">&#8617;</a></p></li>
<li id="fn13"><p><span class="citation">Fisher (1921)</span>.<a href="#fnref13">&#8617;</a></p></li>
<li id="fn14"><p><span class="citation">Salsburg (2001)</span>.<a href="#fnref14">&#8617;</a></p></li>
<li id="fn15"><p><span class="citation">Reid (1998)</span>.<a href="#fnref15">&#8617;</a></p></li>
<li id="fn16"><p><span class="citation">Stuart, Ord, &amp; Arnold (2010)</span>.<a href="#fnref16">&#8617;</a></p></li>
<li id="fn17"><p><span class="citation">James (2006)</span>.<a href="#fnref17">&#8617;</a></p></li>
<li id="fn18"><p><span class="citation">Cousins (2018)</span>.<a href="#fnref18">&#8617;</a></p></li>
<li id="fn19"><p><span class="citation">Cowan (1998)</span> and <span class="citation">Cowan (2016)</span>.<a href="#fnref19">&#8617;</a></p></li>
<li id="fn20"><p><span class="citation">Lista (2016)</span>.<a href="#fnref20">&#8617;</a></p></li>
<li id="fn21"><p><span class="citation">Cranmer (2015)</span>.<a href="#fnref21">&#8617;</a></p></li>
<li id="fn22"><p><span class="citation">Weisberg (2019)</span>.<a href="#fnref22">&#8617;</a></p></li>
<li id="fn23"><p><span class="citation">Leemis &amp; McQueston (2008)</span>.<a href="#fnref23">&#8617;</a></p></li>
<li id="fn24"><p><span class="citation">Cranmer, K. et al. (2012)</span>.<a href="#fnref24">&#8617;</a></p></li>
<li id="fn25"><p><span class="citation">Cowan (1998)</span> and <span class="citation">Cowan (2016)</span>, p.&#160;X.<a href="#fnref25">&#8617;</a></p></li>
<li id="fn26"><p><span class="citation">Aldrich (1997)</span>.<a href="#fnref26">&#8617;</a></p></li>
<li id="fn27"><p><span class="citation">Fr&#233;chet (1943)</span>, <span class="citation">Cram&#233;r (1946)</span>, <span class="citation">Rao (1945)</span>, and <span class="citation">Rao (1947)</span>.<a href="#fnref27">&#8617;</a></p></li>
<li id="fn28"><p><span class="citation">Rice (2007)</span>, p.&#160;300&#8211;2.<a href="#fnref28">&#8617;</a></p></li>
<li id="fn29"><p><span class="citation">Fisher (1935)</span>, p.&#160;16.<a href="#fnref29">&#8617;</a></p></li>
<li id="fn30"><p><span class="citation">Goodman (1999)</span>. p.&#160;998.<a href="#fnref30">&#8617;</a></p></li>
<li id="fn31"><p><span class="citation">Neyman &amp; Pearson (1933)</span>.<a href="#fnref31">&#8617;</a></p></li>
<li id="fn32"><p><span class="citation">Sinervo (2002)</span> and <span class="citation">Cowan (2012)</span>.<a href="#fnref32">&#8617;</a></p></li>
<li id="fn33"><p><span class="citation">Feldman &amp; Cousins (1998)</span>.<a href="#fnref33">&#8617;</a></p></li>
<li id="fn34"><p><span class="citation">Junk (1999)</span>.<a href="#fnref34">&#8617;</a></p></li>
<li id="fn35"><p><span class="citation">Read (2002)</span>.<a href="#fnref35">&#8617;</a></p></li>
<li id="fn36"><p><span class="citation">ATLAS Statistics Forum (2011)</span>.<a href="#fnref36">&#8617;</a></p></li>
<li id="fn37"><p><span class="citation">Wilks (1938)</span>.<a href="#fnref37">&#8617;</a></p></li>
<li id="fn38"><p><span class="citation">Wald (1943)</span>.<a href="#fnref38">&#8617;</a></p></li>
<li id="fn39"><p><span class="citation">Cowan, Cranmer, Gross, &amp; Vitells (2011)</span>.<a href="#fnref39">&#8617;</a></p></li>
<li id="fn40"><p><span class="citation">Murphy (2012)</span>, p.&#160;197.<a href="#fnref40">&#8617;</a></p></li>
<li id="fn41"><p><span class="citation">Sinervo (2003)</span>.<a href="#fnref41">&#8617;</a></p></li>
<li id="fn42"><p><span class="citation">Tukey (1977)</span>.<a href="#fnref42">&#8617;</a></p></li>
<li id="fn43"><p><span class="citation">Edwards (1974)</span>.<a href="#fnref43">&#8617;</a></p></li>
<li id="fn44"><p><span class="citation">Birnbaum (1962)</span>.<a href="#fnref44">&#8617;</a></p></li>
<li id="fn45"><p><span class="citation">Hacking (1965)</span>.<a href="#fnref45">&#8617;</a></p></li>
<li id="fn46"><p><span class="citation">Berger &amp; Wolpert (1988)</span>.<a href="#fnref46">&#8617;</a></p></li>
<li id="fn47"><p><span class="citation">O&#8217;Hagan (2010)</span>, p.&#160;17&#8211;18.<a href="#fnref47">&#8617;</a></p></li>
<li id="fn48"><p><span class="citation">Evans (2013)</span>.<a href="#fnref48">&#8617;</a></p></li>
<li id="fn49"><p><span class="citation">Mayo (2014)</span>.<a href="#fnref49">&#8617;</a></p></li>
<li id="fn50"><p><span class="citation">Gandenberger (2015)</span>.<a href="#fnref50">&#8617;</a></p></li>
<li id="fn51"><p>Mayo, D.G. (2019). <a href="https://errorstatistics.com/2019/04/04/excursion-1-tour-ii-error-probing-tools-versus-logics-of-evidence-excerpt/">The Law of Likelihood and Error Statistics</a>.<a href="#fnref51">&#8617;</a></p></li>
<li id="fn52"><p><span class="citation">Sznajder (2018)</span>.<a href="#fnref52">&#8617;</a></p></li>
<li id="fn53"><p><span class="citation">Hacking (1965)</span>.<a href="#fnref53">&#8617;</a></p></li>
<li id="fn54"><p><span class="citation">Royall (1997)</span>.<a href="#fnref54">&#8617;</a></p></li>
<li id="fn55"><p><span class="citation">Berger (2003)</span>.<a href="#fnref55">&#8617;</a></p></li>
<li id="fn56"><p><span class="citation">Mayo (1981)</span>.<a href="#fnref56">&#8617;</a></p></li>
<li id="fn57"><p><span class="citation">Mayo (1996)</span>.<a href="#fnref57">&#8617;</a></p></li>
<li id="fn58"><p><span class="citation">Mayo &amp; Spanos (2011)</span>.<a href="#fnref58">&#8617;</a></p></li>
<li id="fn59"><p><span class="citation">Mayo (2018)</span>.<a href="#fnref59">&#8617;</a></p></li>
<li id="fn60"><p><span class="citation">Gelman &amp; Hennig (2017)</span>.<a href="#fnref60">&#8617;</a></p></li>
<li id="fn61"><p><span class="citation">Murphy (2012)</span>, ch.&#160;6.6.<a href="#fnref61">&#8617;</a></p></li>
<li id="fn62"><p><span class="citation">Gandenberger (2016)</span>.<a href="#fnref62">&#8617;</a></p></li>
<li id="fn63"><p><span class="citation">Ioannidis (2005)</span>.<a href="#fnref63">&#8617;</a></p></li>
<li id="fn64"><p><span class="citation">Wasserstein &amp; Lazar (2016)</span>.<a href="#fnref64">&#8617;</a></p></li>
<li id="fn65"><p><span class="citation">Benjamin, D.J. et al. (2017)</span>.<a href="#fnref65">&#8617;</a></p></li>
<li id="fn66"><p><span class="citation">Fisher (1935)</span>, p.&#160;13&#8211;14.<a href="#fnref66">&#8617;</a></p></li>
<li id="fn67"><p><span class="citation">Smith (2019)</span>.<a href="#fnref67">&#8617;</a></p></li>
<li id="fn68"><p><span class="citation">Nielsen (2018)</span>.<a href="#fnref68">&#8617;</a></p></li>
<li id="fn69"><p><span class="citation">Amari (2016)</span>.<a href="#fnref69">&#8617;</a></p></li>
<li id="fn70"><p><span class="citation">Balasubramanian (1996a)</span>.<a href="#fnref70">&#8617;</a></p></li>
<li id="fn71"><p><span class="citation">Balasubramanian (1996b)</span>.<a href="#fnref71">&#8617;</a></p></li>
<li id="fn72"><p><span class="citation">Calin &amp; Udriste (2014)</span>.<a href="#fnref72">&#8617;</a></p></li>
<li id="fn73"><p><span class="citation">Lei, Luo, Yau, &amp; Gu (2018)</span><a href="#fnref73">&#8617;</a></p></li>
<li id="fn74"><p><span class="citation">Cohen &amp; Welling (2016)</span>.<a href="#fnref74">&#8617;</a></p></li>
<li id="fn75"><p><span class="citation">Hastie, Tibshirani, &amp; Friedman (2009)</span>.<a href="#fnref75">&#8617;</a></p></li>
<li id="fn76"><p><span class="citation">MacKay (2003)</span>.<a href="#fnref76">&#8617;</a></p></li>
<li id="fn77"><p><span class="citation">Vapnik, Levin, &amp; LeCun (1994)</span>.<a href="#fnref77">&#8617;</a></p></li>
<li id="fn78"><p><span class="citation">Rumelhart, Hinton, &amp; Williams (1986)</span>.<a href="#fnref78">&#8617;</a></p></li>
<li id="fn79"><p><span class="citation">LeCun, Bengio, &amp; Hinton (2015)</span>.<a href="#fnref79">&#8617;</a></p></li>
<li id="fn80"><p><span class="citation">Bengio (2009)</span>.<a href="#fnref80">&#8617;</a></p></li>
<li id="fn81"><p><span class="citation">Goodfellow, Bengio, &amp; Courville (2016)</span>.<a href="#fnref81">&#8617;</a></p></li>
<li id="fn82"><p><span class="citation">Watson &amp; Floridi (2019)</span>.<a href="#fnref82">&#8617;</a></p></li>
<li id="fn83"><p><span class="citation">Belkin, Hsu, Ma, &amp; Mandal (2019)</span>.<a href="#fnref83">&#8617;</a></p></li>
<li id="fn84"><p><span class="citation">Nakkiran, P. et al. (2019)</span>.<a href="#fnref84">&#8617;</a></p></li>
<li id="fn85"><p><span class="citation">Bengio (2009)</span>.<a href="#fnref85">&#8617;</a></p></li>
<li id="fn86"><p><span class="citation">LeCun, Y. et al. (1989)</span>.<a href="#fnref86">&#8617;</a></p></li>
<li id="fn87"><p><span class="citation">LeCun, Bottou, Bengio, &amp; Haffner (1998)</span>.<a href="#fnref87">&#8617;</a></p></li>
<li id="fn88"><p><span class="citation">Krizhevsky, Sutskever, &amp; Hinton (2012)</span>.<a href="#fnref88">&#8617;</a></p></li>
<li id="fn89"><p><span class="citation">Hochreiter &amp; Schmidhuber (1997)</span>.<a href="#fnref89">&#8617;</a></p></li>
<li id="fn90"><p><span class="citation">Sutskever, Vinyals, &amp; Le (2014)</span>.<a href="#fnref90">&#8617;</a></p></li>
<li id="fn91"><p><span class="citation">Stahlberg (2019)</span>.<a href="#fnref91">&#8617;</a></p></li>
<li id="fn92"><p><span class="citation">Church &amp; Hestness (2019)</span>.<a href="#fnref92">&#8617;</a></p></li>
<li id="fn93"><p><span class="citation">Bahri, Y. et al. (2020)</span>.<a href="#fnref93">&#8617;</a></p></li>
<li id="fn94"><p><span class="citation">Cranmer, Brehmer, &amp; Louppe (2019)</span>.<a href="#fnref94">&#8617;</a></p></li>
<li id="fn95"><p><span class="citation">D&#8217;Agnolo &amp; Wulzer (2019)</span>.<a href="#fnref95">&#8617;</a></p></li>
<li id="fn96"><p><span class="citation">Asch, M. et al. (2018)</span>.<a href="#fnref96">&#8617;</a></p></li>
<li id="fn97"><p><span class="citation">Anderson (2008)</span>.<a href="#fnref97">&#8617;</a></p></li>
<li id="fn98"><p><span class="citation">Asch, M. et al. (2018)</span>.<a href="#fnref98">&#8617;</a></p></li>
<li id="fn99"><p><span class="citation">Korb (2001)</span>.<a href="#fnref99">&#8617;</a></p></li>
<li id="fn100"><p><span class="citation">Williamson (2009)</span>.<a href="#fnref100">&#8617;</a></p></li>
<li id="fn101"><p><span class="citation">Bensusan (2000)</span>.<a href="#fnref101">&#8617;</a></p></li>
<li id="fn102"><p><span class="citation">Perone (2018)</span>.<a href="#fnref102">&#8617;</a></p></li>
<li id="fn103"><p><span class="citation">Wasserman (2003)</span>.<a href="#fnref103">&#8617;</a></p></li>
<li id="fn104"><p><span class="citation">Savage (1954)</span>.<a href="#fnref104">&#8617;</a></p></li>
</ol>
</div>

</div> <!-- end pagecontainer -->
</div> <!-- end mainbody -->

<div class="nav">
<ul>
    <li> <a href="./">&#8634;&nbsp;Contents</a> </li>
    <li> <a href="#site_header">&#8613;&nbsp;Top</a> </li>
    <li> <a href="./scientific-method.html">&#8612;&nbsp;Previous</a> </li>
    <li> <a href="./scientific-realism.html">&#8614;&nbsp;Next</a> </li>
</ul>
</div>

<!--
<div id="afterbody">
</div>
-->

<div id="site_footer">
  <div class="signature">
    <p><i>Ryan Reece</i></p>
    <p><a href="https://twitter.com/RyanDavidReece">@RyanDavidReece</a><br/><img class="email" src="img/my-email-alt-blue.png" alt="my email address"/></p>
    <p>Mon May 25, 2020</p>
  </div>
  <div class="license">
    <p>&#169; 2014-2020 Ryan Reece. All rights reserved.</p>
    <p>Made with <a href="https://github.com/rreece/markdown-memo">markdown-memo</a>.</p>
  </div>
  <div style="clear:both;"></div>
</div>

<!-- disqus stuff -->
<div id="disqus_stuff">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'ryans-outline-of-philosophy';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript><p>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></p></noscript>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'ryans-outline-of-philosophy';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</div> <!-- end disqus_stuff -->

</body>
</html>

