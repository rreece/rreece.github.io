<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
      "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<!--- Google Analytics Tracking stuff  -->
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-QD5EP98QH0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-QD5EP98QH0');
</script>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Style-Type" content="text/css" />
<meta name="generator" content="pandoc" />
<meta name="author" content="Ryan Reece" />
<meta name="date" content="Mon May 20, 2024" />
<title>Philosophy of statistics - Ryan's Outline of Philosophy</title>
<meta name="description" content="Ryan Reece&#x2019;s outline of philosophy" />
<link rel="stylesheet" type="text/css" href="templates/markdown-memo-alt-blue-dark.css"/>
<link rel="icon" type="image/png" href="img/markdown-favicon-196x196.png" />

<!--- MathJax stuff -->
<script type="text/javascript" src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'></script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "all"} } });
</script>

<!--- toggle_more function  -->
<script type="text/javascript">
    //<![CDATA[
    function toggle_more(objID) {
        if (!document.getElementById) return;
        var ob = document.getElementById(objID).style;
        ob.display = (ob.display == 'block')?'none':'block';
        var ob2 = document.getElementById('link:'+objID);
        ob2.className = (ob2.className == 'open') ? 'closed' : 'open';
    }
    //]]>
</script>

</head>

<body>

<div id="site_header">
    <a href="./">Ryan&#x2019;s Outline of Philosophy</a>
</div>

<!--
<div id="beforebody">
</div>
-->

<div class="nav">
<ul>
    <li> <a href="./">&#8634;&nbsp;Contents</a> </li>
    <li> <a href="#site_footer">&#8615;&nbsp;Bottom</a> </li>
    <li> <a href="./scientific-method.html">&#8612;&nbsp;Previous</a> </li>
    <li> <a href="./scientific-realism.html">&#8614;&nbsp;Next</a> </li>
</ul>
</div>

<div id="mainbody">
<div id="pagecontainer">

<h1 id="philosophy-of-statistics">Philosophy of statistics</h1>
<p>Statistical analysis is very important in addressing the problem of induction. Can inductive inference be formalized? What are the caveats? Can inductive inference be automated? How does machine learning work?</p>
<blockquote>
<p>All knowledge is, in final analysis, history. All sciences are, in the abstract, mathematics. All judgements are, in their rationale, statistics.</p>
<p>&#x2013; C. R. Rao</p>
</blockquote>
<h3 class="unnumbered" id="contents">Contents</h3>
<ol type="1">
<li><a href="statistics.html#introduction-to-the-foundations-of-statistics">Introduction to the foundations of statistics</a>
<ol type="1">
<li><a href="statistics.html#problem-of-induction">Problem of induction</a></li>
<li><a href="statistics.html#early-investigators">Early investigators</a></li>
<li><a href="statistics.html#foundations-of-modern-statistics">Foundations of modern statistics</a></li>
<li><a href="statistics.html#pedagogy">Pedagogy</a></li>
</ol></li>
<li><a href="statistics.html#probability-and-its-related-concepts">Probability and its related concepts</a>
<ol type="1">
<li><a href="statistics.html#probability">Probability</a></li>
<li><a href="statistics.html#expectation-and-variance">Expectation and variance</a></li>
<li><a href="statistics.html#cross-entropy">Cross entropy</a></li>
<li><a href="statistics.html#uncertainty">Uncertainty</a></li>
<li><a href="statistics.html#bayes-theorem">Bayes&#x2019; theorem</a></li>
<li><a href="statistics.html#likelihood-and-frequentist-vs-bayesian-probability">Likelihood and frequentist vs bayesian probability</a></li>
<li><a href="statistics.html#curse-of-dimensionality">Curse of dimensionality</a></li>
</ol></li>
<li><a href="statistics.html#statistical-models">Statistical models</a>
<ol type="1">
<li><a href="statistics.html#parametric-models">Parametric models</a></li>
<li><a href="statistics.html#canonical-distributions">Canonical distributions</a></li>
<li><a href="statistics.html#mixture-models">Mixture models</a></li>
</ol></li>
<li><a href="statistics.html#point-estimation-and-confidence-intervals">Point estimation and confidence intervals</a>
<ol type="1">
<li><a href="statistics.html#inverse-problems">Inverse problems</a></li>
<li><a href="statistics.html#bias-and-variance">Bias and variance</a></li>
<li><a href="statistics.html#maximum-likelihood-estimation">Maximum likelihood estimation</a></li>
<li><a href="statistics.html#variance-of-mles">Variance of MLEs</a></li>
<li><a href="statistics.html#bayesian-credibility-intervals">Bayesian credibility intervals</a></li>
<li><a href="statistics.html#uncertainty-on-measuring-an-efficiency">Uncertainty on measuring an efficiency</a></li>
<li><a href="statistics.html#examples">Examples</a></li>
</ol></li>
<li><a href="statistics.html#statistical-hypothesis-testing">Statistical hypothesis testing</a>
<ol type="1">
<li><a href="statistics.html#null-hypothesis-significance-testing">Null hypothesis significance testing</a></li>
<li><a href="statistics.html#neyman-pearson-theory">Neyman-Pearson theory</a></li>
<li><a href="statistics.html#p-values-and-significance">p-values and significance</a></li>
<li><a href="statistics.html#asymptotics">Asymptotics</a></li>
<li><a href="statistics.html#students-t-test">Student&#x2019;s t-test</a></li>
<li><a href="statistics.html#frequentist-vs-bayesian-decision-theory">Frequentist vs bayesian decision theory</a></li>
<li><a href="statistics.html#examples-1">Examples</a></li>
</ol></li>
<li><a href="statistics.html#uncertainty-quantification">Uncertainty quantification</a>
<ol type="1">
<li><a href="statistics.html#sinervo-classification-of-systematic-uncertainties">Sinervo classification of systematic uncertainties</a></li>
<li><a href="statistics.html#profile-likelihoods">Profile likelihoods</a></li>
<li><a href="statistics.html#examples-of-poor-estimates-of-systematic-uncertanties">Examples of poor estimates of systematic uncertanties</a></li>
</ol></li>
<li><a href="statistics.html#statistical-classification">Statistical classification</a>
<ol type="1">
<li><a href="statistics.html#introduction-1">Introduction</a></li>
<li><a href="statistics.html#examples-2">Examples</a></li>
</ol></li>
<li><a href="statistics.html#causal-inference">Causal inference</a>
<ol type="1">
<li><a href="statistics.html#introduction-2">Introduction</a></li>
<li><a href="statistics.html#causal-models">Causal models</a></li>
<li><a href="statistics.html#counterfactuals">Counterfactuals</a></li>
</ol></li>
<li><a href="statistics.html#exploratory-data-analysis">Exploratory data analysis</a>
<ol type="1">
<li><a href="statistics.html#introduction-3">Introduction</a></li>
<li><a href="statistics.html#look-elsewhere-effect">Look-elsewhere effect</a></li>
<li><a href="statistics.html#archiving-and-data-science">Archiving and data science</a></li>
</ol></li>
<li><a href="statistics.html#statistics-wars">&#x201C;Statistics Wars&#x201D;</a>
<ol type="1">
<li><a href="statistics.html#introduction-4">Introduction</a></li>
<li><a href="statistics.html#likelihood-principle">Likelihood principle</a></li>
<li><a href="statistics.html#discussion">Discussion</a></li>
</ol></li>
<li><a href="statistics.html#replication-crisis">Replication crisis</a>
<ol type="1">
<li><a href="statistics.html#introduction-5">Introduction</a></li>
<li><a href="statistics.html#p-value-controversy">p-value controversy</a></li>
</ol></li>
<li><a href="statistics.html#classical-machine-learning">Classical machine learning</a>
<ol type="1">
<li><a href="statistics.html#introduction-6">Introduction</a></li>
<li><a href="statistics.html#history">History</a></li>
<li><a href="statistics.html#logistic-regression">Logistic regression</a></li>
<li><a href="statistics.html#softmax-regression">Softmax regression</a></li>
<li><a href="statistics.html#decision-trees">Decision trees</a></li>
<li><a href="statistics.html#clustering">Clustering</a></li>
</ol></li>
<li><a href="statistics.html#deep-learning">Deep learning</a>
<ol type="1">
<li><a href="statistics.html#introduction-7">Introduction</a></li>
<li><a href="statistics.html#deep-double-descent">Deep double descent</a></li>
<li><a href="statistics.html#regularization">Regularization</a></li>
<li><a href="statistics.html#batch-size-vs-learning-rate">Batch size vs learning rate</a></li>
<li><a href="statistics.html#normalization">Normalization</a></li>
<li><a href="statistics.html#computer-vision">Computer vision</a></li>
<li><a href="statistics.html#natural-language-processing">Natural language processing</a></li>
<li><a href="statistics.html#reinforcement-learning">Reinforcement learning</a></li>
<li><a href="statistics.html#applications-in-physics">Applications in physics</a></li>
</ol></li>
<li><a href="statistics.html#theoretical-machine-learning">Theoretical machine learning</a>
<ol type="1">
<li><a href="statistics.html#algorithmic-information-theory">Algorithmic information theory</a></li>
<li><a href="statistics.html#no-free-lunch-theorems">No free lunch theorems</a></li>
<li><a href="statistics.html#graphical-tensor-notation">Graphical tensor notation</a></li>
<li><a href="statistics.html#universal-approximation-theorem">Universal approximation theorem</a></li>
<li><a href="statistics.html#relationship-to-statistical-mechanics">Relationship to statistical mechanics</a></li>
<li><a href="statistics.html#relationship-to-gauge-theory">Relationship to gauge theory</a></li>
<li><a href="statistics.html#thermodynamics-of-computation">Thermodynamics of computation</a></li>
</ol></li>
<li><a href="statistics.html#information-geometry">Information geometry</a>
<ol type="1">
<li><a href="statistics.html#introduction-9">Introduction</a></li>
<li><a href="statistics.html#geometric-understanding-of-classical-statistics">Geometric understanding of classical statistics</a></li>
<li><a href="statistics.html#geometric-understanding-of-deep-learning">Geometric understanding of deep learning</a></li>
</ol></li>
<li><a href="statistics.html#automation">Automation</a>
<ol type="1">
<li><a href="statistics.html#automl">AutoML</a></li>
<li><a href="statistics.html#surrogate-models">Surrogate models</a></li>
<li><a href="statistics.html#autoscience">AutoScience</a></li>
</ol></li>
<li><a href="statistics.html#implications-for-the-realism-debate">Implications for the realism debate</a>
<ol type="1">
<li><a href="statistics.html#introduction-10">Introduction</a></li>
<li><a href="statistics.html#real-clusters">Real clusters</a></li>
<li><a href="statistics.html#word-meanings">Word meanings</a></li>
</ol></li>
<li><a href="statistics.html#my-thoughts">My thoughts</a></li>
<li><a href="statistics.html#annotated-bibliography">Annotated bibliography</a>
<ol type="1">
<li><a href="statistics.html#mayo-d.g.-1996.-error-and-the-growth-of-experimental-knowledge.">Mayo, D.G. (1996). Error and the Growth of Experimental Knowledge.</a></li>
<li><a href="statistics.html#cowan-g.-1998.-statistical-data-analysis.">Cowan, G. (1998). Statistical Data Analysis.</a></li>
<li><a href="statistics.html#james-f.-2006.-statistical-methods-in-experimental-physics.">James, F. (2006). Statistical Methods in Experimental Physics.</a></li>
<li><a href="statistics.html#cowan-g.-et-al.-2011.-asymptotic-formulae-for-likelihood-based-tests-of-new-physics.">Cowan, G. et al.&#xA0;(2011). Asymptotic formulae for likelihood-based tests of new physics.</a></li>
<li><a href="statistics.html#atlas-collaboration.-2012.-combined-search-for-the-standard-model-higgs-boson.">ATLAS Collaboration. (2012). Combined search for the Standard Model Higgs boson.</a></li>
<li><a href="statistics.html#cranmer-k.-2015.-practical-statistics-for-the-lhc.">Cranmer, K. (2015). Practical statistics for the LHC.</a></li>
<li><a href="statistics.html#more-articles-to-do">More articles to do</a></li>
</ol></li>
<li><a href="statistics.html#links-and-encyclopedia-articles">Links and encyclopedia articles</a>
<ol type="1">
<li><a href="statistics.html#sep">SEP</a></li>
<li><a href="statistics.html#iep">IEP</a></li>
<li><a href="statistics.html#scholarpedia">Scholarpedia</a></li>
<li><a href="statistics.html#wikipedia">Wikipedia</a></li>
<li><a href="statistics.html#others">Others</a></li>
</ol></li>
<li><a href="statistics.html#references">References</a></li>
</ol>
<h2 id="introduction-to-the-foundations-of-statistics">Introduction to the foundations of statistics</h2>
<h3 id="problem-of-induction">Problem of induction</h3>
<p>A key issue for the scientific method, as discussed in the <a href="scientific-method.html#induction">previous outline</a>, is the <a href="scientific-method.html#induction">problem of induction</a>. Inductive inferences are used in the scientific method to make generalizations from finite data. This introduces unique avenues of error not found in purely deductive inferences, like in logic and mathematics. Compared to deductive inferences, which are sound and necessarily follow if an argument is valid and all of its premises obtain, inductive inferences can be valid and probably (not certainly) sound, and therefore can still result in error in some cases because the support of the argument is ultimately probabilistic.</p>
<p>A skeptic may further probe if we are even justified in using the probabilities we use in inductive arguments. What is the probability the Sun will rise tomorrow? What kind of probabilities are reasonable?</p>
<p>In this outline, we sketch and explore how the mathematical theory of statistics has arisen to wrestle with the problem of induction, and how it equips us with careful ways of framing inductive arguments and notions of confidence in them.</p>
<p>See also:</p>
<ul>
<li><a href="scientific-method.html#statistics-as-a-solution-to-the-problem-of-induction">Statistics as a solution to the problem of induction</a></li>
</ul>
<h3 id="early-investigators">Early investigators</h3>
<ul>
<li>&#x201C;Ibn al-Haytham was an early proponent of the concept that a hypothesis must be supported by experiments based on confirmable procedures or mathematical evidence&#x2014;an early pioneer in the scientific method five centuries before Renaissance scientists.&#x201D; - Wikipedia</li>
<li><a href="https://en.wikipedia.org/wiki/Gerolamo_Cardano">Gerolamo Cardano</a> (1501-1576)
<ul>
<li><em>Book on Games of Chance</em> (1564)</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/John_Graunt">John Graunt</a> (1620-1674)</li>
<li><a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Jacob Bernoulli</a> (1655-1705)
<ul>
<li><em>Ars Conjectandi</em> (1713, posthumous)</li>
<li>First modern phrasing of the problem of parameter estimation<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></li>
<li>See Hacking<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></li>
<li>Early vision of decision theory:</li>
</ul></li>
</ul>
<blockquote>
<p>The art of measuring, as precisely as possible, probabilities of things, with the goal that we would be able always to choose or follow in our judgments and actions that course, which will have been determined to be better, more satisfactory, safer or more advantageous.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p>
</blockquote>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Thomas Bayes</a> (1701-1761)</li>
<li><a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Pierre-Simon Laplace</a> (1749-1827)
<ul>
<li>The rule of succession, bayesian</li>
</ul></li>
<li><a href="http://en.wikipedia.org/wiki/Gauss">Carl Friedrich Gauss</a> (1777-1855)</li>
<li><a href="https://en.wikipedia.org/wiki/John_Stuart_Mill">John Stuart Mill</a> (1806-1873)</li>
<li><a href="https://en.wikipedia.org/wiki/Francis_Galton">Francis Galton</a> (1822-1911)
<ul>
<li>Regression towards the mean in phenotypes</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/John_Venn">John Venn</a> (1834-1923)
<ul>
<li><em>The Logic of Chance</em> (1866)<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></li>
</ul></li>
</ul>
<h3 id="foundations-of-modern-statistics">Foundations of modern statistics</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Central_limit_theorem">Central limit theorem</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem">De Moivre-Laplace theorem</a> (1738)</li>
<li><a href="https://en.wikipedia.org/wiki/Glivenko%E2%80%93Cantelli_theorem">Glivenko-Cantelli theorem</a> (1933)</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Charles_Sanders_Peirce">Charles Sanders Peirce</a> (1839-1914)
<ul>
<li>Formulated modern statistics in &#x201C;Illustrations of the Logic of Science,&#x201D; a series published in <em>Popular Science Monthly</em> (1877-1878), and also &#x201C;A Theory of Probable Inference&#x201D; in <em>Studies in Logic</em> (1883).<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
<li>With a repeated measures design, introduced blinded, controlled randomized experiments (before Fisher).</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Karl Pearson</a> (1857-1936)
<ul>
<li><a href="https://en.wikipedia.org/wiki/The_Grammar_of_Science"><em>The Grammar of Science</em></a> (1892)</li>
<li>&#x201C;On the criterion that a given system of deviations&#x2026;&#x201D; (1900)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a>
<ul>
<li>Proposed testing the validity of hypothesized values by evaluating the chi distance between the hypothesized and the empirically observed values via the <span class="math inline">\(p\)</span>-value.</li>
</ul></li>
<li>With Frank Raphael Weldon, he established the journal <em>Biometrika</em> in 1902.</li>
<li>Founded the world&#x2019;s first university statistics department at University College, London in 1911.</li>
</ul></li>
<li>John Maynard Keynes (1883-1946)
<ul>
<li>Keynes, J. M. (1921). <em>A Treatise on Probability</em>.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Ronald Fisher</a> (1890-1972)
<ul>
<li>Fisher significance of the null hypothesis (<span class="math inline">\(p\)</span>-values)
<ul>
<li>&#x201C;On an absolute criterion for fitting frequency curves&#x201D;<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a></li>
<li>&#x201C;Frequency distribution of the values of the correlation coefficient in samples of indefinitely large population&#x201D;<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a></li>
</ul></li>
<li>&#x201C;On the &#x2018;probable error&#x2019; of a coefficient of correlation deduced from a small sample&#x201D;<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a>
<ul>
<li>Definition of <em>likelihood</em></li>
<li>ANOVA</li>
</ul></li>
<li><em>Statistical Methods for Research Workers</em> (1925)</li>
<li><em>The Design of Experiments</em> (1935)</li>
<li>&#x201C;Statistical methods and scientific induction&#x201D;<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></li>
<li><em>The Lady Tasting Tea</em><a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Jerzy Neyman</a> (1894-1981)
<ul>
<li>biography by Reid<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></li>
<li>Neyman, J. (1955). <a href="https://errorstatistics.files.wordpress.com/2017/04/neyman-1955-the-problem-of-inductive-inference-searchable.pdf">The problem of inductive inference</a>.<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a>
<ul>
<li>Shows that Neyman read Carnap, but did Carnap read Neyman?</li>
<li>Discussion: Mayo, D.G. (2014). <a href="https://errorstatistics.com/2014/03/19/power-taboos-statue-of-liberty-senn-neyman-carnap-severity/">Power taboos: Statue of Liberty, Senn, Neyman, Carnap, Severity</a>.</li>
</ul></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Egon_Pearson">Egon Pearson</a> (1895-1980)
<ul>
<li>Neyman-Pearson confidence intervals with fixed error probabilities (also <span class="math inline">\(p\)</span>-values but considering two hypotheses involves two types of errors)</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Harold Jeffreys</a> (1891-1989)
<ul>
<li>objective (non-informative) Jeffreys priors</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Andrey Kolmogorov</a> (1903-1987)</li>
<li><a href="https://en.wikipedia.org/wiki/C._R._Rao">C.R. Rao</a> (1920-2023)</li>
<li><a href="https://en.wikipedia.org/wiki/Ray_Solomonoff">Ray Solomonoff</a> (1926-2009)</li>
<li><a href="https://en.wikipedia.org/wiki/Shun%27ichi_Amari">Shun&#x2019;ichi Amari</a> (b. 1936)</li>
<li><a href="https://en.wikipedia.org/wiki/Judea_Pearl">Judea Pearl</a> (b. 1936)</li>
</ul>
<h3 id="pedagogy">Pedagogy</h3>
<ul>
<li>Kendall<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></li>
<li>James<a href="#fn16" class="footnote-ref" id="fnref16" role="doc-noteref"><sup>16</sup></a></li>
<li>Cowan<a href="#fn17" class="footnote-ref" id="fnref17" role="doc-noteref"><sup>17</sup></a></li>
<li>Cranmer<a href="#fn18" class="footnote-ref" id="fnref18" role="doc-noteref"><sup>18</sup></a></li>
<li>Lista: book,<a href="#fn19" class="footnote-ref" id="fnref19" role="doc-noteref"><sup>19</sup></a> notes<a href="#fn20" class="footnote-ref" id="fnref20" role="doc-noteref"><sup>20</sup></a></li>
<li>Cox<a href="#fn21" class="footnote-ref" id="fnref21" role="doc-noteref"><sup>21</sup></a></li>
<li>Behnke, O., Kr&#xF6;ninger, K., Schott, G., &amp; Sch&#xF6;rner-Sadenius, T. (2013). <em>Data Analysis in High Energy Physics: A Practical Guide to Statistical Methods</em>.<a href="#fn22" class="footnote-ref" id="fnref22" role="doc-noteref"><sup>22</sup></a></li>
<li>Cousins<a href="#fn23" class="footnote-ref" id="fnref23" role="doc-noteref"><sup>23</sup></a></li>
<li>Weisberg<a href="#fn24" class="footnote-ref" id="fnref24" role="doc-noteref"><sup>24</sup></a></li>
<li>Cranmer, K. (2020). <a href="https://cranmer.github.io/stats-ds-book/intro.html"><em>Statistics and Data Science</em></a>.</li>
<li>Cosma Shalizi&#x2019;s notes on
<ul>
<li><a href="http://bactra.org/notebooks/probability.html">probability theory</a></li>
<li><a href="http://bactra.org/notebooks/statistics.html">statistics</a></li>
<li><a href="http://bactra.org/notebooks/teaching-statistics.html">teaching statistics</a></li>
<li><a href="http://bactra.org/notebooks/info-geo.html">information geometry</a></li>
<li><a href="https://www.stat.cmu.edu/~cshalizi/ADAfaEPoV/"><em>Advanced Data Analysis from an Elementary Point of View</em></a></li>
</ul></li>
<li>Gelman, A. &amp; Vehtari, A. (2021). <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1938081">What are the most important statistical ideas of the past 50 years?</a><a href="#fn25" class="footnote-ref" id="fnref25" role="doc-noteref"><sup>25</sup></a></li>
<li>Taboga, M. (2022). <a href="https://www.statlect.com/">statlect.com</a>.</li>
<li>Otsuka, J. (2023). <em>Thinking About Statistics: The Philosophical Foundations</em>.<a href="#fn26" class="footnote-ref" id="fnref26" role="doc-noteref"><sup>26</sup></a>
<ul>
<li>Otsuka, J. (2023). Talk: <a href="https://www.youtube.com/watch?v=Lqt7TgYk8rU">What machine learning tells us about the mathematical structures of concepts</a>.</li>
</ul></li>
</ul>
<h2 id="probability-and-its-related-concepts">Probability and its related concepts</h2>
<h3 id="probability">Probability</h3>
<p>Probability is of epistemic interest, being in some sense a measure of inductive confidence.</p>
<p>TODO:</p>
<ul>
<li>Kolmogorov axioms</li>
<li>Probability vs odds: <span class="math inline">\(p/(p+q)\)</span> vs <span class="math inline">\(p/q\)</span></li>
<li>Carnap, R. (1947). Probability as a guide in life.<a href="#fn27" class="footnote-ref" id="fnref27" role="doc-noteref"><sup>27</sup></a></li>
<li>Carnap, R. (1953). <a href="https://www.jstor.org/stable/24944342">What is probability?</a>.<a href="#fn28" class="footnote-ref" id="fnref28" role="doc-noteref"><sup>28</sup></a></li>
</ul>
<h3 id="expectation-and-variance">Expectation and variance</h3>
<p>Expectation:</p>
<p><span class="math display">\[ \mathbb{E}(y) \equiv \int dx \: p(x) \: y(x) \label{eq:expectation} \]</span></p>
<p>Expectation values can be approximated with a partial sum over some data or Monte Carlo sample:</p>
<p><span class="math display">\[ \mathbb{E}(y) \approx \frac{1}{n} \sum_s^n y(x_s) \label{eq:expectation_sum} \]</span></p>
<p>The variance of a random variable, <span class="math inline">\(y\)</span>, is defined as</p>
<p><span class="math display">\[\begin{align}
    \mathrm{Var}(y) &amp;\equiv \mathbb{E}((y - \mathbb{E}(y))^2) \nonumber \\
    &amp;= \mathbb{E}(y^2 - 2 \: y \: \mathbb{E}(y) + \mathbb{E}(y)^2) \nonumber \\
    &amp;= \mathbb{E}(y^2) - 2 \: \mathbb{E}(y) \: \mathbb{E}(y) + \mathbb{E}(y)^2 \nonumber \\
    &amp;= \mathbb{E}(y^2) - \mathbb{E}(y)^2 \label{eq:variance}
\end{align}\]</span></p>
<p>The covariance matrix, <span class="math inline">\(\boldsymbol{V}\)</span>, of random variables <span class="math inline">\(x_i\)</span> is</p>
<p><span class="math display">\[\begin{align}
    V_{ij} &amp;= \mathrm{Cov}(x_i, x_j) \equiv \mathbb{E}[(x_i - \mathbb{E}(x_i)) \: (x_j - \mathbb{E}(x_j))] \nonumber \\
           &amp;= \mathbb{E}(x_i \: x_{j} - \mu_i \: x_j - x_i \: \mu_j + \mu_i \: \mu_j ) \nonumber \\
           &amp;= \mathbb{E}(x_i \: x_{j}) - \mu_i \: \mu_j \label{eq:covariance_matrix_indexed}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{equation}
\boldsymbol{V} = 
\begin{pmatrix}
    \mathrm{Var}(x_1) &amp; \mathrm{Cov}(x_1, x_2) &amp; \cdots &amp; \mathrm{Cov}(x_1, x_n) \\
    \mathrm{Cov}(x_2, x_1) &amp; \mathrm{Var}(x_2) &amp; \cdots &amp; \mathrm{Cov}(x_2, x_n) \\
    \vdots &amp; \vdots &amp; \ddots &amp;  \vdots \\
    \mathrm{Cov}(x_n, x_1) &amp; \mathrm{Cov}(x_n, x_2) &amp; \cdots &amp; \mathrm{Var}(x_n)
\end{pmatrix}
\label{eq:covariance_matrix_array}
\end{equation}\]</span></p>
<p>Diagonal elements of the covariance matrix are the variances of each variable.</p>
<p><span class="math display">\[ \mathrm{Cov}(x_i, x_i) = \mathrm{Var}(x_i) \]</span></p>
<p>Off-diagonal elements of a covariance matrix measure how related two variables are, linearly. Covariance can be normalized to give the correlation coefficient between variables:</p>
<p><span class="math display">\[ \mathrm{Cor}(x_i, x_j) \equiv \frac{ \mathrm{Cov}(x_i, x_j) }{ \sqrt{ \mathrm{Var}(x_i) \: \mathrm{Var}(x_j)  }  } \label{eq:correlation_matrix} \]</span></p>
<p>which is bounded: <span class="math inline">\(-1 \leq \mathrm{Cor}(x_i, x_j) \leq 1\)</span>.</p>
<p>The covariance of two random vectors is given by</p>
<p><span class="math display">\[ \boldsymbol{V} = \mathrm{Cov}(\vec{x}, \vec{y}) = \mathbb{E}(\vec{x} \: \vec{y}^{\mathsf{T}}) - \vec{\mu}_x \: \vec{\mu}_{y}^{\mathsf{T}}\label{eq:covariance_matrix_vectors} \]</span></p>
<h3 id="cross-entropy">Cross entropy</h3>
<p>TODO: discuss the Shannon entropy and Kullback-Leibler (KL) divergence.<a href="#fn29" class="footnote-ref" id="fnref29" role="doc-noteref"><sup>29</sup></a></p>
<p>Shannon entropy:</p>
<p><span class="math display">\[ H(p) = - \underset{x\sim{}p}{\mathbb{E}}\big[ \log p(x) \big] \label{eq:shannon_entropy} \]</span></p>
<p>Cross entropy:</p>
<p><span class="math display">\[ H(p, q) = - \underset{x\sim{}p}{\mathbb{E}}\big[ \log q(x) \big] = - \sum_{x} p(x) \: \log q(x) \label{eq:cross_entropy} \]</span></p>
<p>Kullback-Leibler (KL) divergence:</p>
<p><span class="math display">\[\begin{align}
D_\mathrm{KL}(p, q)
    &amp;= \underset{x\sim{}p}{\mathbb{E}}\left[ \log \left(\frac{p(x)}{q(x)}\right) \right] = \underset{x\sim{}p}{\mathbb{E}}\big[ \log p(x) - \log q(x) \big] \label{eq:kl_divergence} \\
    &amp;= - H(p) + H(p, q) \\
\end{align}\]</span></p>
<p>See also the section on <a href="#logistic-regression">logistic regression</a>.</p>
<h3 id="uncertainty">Uncertainty</h3>
<h4 id="quantiles-and-standard-error">Quantiles and standard error</h4>
<p>TODO:</p>
<ul>
<li>Quantiles</li>
<li>Practice of standard error for uncertainty quantification.</li>
</ul>
<h4 id="propagation-of-error">Propagation of error</h4>
<p>Given some vector of random variables, <span class="math inline">\(\vec{x}\)</span>, with estimated means, <span class="math inline">\(\vec{\mu}\)</span>, and estimated covariance matrix, <span class="math inline">\(\boldsymbol{V}\)</span>, suppose we are concerned with estimating the variance of some variable, <span class="math inline">\(y\)</span>, that is a function of <span class="math inline">\(\vec{x}\)</span>. The variance of <span class="math inline">\(y\)</span> is given by</p>
<p><span class="math display">\[ \sigma^2_y   = \mathbb{E}(y^2) - \mathbb{E}(y)^2 \,. \]</span></p>
<p>Taylor expanding <span class="math inline">\(y(\vec{x})\)</span> about <span class="math inline">\(x=\mu\)</span> gives</p>
<p><span class="math display">\[ y(\vec{x}) \approx y(\vec{\mu}) + \left.\frac{\partial y}{\partial x_i}\right|_{\vec{x}=\vec{\mu}} (x_i - \mu_i) \,. \]</span></p>
<p>Therefore, to first order</p>
<p><span class="math display">\[ \mathbb{E}(y) \approx y(\vec{\mu}) \]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
\mathbb{E}(y^2)
    &amp;\approx y^2(\vec{\mu}) + 2 \, y(\vec{\mu}) \, \left.\frac{\partial y}{\partial x_i}\right|_{\vec{x}=\vec{\mu}} \mathbb{E}(x_i - \mu_i) \nonumber \\
    &amp;+ \mathbb{E}\left[ \left(\left.\frac{\partial y}{\partial x_i}\right|_{\vec{x}=\vec{\mu}}(x_i - \mu_i)\right) \left(\left.\frac{\partial y}{\partial x_j}\right|_{\vec{x}=\vec{\mu}}(x_j - \mu_j)\right) \right] \\
    &amp;= y^2(\vec{\mu}) + \, \left.\frac{\partial y}{\partial x_i}\frac{\partial y}{\partial x_j}\right|_{\vec{x}=\vec{\mu}} V_{ij} \\
\end{align}\]</span></p>
<p>TODO: clarify above, then specific examples.</p>
<p>See:</p>
<ul>
<li>Cowan.<a href="#fn30" class="footnote-ref" id="fnref30" role="doc-noteref"><sup>30</sup></a></li>
<li>Arras, K.O. (1998). <a href="http://srl.informatik.uni-freiburg.de/papers/arrasTR98.pdf">An introduction to error propagation: Derivation, meaning and examples of <span class="math inline">\(C_y= F_x C_x F_{x}^{\top}\)</span></a>.<a href="#fn31" class="footnote-ref" id="fnref31" role="doc-noteref"><sup>31</sup></a></li>
</ul>
<h3 id="bayes-theorem">Bayes&#x2019; theorem</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes, Thomas</a> (1701-1761)</li>
<li>Bayes&#x2019; theorem</li>
</ul>
<p><span class="math display">\[ P(A|B) = P(B|A) \: P(A) \: / \: P(B) \label{eq:bayes_theorem} \]</span></p>
<ul>
<li>Extended version of Bayes theorem</li>
<li>Example of conditioning with medical diagnostics</li>
</ul>
<h3 id="likelihood-and-frequentist-vs-bayesian-probability">Likelihood and frequentist vs bayesian probability</h3>
<ul>
<li>Frequentist vs bayesian probability</li>
<li>Frequentism grew out of theories of statistical sampling error.</li>
<li>Bayesianism grew out of what used to be called &#x201C;inverse probability.&#x201D;
<ul>
<li>Fienberg, S.E. (2006). <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-1/When-did-Bayesian-inference-become-Bayesian/10.1214/06-BA101.full">When did Bayesian inference become &#x201C;Bayesian?&#x201D;</a><a href="#fn32" class="footnote-ref" id="fnref32" role="doc-noteref"><sup>32</sup></a></li>
</ul></li>
<li>Weisberg: <a href="https://jonathanweisberg.org/vip/two-schools.html">&#x201C;Two Schools&#x201D;</a><a href="#fn33" class="footnote-ref" id="fnref33" role="doc-noteref"><sup>33</sup></a></li>
</ul>
<p><span class="math display">\[ P(H|D) = P(D|H) \: P(H) \: / \: P(D) \label{eq:bayes_theorem_hd} \]</span></p>
<ul>
<li>Likelihood</li>
</ul>
<p><span class="math display">\[ L(\theta) = P(D|\theta) \label{eq:likelihood_def_x} \]</span></p>
<ul>
<li>We will return to the frequentist vs bayesian debate in the section on the <a href="#statistics-wars">&#x201C;Statistics Wars&#x201D;</a>.</li>
</ul>
<p>Fisher:</p>
<blockquote>
<p>To appeal to such a result is absurd. Bayes&#x2019; theorem ought only to be used where we have in past experience, as for example in the case of probabilities and other statistical ratios, met with every admissible value with roughly equal frequency. There is no such experience in this case.<a href="#fn34" class="footnote-ref" id="fnref34" role="doc-noteref"><sup>34</sup></a></p>
</blockquote>
<h3 id="curse-of-dimensionality">Curse of dimensionality</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Curse_of_dimensionality">Curse of dimensionality</a>
<ul>
<li>The volume of the space increases so fast that the available data become sparse.</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Stein%27s_example">Stein&#x2019;s paradox</a>
<ul>
<li>The ordinary decision rule for estimating the mean of a multivariate Gaussian distribution (with dimensions, <span class="math inline">\(n \geq 3\)</span>) is inadmissible under mean squared error risk.</li>
<li>Stein, C. (1956). Inadmissibility of the usual estimator for the mean of a multivariate normal distribution.<a href="#fn35" class="footnote-ref" id="fnref35" role="doc-noteref"><sup>35</sup></a></li>
<li>James, W. &amp; Stein, C. (1961). <a href="https://projecteuclid.org/accountAjax/Download?urlId=bsmsp%2F1200512173&amp;downloadType=presschapter&amp;isResultClick=True">Estimation with quadratic loss</a>.<a href="#fn36" class="footnote-ref" id="fnref36" role="doc-noteref"><sup>36</sup></a></li>
<li><a href="https://en.wikipedia.org/wiki/Proof_of_Stein%27s_example">Proof of Stein&#x2019;s example</a></li>
</ul></li>
<li>van Handel, R. (2016). <a href="https://web.math.princeton.edu/~rvan/APC550.pdf">Probability in high dimensions</a>.<a href="#fn37" class="footnote-ref" id="fnref37" role="doc-noteref"><sup>37</sup></a></li>
<li>Vershynin, R. (2018). <em>High-Dimensional Probability: An Introduction with Applications in Data Dcience</em>.<a href="#fn38" class="footnote-ref" id="fnref38" role="doc-noteref"><sup>38</sup></a></li>
</ul>
<h2 id="statistical-models">Statistical models</h2>
<h3 id="parametric-models">Parametric models</h3>
<ul>
<li>Data: <span class="math inline">\(x_i\)</span></li>
<li>Parameters: <span class="math inline">\(\theta_j\)</span></li>
<li>Model: <span class="math inline">\(f(\vec{x} ; \vec{\theta})\)</span></li>
</ul>
<h3 id="canonical-distributions">Canonical distributions</h3>
<h4 id="bernoulli-distribution">Bernoulli distribution</h4>
<p><span class="math display">\[ \mathrm{Ber}(k; p) = \begin{cases} p &amp; \mathrm{if}\ k = 1 \\ 1-p &amp; \mathrm{if}\ k = 0 \end{cases} \label{eq:bernoulli} \]</span></p>
<p>which can also be written as</p>
<p><span class="math display">\[ \mathrm{Ber}(k; p) = p^k \: (1-p)^{(1-k)} \quad \mathrm{for}\ k \in \{0, 1\} \]</span></p>
<p>or</p>
<p><span class="math display">\[ \mathrm{Ber}(k; p) = p k + (1-p)(1-k) \quad \mathrm{for}\ k \in \{0, 1\} \]</span></p>
<ul>
<li>Binomial distribution</li>
<li>Poisson distribution</li>
</ul>
<p>TODO: explain, another important relationship is</p>
<figure>
<img src="img/bernoulli-binomial-multinomial.png" id="fig:bernoulli-binomial-multinomial" alt="Figure 1: Relationships among Bernoulli, binomial, categorical, and multinomial distributions." /><figcaption aria-hidden="true">Figure 1: Relationships among Bernoulli, binomial, categorical, and multinomial distributions.</figcaption>
</figure>
<h4 id="normalgaussian-distribution">Normal/Gaussian distribution</h4>
<p><span class="math display">\[ N(x \,|\, \mu, \sigma^2) = \frac{1}{\sqrt{2\,\pi\:\sigma^2}} \: \exp\left(\frac{-(x-\mu)^2}{2\,\sigma^2}\right) \label{eq:gaussian} \]</span></p>
<p>and in <span class="math inline">\(k\)</span> dimensions:</p>
<p><span class="math display">\[ N(\vec{x} \,|\, \vec{\mu}, \boldsymbol{\Sigma}) = (2 \pi)^{-k/2}\:\left|\boldsymbol{\Sigma}\right|^{-1/2} \: \exp\left(\frac{-1}{2}\:(\vec{x}-\vec{\mu})^{\mathsf{T}}\:\boldsymbol{\Sigma}^{-1}\:(\vec{x}-\vec{\mu})\right) \label{eq:gaussian_k_dim} \]</span></p>
<p>where <span class="math inline">\(\boldsymbol{\Sigma}\)</span> is the covariance matrix (defined in eq.&#xA0;<span class="math inline">\(\eqref{eq:covariance_matrix_indexed}\)</span>) of the distribution.</p>
<ul>
<li>Central limit theorem</li>
<li><span class="math inline">\(\chi^2\)</span> distribution</li>
<li>Univariate distribution relationships</li>
</ul>
<figure>
<img src="img/Leemis-univariate-distribution-relationships.png" id="fig:Leemis-univariate-distribution-relationships" alt="Figure 2: Detail of a figure showing relationships among univariate distributions. See the full figure here." /><figcaption aria-hidden="true">Figure 2: Detail of a figure showing relationships among univariate distributions. See the <a href="http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf">full figure here</a>.<a href="#fn39" class="footnote-ref" id="fnref39" role="doc-noteref"><sup>39</sup></a></figcaption>
</figure>
<ul>
<li>The exponential family of distributions are maximum entropy distributions.</li>
</ul>
<h3 id="mixture-models">Mixture models</h3>
<ul>
<li>Gaussian mixture models (GMM)</li>
<li>Marked poisson
<ul>
<li><a href="https://scikit-hep.org/pyhf/intro.html">pyhf model description</a></li>
<li>HistFactory<a href="#fn40" class="footnote-ref" id="fnref40" role="doc-noteref"><sup>40</sup></a></li>
</ul></li>
</ul>
<h2 id="point-estimation-and-confidence-intervals">Point estimation and confidence intervals</h2>
<h3 id="inverse-problems">Inverse problems</h3>
<p>Recall that in the context of parametric models of data, <span class="math inline">\(x_i\)</span> the pdf of which is modeled by a function, <span class="math inline">\(f(x_i ; \theta_j)\)</span> with parameters, <span class="math inline">\(\theta_j\)</span>. In a statistical <a href="https://en.wikipedia.org/wiki/Inverse_problem">inverse problem</a>, the goal is to infer values of the model parameters, <span class="math inline">\(\theta_j\)</span> given some finite set of data, <span class="math inline">\(\{x_i\}\)</span> sampled from a probability density, <span class="math inline">\(f(x_i; \theta_j)\)</span> that models the data reasonably well.<a href="#fn41" class="footnote-ref" id="fnref41" role="doc-noteref"><sup>41</sup></a></p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Inverse_problem">Inverse problem</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Inverse_probability">Inverse probability</a> (Fisher)</li>
<li><a href="https://en.wikipedia.org/wiki/Statistical_inference">Statistical inference</a></li>
<li>See also: <a href="scientific-realism.html#structural-realism">Structural realism</a></li>
</ul></li>
<li>Estimators</li>
<li>Regression</li>
<li>Accuracy vs precision<a href="#fn42" class="footnote-ref" id="fnref42" role="doc-noteref"><sup>42</sup></a></li>
</ul>
<h3 id="bias-and-variance">Bias and variance</h3>
<p>The bias of an estimator, <span class="math inline">\(\hat\theta\)</span>, is defined as</p>
<p><span class="math display">\[ \mathrm{Bias}(\hat{\theta}) \equiv \mathbb{E}(\hat{\theta} - \theta) = \int dx \: P(x|\theta) \: (\hat{\theta} - \theta) \label{eq:bias} \]</span></p>
<p>The mean squared error (MSE) of an estimator has a similar formula to variance (eq.&#xA0;<span class="math inline">\(\eqref{eq:variance}\)</span>) except that instead of quantifying the square of the difference of the estimator and its expected value, the MSE uses the square of the difference of the estimator and the true parameter:</p>
<p><span class="math display">\[ \mathrm{MSE}(\hat{\theta}) \equiv \mathbb{E}((\hat{\theta} - \theta)^2) \label{eq:mse} \]</span></p>
<p>The MSE of an estimator can be related to its bias and its variance by the following proof:</p>
<p><span class="math display">\[\begin{align}
    \mathrm{MSE}(\hat{\theta}) &amp;= \mathbb{E}(\hat{\theta}^2 - 2 \: \hat{\theta} \: \theta + \theta^2) \nonumber \\
    &amp;= \mathbb{E}(\hat{\theta}^2) - 2 \: \mathbb{E}(\hat{\theta}) \: \theta + \theta^2
\end{align}\]</span></p>
<p>noting that</p>
<p><span class="math display">\[ \mathrm{Var}(\hat{\theta}) = \mathbb{E}(\hat{\theta}^2) - \mathbb{E}(\hat{\theta})^2 \]</span></p>
<p>and</p>
<p><span class="math display">\[\begin{align}
    \mathrm{Bias}(\hat{\theta})^2 &amp;= \mathbb{E}(\hat{\theta} - \theta)^2 \nonumber \\
    &amp;= \mathbb{E}(\hat{\theta})^2 - 2 \: \mathbb{E}(\hat{\theta}) \: \theta + \theta^2
\end{align}\]</span></p>
<p>we see that MSE is equivalent to</p>
<p><span class="math display">\[ \mathrm{MSE}(\hat{\theta}) = \mathrm{Var}(\hat{\theta}) + \mathrm{Bias}(\hat{\theta})^2 \label{eq:mse_variance_bias} \]</span></p>
<p>For an unbiased estimator, the MSE is the variance of the estimator.</p>
<p>TODO:</p>
<ul>
<li>Note the discussion of the bias-variance tradeoff by <a href="http://theoryandpractice.org/stats-ds-book/statistics/bias-variance.html">Cranmer</a>.</li>
<li>Note the new deep learning view. See <a href="#deep-learning">Deep learning</a>.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="#deep-double-descent">Deep double descent</a></li>
</ul>
<h3 id="maximum-likelihood-estimation">Maximum likelihood estimation</h3>
<p>A maximum likelihood estimator (MLE) was first used by Fisher.<a href="#fn43" class="footnote-ref" id="fnref43" role="doc-noteref"><sup>43</sup></a></p>
<p><span class="math display">\[\hat{\theta} \equiv \underset{\theta}{\mathrm{argmax}} \: \mathrm{log} \: L(\theta) \label{eq:mle} \]</span></p>
<p>Maximizing <span class="math inline">\(\mathrm{log} \: L(\theta)\)</span> is equivalent to maximizing <span class="math inline">\(L(\theta)\)</span>, and the former is more convenient because for data that are independent and identically distributed (<em>i.i.d.</em>) the joint likelihood can be factored into a product of individual measurements:</p>
<p><span class="math display">\[ L(\theta) = \prod_i L(\theta|x_i) = \prod_i P(x_i|\theta) \]</span></p>
<p>and taking the log of the product makes it a sum:</p>
<p><span class="math display">\[ \mathrm{log} \: L(\theta) = \sum_i \mathrm{log} \: L(\theta|x_i) = \sum_i \mathrm{log} \: P(x_i|\theta) \]</span></p>
<p>Maximizing <span class="math inline">\(\mathrm{log} \: L(\theta)\)</span> is also equivalent to minimizing <span class="math inline">\(-\mathrm{log} \: L(\theta)\)</span>, the negative log-likelihood (NLL). For distributions that are <em>i.i.d.</em>,</p>
<p><span class="math display">\[ \mathrm{NLL} \equiv - \log L = - \log \prod_i L_i = - \sum_i \log L_i = \sum_i \mathrm{NLL}_i \]</span></p>
<h4 id="invariance-of-likelihoods-under-reparametrization">Invariance of likelihoods under reparametrization</h4>
<ul>
<li>Likelihoods are invariant under reparametrization.<a href="#fn44" class="footnote-ref" id="fnref44" role="doc-noteref"><sup>44</sup></a></li>
<li>Bayesian posteriors are not invariant in general.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="#bayesian-credibility-intervals">Bayesian credibility intervals</a></li>
</ul>
<h4 id="ordinary-least-squares">Ordinary least squares</h4>
<ul>
<li>Least squares from MLE of gaussian models: <span class="math inline">\(\chi^2\)</span></li>
<li>Ordinary Least Squares (OLS)</li>
<li>Geometric interpretation
<ul>
<li>Cox<a href="#fn45" class="footnote-ref" id="fnref45" role="doc-noteref"><sup>45</sup></a></li>
<li>Murphy<a href="#fn46" class="footnote-ref" id="fnref46" role="doc-noteref"><sup>46</sup></a></li>
</ul></li>
</ul>
<h3 id="variance-of-mles">Variance of MLEs</h3>
<ul>
<li>Taylor expansion of a likelihood near its maximum</li>
<li>Cram&#xE9;r-Rao bound<a href="#fn47" class="footnote-ref" id="fnref47" role="doc-noteref"><sup>47</sup></a>
<ul>
<li>Define efficiency of an estimator.</li>
<li>Common formula for variance of unbiased and efficient estimators</li>
<li>Proof in Rice<a href="#fn48" class="footnote-ref" id="fnref48" role="doc-noteref"><sup>48</sup></a></li>
<li>Cranmer: <a href="http://theoryandpractice.org/stats-ds-book/statistics/cramer-rao-bound.html">Cram&#xE9;r-Rao bound</a></li>
<li>Nielsen, F. (2013). <a href="https://arxiv.org/abs/1301.3578">Cramer-Rao lower bound and information geometry</a>.<a href="#fn49" class="footnote-ref" id="fnref49" role="doc-noteref"><sup>49</sup></a></li>
<li>Under some reasonable conditions, one can show that MLEs are efficient and unbiased. TODO: find ref.</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Fisher_information">Fisher information matrix</a>
<ul>
<li>&#x201C;is the key part of the proof of Wilks&#x2019; theorem, which allows confidence region estimates for maximum likelihood estimation (for those conditions for which it applies) without needing the Likelihood Principle.&#x201D;</li>
</ul></li>
<li>Variance of MLEs
<ul>
<li><a href="https://en.wikipedia.org/wiki/Wilks%27_theorem">Wilks&#x2019;s theorem</a></li>
<li>Method of <span class="math inline">\(\Delta\chi^2\)</span> or <span class="math inline">\(\Delta{}L\)</span></li>
<li>Frequentist confidence intervals (e.g.&#xA0;at 95% CL)</li>
<li>Cowan<a href="#fn50" class="footnote-ref" id="fnref50" role="doc-noteref"><sup>50</sup></a></li>
<li>Likelihood need not be Gaussian<a href="#fn51" class="footnote-ref" id="fnref51" role="doc-noteref"><sup>51</sup></a></li>
<li>Minos method in particle physics in MINUIT<a href="#fn52" class="footnote-ref" id="fnref52" role="doc-noteref"><sup>52</sup></a></li>
<li>See slides for my talk: <a href="http://rreece.github.io/talks/pdf/2018-02-16-RReece-statistics-workshop-insight.pdf">Primer on statistics: MLE, Confidence Intervals, and Hypothesis Testing</a></li>
</ul></li>
<li>Asymptotics
<ul>
<li>Cowan, G., Cranmer, K., Gross, E., &amp; Vitells, O. (2012). <a href="https://arxiv.org/abs/1210.6948">Asymptotic distribution for two-sided tests with lower and upper boundaries on the parameter of interest</a>.<a href="#fn53" class="footnote-ref" id="fnref53" role="doc-noteref"><sup>53</sup></a></li>
</ul></li>
</ul>
<figure>
<img src="img/DeltaL_nonparabolic.png" id="fig:DeltaL_nonparabolic" alt="Figure 3: Transformation of non-parabolic log-likelihood to parabolic (source: my slides, recreation of F. James (2006), p.&#xA0;235)." /><figcaption aria-hidden="true">Figure 3: Transformation of non-parabolic log-likelihood to parabolic (source: <a href="http://rreece.github.io/talks/pdf/2018-02-16-RReece-statistics-workshop-insight.pdf">my slides</a>, recreation of <span class="citation" data-cites="James_2006_Statistical_Methods_in_Experimental_Particle">F. James (2006)</span>, p.&#xA0;235).</figcaption>
</figure>
<ul>
<li>Common error bars
<ul>
<li>Poisson error bars
<ul>
<li>Gaussian approximation: <span class="math inline">\(\sqrt{n}\)</span></li>
<li><a href="https://www.johndcook.com/blog/wilson_hilferty/">Wilson-Hilferty approximation</a></li>
</ul></li>
<li>Binomial error bars
<ul>
<li>Error on efficiency or proportion</li>
<li>See: <a href="#statistical-classification">Statistical classification</a></li>
</ul></li>
</ul></li>
<li>Discussion
<ul>
<li>Wainer, H. (2007). <a href="https://sites.stat.washington.edu/people/peter/498.Sp16/Equation.pdf">The most dangerous equation</a>. (de Moivre&#x2019;s equation for variance of means)<a href="#fn54" class="footnote-ref" id="fnref54" role="doc-noteref"><sup>54</sup></a></li>
</ul></li>
<li>Misc
<ul>
<li>Karhunen-Lo&#xE8;ve eigenvalue problems in cosmology: How should we tackle large data sets?<a href="#fn55" class="footnote-ref" id="fnref55" role="doc-noteref"><sup>55</sup></a></li>
</ul></li>
</ul>
<h3 id="bayesian-credibility-intervals">Bayesian credibility intervals</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Inverse_problem">Inverse problem</a> to find a posterior probability distribution.
<ul>
<li>See also: <a href="#likelihood-and-frequentist-vs-bayesian-probability">Likelihood and frequentist vs bayesian probability</a></li>
</ul></li>
<li>Maximum a posteriori estimation (MAP)</li>
<li>Prior sensitivity
<ul>
<li>Betancourt, M. (2018). <a href="https://github.com/betanalpha/jupyter_case_studies/blob/master/principled_bayesian_workflow/principled_bayesian_workflow.ipynb">Towards a principled Bayesian workflow</a> - ipynb</li>
</ul></li>
<li>Not invariant to reparametrization in general
<ul>
<li>Jeffreys priors are</li>
<li>TODO: James</li>
</ul></li>
</ul>
<h3 id="uncertainty-on-measuring-an-efficiency">Uncertainty on measuring an efficiency</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Binomial_proportion_confidence_interval">Binomial proportion confidence interval</a></li>
<li>Normal/Gaussian/Wald interval
<ul>
<li><a href="https://math.stackexchange.com/questions/1448233/the-derivation-of-the-wald-interval">Derivation of the Wald interval</a></li>
</ul></li>
<li>Wilson score interval</li>
<li>Clopper-Pearson interval (1934)<a href="#fn56" class="footnote-ref" id="fnref56" role="doc-noteref"><sup>56</sup></a></li>
<li>Agresti-Coull interval (1998)<a href="#fn57" class="footnote-ref" id="fnref57" role="doc-noteref"><sup>57</sup></a>
<ul>
<li><a href="https://www.graphpad.com/support/faq/the-modified-wald-method-for-computing-the-confidence-interval-of-a-proportion/">The modified Wald method for computing the confidence interval of a proportion</a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Rule_of_three_(statistics)">Rule of three</a> (1983)<a href="#fn58" class="footnote-ref" id="fnref58" role="doc-noteref"><sup>58</sup></a></li>
<li>Review by Brown, Cai, &amp; DasGupta (2001)<a href="#fn59" class="footnote-ref" id="fnref59" role="doc-noteref"><sup>59</sup></a></li>
<li>Casadei, D. (2012). <a href="https://arxiv.org/abs/0908.0130">Estimating the selection efficiency</a>.<a href="#fn60" class="footnote-ref" id="fnref60" role="doc-noteref"><sup>60</sup></a></li>
<li>Precision vs recall for classification, again</li>
<li>Classification and logistic regression</li>
<li>See also:
<ul>
<li><a href="#logistic-regression">Logistic regression</a> in the section on <a href="#classical-machine-learning">Classical machine learning</a>.</li>
<li><a href="#clustering">Clustering</a> in the section on <a href="#classical-machine-learning">Classical machine learning</a>.</li>
</ul></li>
</ul>
<h3 id="examples">Examples</h3>
<ul>
<li>Some sample mean</li>
<li>Bayesian lighthouse</li>
<li><a href="#uncertainty-on-measuring-an-efficiency">Measuring an efficiency</a></li>
<li>Some HEP fit</li>
</ul>
<h2 id="statistical-hypothesis-testing">Statistical hypothesis testing</h2>
<h3 id="null-hypothesis-significance-testing">Null hypothesis significance testing</h3>
<ul>
<li>Karl Pearson observing how rare sequences of roulette spins are</li>
<li>Null hypothesis significance testing (NHST)</li>
<li>goodness of fit</li>
<li>Fisher</li>
</ul>
<p>Fisher:</p>
<blockquote>
<p>[T]he null hypothesis is never proved or established, but is possibly disproved, in the course of experimentation.<a href="#fn61" class="footnote-ref" id="fnref61" role="doc-noteref"><sup>61</sup></a></p>
</blockquote>
<h3 id="neyman-pearson-theory">Neyman-Pearson theory</h3>
<h4 id="introduction">Introduction</h4>
<ul>
<li>probes an alternative hypothesis<a href="#fn62" class="footnote-ref" id="fnref62" role="doc-noteref"><sup>62</sup></a></li>
<li>Type-1 and type-2 errors</li>
<li>Power and confidence</li>
<li>Cranmer, K. (2020). <a href="http://theoryandpractice.org/stats-ds-book/statistics/lhc_stats_thumbnail.html">Thumbnail of LHC statistical procedures</a>.</li>
<li>ATLAS and CMS Collaborations. (2011). <a href="http://cds.cern.ch/record/1379837">Procedure for the LHC Higgs boson search combination in Summer 2011</a>.<a href="#fn63" class="footnote-ref" id="fnref63" role="doc-noteref"><sup>63</sup></a></li>
<li>Cowan, G., Cranmer, K., Gross, E., &amp; Vitells, O. (2011). <a href="https://arxiv.org/abs/1007.1727">Asymptotic formulae for likelihood-based tests of new physics</a>.<a href="#fn64" class="footnote-ref" id="fnref64" role="doc-noteref"><sup>64</sup></a></li>
</ul>
<figure>
<img src="img/ROC-explainer.png" id="fig:ROC-explainer" alt="Figure 4: TODO: ROC explainer. (Wikimedia, 2015)." /><figcaption aria-hidden="true">Figure 4: TODO: ROC explainer. (<a href="https://commons.wikimedia.org/wiki/File:ROC_curves.svg">Wikimedia</a>, 2015).</figcaption>
</figure>
<p>See also:</p>
<ul>
<li><a href="#statistical-classification">Statistical classification</a></li>
</ul>
<h4 id="neyman-pearson-lemma">Neyman-Pearson lemma</h4>
<p>Neyman-Pearson lemma:<a href="#fn65" class="footnote-ref" id="fnref65" role="doc-noteref"><sup>65</sup></a></p>
<p>For a fixed signal efficiency, <span class="math inline">\(1-\alpha\)</span>, the selection that corresponds to the lowest possible misidentification probability, <span class="math inline">\(\beta\)</span>, is given by</p>
<p><span class="math display">\[ \frac{L(H_1)}{L(H_0)} &gt; k_{\alpha} \,, \label{eq:np-lemma} \]</span></p>
<p>where <span class="math inline">\(k_{\alpha}\)</span> is the cut value required to achieve a type-1 error rate of <span class="math inline">\(\alpha\)</span>.</p>
<p>Neyman-Pearson test statistic:</p>
<p><span class="math display">\[ q_\mathrm{NP} = - 2 \ln \frac{L(H_1)}{L(H_0)} \label{eq:qnp-test-stat} \]</span></p>
<p>Profile likelihood ratio:</p>
<p><span class="math display">\[ \lambda(\mu) = \frac{ L(\mu, \hat{\theta}_\mu) }{ L(\hat{\mu}, \hat{\theta}) } \label{eq:profile-llh-ratio} \]</span></p>
<p>where <span class="math inline">\(\hat{\theta}\)</span> is the (unconditional) maximum-likelihood estimator that maximizes <span class="math inline">\(L\)</span>, while <span class="math inline">\(\hat{\theta}_\mu\)</span> is the conditional maximum-likelihood estimator that maximizes <span class="math inline">\(L\)</span> for a specified signal strength, <span class="math inline">\(\mu\)</span>, and <span class="math inline">\(\theta\)</span> as a vector includes all other parameters of interest and nuisance parameters.</p>
<h4 id="neyman-construction">Neyman construction</h4>
<p>Cranmer: <a href="http://theoryandpractice.org/stats-ds-book/statistics/neyman_construction.html">Neyman construction</a>.</p>
<figure>
<img src="img/neyman-construction.png" id="fig:neyman-construction" alt="Figure 5: Neyman construction for a confidence belt for \theta (source: K. Cranmer, 2020)." /><figcaption aria-hidden="true">Figure 5: Neyman construction for a confidence belt for <span class="math inline">\(\theta\)</span> (source: <a href="http://theoryandpractice.org/stats-ds-book/statistics/neyman_construction.html">K. Cranmer</a>, 2020).</figcaption>
</figure>
<p>TODO: fix</p>
<p><span class="math display">\[ q = - 2 \ln \frac{L(\mu\,s + b)}{L(b)} \label{eq:q0-test-stat} \]</span></p>
<h4 id="flip-flopping">Flip-flopping</h4>
<ul>
<li>Flip-flopping and Feldman-Cousins confidence intervals<a href="#fn66" class="footnote-ref" id="fnref66" role="doc-noteref"><sup>66</sup></a></li>
</ul>
<h3 id="p-values-and-significance"><em>p</em>-values and significance</h3>
<ul>
<li><span class="math inline">\(p\)</span>-values and significance<a href="#fn67" class="footnote-ref" id="fnref67" role="doc-noteref"><sup>67</sup></a></li>
<li>Coverage</li>
<li>Fisherian vs Neyman-Pearson <span class="math inline">\(p\)</span>-values</li>
</ul>
<p>Cowan <em>et al.</em> define a <span class="math inline">\(p\)</span>-value as</p>
<blockquote>
<p>a probability, under assumption of <span class="math inline">\(H\)</span>, of finding data of equal or greater incompatibility with the predictions of <span class="math inline">\(H\)</span>.<a href="#fn68" class="footnote-ref" id="fnref68" role="doc-noteref"><sup>68</sup></a></p>
</blockquote>
<p>Also:</p>
<blockquote>
<p>It should be emphasized that in an actual scientific context, rejecting the background-only hypothesis in a statistical sense is only part of discovering a new phenomenon. One&#x2019;s degree of belief that a new process is present will depend in general on other factors as well, such as the plausibility of the new signal hypothesis and the degree to which it can describe the data. Here, however, we only consider the task of determining the <span class="math inline">\(p\)</span>-value of the background-only hypothesis; if it is found below a specified threshold, we regard this as &#x201C;discovery.&#x201D;<a href="#fn69" class="footnote-ref" id="fnref69" role="doc-noteref"><sup>69</sup></a></p>
</blockquote>
<h4 id="uppper-limits">Uppper limits</h4>
<ul>
<li>Cousins, R.D. &amp; Highland, V.L. (1992). <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.193.1581&amp;rep=rep1&amp;type=pdf">Incorporating systematic uncertainties into an upper limit</a>.<a href="#fn70" class="footnote-ref" id="fnref70" role="doc-noteref"><sup>70</sup></a></li>
</ul>
<h4 id="cls-method">CLs method</h4>
<ul>
<li>Conservative coverage; used in particle physics</li>
<li>Junk<a href="#fn71" class="footnote-ref" id="fnref71" role="doc-noteref"><sup>71</sup></a></li>
<li>Read<a href="#fn72" class="footnote-ref" id="fnref72" role="doc-noteref"><sup>72</sup></a></li>
<li>ATLAS<a href="#fn73" class="footnote-ref" id="fnref73" role="doc-noteref"><sup>73</sup></a></li>
</ul>
<h3 id="asymptotics">Asymptotics</h3>
<ul>
<li>Analytic variance of the likelihood-ratio of gaussians: <span class="math inline">\(\chi^2\)</span>
<ul>
<li>Wilks<a href="#fn74" class="footnote-ref" id="fnref74" role="doc-noteref"><sup>74</sup></a>
<ul>
<li>Under the null hypothesis, <span class="math inline">\(-2 \ln(\lambda) \sim \chi^{2}_{k}\)</span>, where <span class="math inline">\(k\)</span>, the degrees of freedom for the <span class="math inline">\(\chi^{2}\)</span> distribution is the number of parameters of interest (including signal strength) in the signal model but not in the null hypothesis background model.</li>
</ul></li>
<li>Wald<a href="#fn75" class="footnote-ref" id="fnref75" role="doc-noteref"><sup>75</sup></a>
<ul>
<li>Wald generalized the work of Wilks for the case of testing some nonzero signal for exclusion, showing <span class="math inline">\(-2 \ln(\lambda) \approx (\hat{\theta} - \theta)^{\mathsf{T}}V^{-1} (\hat{\theta} - \theta) \sim \mathrm{noncentral}\:\chi^{2}_{k}\)</span>.</li>
<li>In the simplest case where there is only one parameter of interest (the signal strength, <span class="math inline">\(\mu\)</span>), then <span class="math inline">\(-2 \ln(\lambda) \approx \frac{ (\hat{\mu} - \mu)^{2} }{ \sigma^2 } \sim \mathrm{noncentral}\:\chi^{2}_{1}\)</span>.</li>
</ul></li>
<li>Pearson <span class="math inline">\(\chi^2\)</span>-test</li>
</ul></li>
<li>Cowan <em>et al.</em><a href="#fn76" class="footnote-ref" id="fnref76" role="doc-noteref"><sup>76</sup></a>
<ul>
<li>Wald approximation</li>
<li>Asimov dataset</li>
<li>Talk by Armbruster: <a href="https://indico.cern.ch/event/233551/contributions/493678/attachments/389871/542293/asymptotics_armbruster.pdf">Asymptotic formulae</a> (2013).</li>
</ul></li>
<li>Criteria for projected discovery and exclusion sensitivities of counting experiments<a href="#fn77" class="footnote-ref" id="fnref77" role="doc-noteref"><sup>77</sup></a>
<ul>
<li><a href="https://github.com/prudhvibhattiprolu/Zstats">github.com/prudhvibhattiprolu/Zstats</a></li>
</ul></li>
</ul>
<h3 id="students-t-test">Student&#x2019;s <em>t</em>-test</h3>
<ul>
<li>Student&#x2019;s <em>t</em>-test</li>
<li>ANOVA</li>
<li>A/B-testing</li>
</ul>
<h3 id="frequentist-vs-bayesian-decision-theory">Frequentist vs bayesian decision theory</h3>
<ul>
<li>Frequentist vs bayesian decision theory<a href="#fn78" class="footnote-ref" id="fnref78" role="doc-noteref"><sup>78</sup></a></li>
<li>Goodman, S.N. (1999). <a href="https://courses.botany.wisc.edu/botany_940/06EvidEvol/papers/goodman2.pdf">Toward evidence-based medical statistics 2: The Bayes factor</a>.<a href="#fn79" class="footnote-ref" id="fnref79" role="doc-noteref"><sup>79</sup></a></li>
</ul>
<p>Support for using Bayes factors:</p>
<blockquote>
<p>which properly separates issues of long-run behavior from evidential strength and allows the integration of background knowledge with statistical findings.<a href="#fn80" class="footnote-ref" id="fnref80" role="doc-noteref"><sup>80</sup></a></p>
</blockquote>
<p>See also:</p>
<ul>
<li><a href="#statistics-wars">&#x201C;Statistics Wars&#x201D;</a></li>
</ul>
<h3 id="examples-1">Examples</h3>
<ul>
<li>Difference of two means: <span class="math inline">\(t\)</span>-test</li>
<li>A/B-testing</li>
<li>New physics
<ul>
<li><a href="http://rreece.github.io/talks/pdf/2016-11-29-RReece-ATLAS-Epistemology.pdf">Slides by Me, Ryan Reece: &#x201C;ATLAS, data reduction, and epistemology&#x201D;</a></li>
<li><a href="https://www.youtube.com/watch?v=NA5u5X23QLo">Talk by Tommaso Dorigo: &#x201C;Frequentist Statistics, the Particle Physicists&#x2019; Way&#x201D;</a></li>
</ul></li>
</ul>
<h2 id="uncertainty-quantification">Uncertainty quantification</h2>
<h3 id="sinervo-classification-of-systematic-uncertainties">Sinervo classification of systematic uncertainties</h3>
<ul>
<li>Class-1, class-2, and class-3 systematic uncertanties (good, bad, ugly), Classification by Pekka Sinervo (PhyStat2003)<a href="#fn81" class="footnote-ref" id="fnref81" role="doc-noteref"><sup>81</sup></a></li>
<li>Not to be confused with type-1 and type-2 errors in Neyman-Pearson theory</li>
<li>Heinrich, J. &amp; Lyons, L. (2007). Systematic errors.<a href="#fn82" class="footnote-ref" id="fnref82" role="doc-noteref"><sup>82</sup></a></li>
<li>Caldeira &amp; Nord<a href="#fn83" class="footnote-ref" id="fnref83" role="doc-noteref"><sup>83</sup></a></li>
</ul>
<p>Lyons:</p>
<blockquote>
<p>In analyses involving enough data to achieve reasonable statistical accuracy, considerably more effort is devoted to assessing the systematic error than to determining the parameter of interest and its statistical error.<a href="#fn84" class="footnote-ref" id="fnref84" role="doc-noteref"><sup>84</sup></a></p>
</blockquote>
<figure>
<img src="img/systematic-uncertainties-sinervo.png" id="fig:systematic-uncertainties-sinervo" alt="Figure 6: Classification of measurement uncertainties (philosophy-in-figures.tumblr.com, 2016)." /><figcaption aria-hidden="true">Figure 6: Classification of measurement uncertainties (<a href="http://philosophy-in-figures.tumblr.com/post/150371555016/classification-of-measurement-uncertainties">philosophy-in-figures.tumblr.com</a>, 2016).</figcaption>
</figure>
<ul>
<li>Poincar&#xE9;&#x2019;s three levels of ignorance</li>
</ul>
<h3 id="profile-likelihoods">Profile likelihoods</h3>
<ul>
<li>Profiling and the profile likelihood
<ul>
<li>Importance of Wald and Cowan <em>et al</em>.</li>
<li>hybrid Bayesian-frequentist method</li>
</ul></li>
</ul>
<h3 id="examples-of-poor-estimates-of-systematic-uncertanties">Examples of poor estimates of systematic uncertanties</h3>
<ul>
<li>Unaccounted-for effects</li>
<li>CDF <span class="math inline">\(Wjj\)</span> bump
<ul>
<li>Phys.Rev.Lett.106:171801 (2011) / <a href="https://arxiv.org/abs/1104.0699">arxiv:1104.0699</a></li>
<li><a href="https://www-cdf.fnal.gov/physics/ewk/2011/wjj/7_3.html">Invariant mass distribution of jet pairs produced in association with a <span class="math inline">\(W\)</span> boson in <span class="math inline">\(p\bar{p}\)</span> collisions at <span class="math inline">\(\sqrt{s}\)</span> = 1.96 TeV</a></li>
<li>Dorigo, T. (2011). <a href="https://www.science20.com/quantum_diaries_survivor/blog/jet_energy_scale_explanation_cdf_signal-77886">The jet energy scale as an explanation of the CDF signal</a>.</li>
</ul></li>
</ul>
<figure>
<img src="img/cdf-wjj.png" id="fig:cdf-wjj" alt="Figure 7: Demonstration of sensitivity to the jet energy scale for an alleged excess in Wjj by Tommaso Dorigo (2011) (see also: GIF)." /><figcaption aria-hidden="true">Figure 7: Demonstration of sensitivity to the jet energy scale for an alleged excess in <span class="math inline">\(Wjj\)</span> by <a href="https://www.science20.com/quantum_diaries_survivor/blog/jet_energy_scale_explanation_cdf_signal-77886">Tommaso Dorigo (2011)</a> (see also: <a href="img/AnimatedDijet.gif">GIF</a>).</figcaption>
</figure>
<ul>
<li>OPERA. (2011). <a href="https://arxiv.org/abs/1109.4897v1">Faster-than-light neutrinos</a>.</li>
<li>BICEP2 claimed evidence of B-modes in the CMB as evidence of cosmic inflation without accounting for cosmic dust.</li>
</ul>
<h2 id="statistical-classification">Statistical classification</h2>
<h3 id="introduction-1">Introduction</h3>
<ul>
<li>Precision vs recall</li>
<li>Recall is sensitivity</li>
<li>Sensitivity vs specificity</li>
<li>Accuracy</li>
</ul>
<h3 id="examples-2">Examples</h3>
<ul>
<li>TODO</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="#decision-trees">Decision trees</a></li>
</ul>
<h2 id="causal-inference">Causal inference</h2>
<h3 id="introduction-2">Introduction</h3>
<ul>
<li>Rubin, D. B. (1974). <a href="https://psycnet.apa.org/fulltext/1975-06502-001.pdf">Estimating causal effects of treatments in randomized and nonrandomized studies</a>.<a href="#fn85" class="footnote-ref" id="fnref85" role="doc-noteref"><sup>85</sup></a></li>
<li>Lewis, D. (1981). <a href="https://www.andrewmbailey.com/dkl/Causal_Decision_Theory.pdf">Causal decision theory</a>.<a href="#fn86" class="footnote-ref" id="fnref86" role="doc-noteref"><sup>86</sup></a></li>
<li>Pearl, J. (2018). <em>The Book of Why: The new science of cause and effect</em>.<a href="#fn87" class="footnote-ref" id="fnref87" role="doc-noteref"><sup>87</sup></a></li>
</ul>
<p>See also:</p>
<ul>
<li><a href="scientific-method.html#causation">Causation</a></li>
<li><a href="scientific-method.html#induction">Induction</a></li>
</ul>
<h3 id="causal-models">Causal models</h3>
<ul>
<li>Structural Causal Model (SCM)</li>
<li>Pearl, J. (2009). <a href="https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.pdf">Causal inference in statistics: An overview</a>.<a href="#fn88" class="footnote-ref" id="fnref88" role="doc-noteref"><sup>88</sup></a></li>
<li>Robins, J.M. &amp; Wasserman, L. (1999). On the impossibility of inferring causation from association without background knowledge.<a href="#fn89" class="footnote-ref" id="fnref89" role="doc-noteref"><sup>89</sup></a></li>
<li>Peters, J., Janzing, D., &amp; Sch&#xF6;lkopf, B. (2017). <em>Elements of Causal Inference</em>.<a href="#fn90" class="footnote-ref" id="fnref90" role="doc-noteref"><sup>90</sup></a></li>
<li>Lundberg, I., Johnson, R., &amp; Stewart, B.M. (2021). <a href="https://journals.sagepub.com/doi/abs/10.1177/00031224211004187">What is your estimand? Defining the target quantity connects statistical evidence to theory</a>.<a href="#fn91" class="footnote-ref" id="fnref91" role="doc-noteref"><sup>91</sup></a></li>
</ul>
<h3 id="counterfactuals">Counterfactuals</h3>
<ul>
<li>Counterfactuals</li>
<li>Regret</li>
<li>Interventionist conception of causation</li>
<li>Ismael, J. (2023). <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsfs.2022.0081">Reflections on the asymmetry of causation</a>.<a href="#fn92" class="footnote-ref" id="fnref92" role="doc-noteref"><sup>92</sup></a></li>
</ul>
<h2 id="exploratory-data-analysis">Exploratory data analysis</h2>
<h3 id="introduction-3">Introduction</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory data analysis</a></li>
<li><em>Exploratory Data Analysis</em> (1977)<a href="#fn93" class="footnote-ref" id="fnref93" role="doc-noteref"><sup>93</sup></a></li>
</ul></li>
</ul>
<h3 id="look-elsewhere-effect">Look-elsewhere effect</h3>
<ul>
<li>Look-elsewhere effect (LEE)
<ul>
<li>AKA File-drawer effect</li>
</ul></li>
<li>Stopping rules
<ul>
<li>validation dataset</li>
<li>statistical issues, violates the likelihood principle</li>
</ul></li>
</ul>
<h3 id="archiving-and-data-science">Archiving and data science</h3>
<ul>
<li>&#x201C;Data science&#x201D;
<ul>
<li>Data collection, quality, analysis, archival, and reinterpretation</li>
<li><a href="https://plato.stanford.edu/entries/science-big-data/">Scientific research and big data</a></li>
</ul></li>
<li>Reproducible an reinterpretable
<ul>
<li>RECAST</li>
<li>Chen, X. et al.&#xA0;(2018). <a href="https://www.nature.com/articles/s41567-018-0342-2">Open is not enough</a>.<a href="#fn94" class="footnote-ref" id="fnref94" role="doc-noteref"><sup>94</sup></a></li>
</ul></li>
</ul>
<h2 id="statistics-wars">&#x201C;Statistics Wars&#x201D;</h2>
<h3 id="introduction-4">Introduction</h3>
<ul>
<li>Kruschke</li>
<li>Carnap
<ul>
<li>&#x201C;The two concepts of probability&#x201D;<a href="#fn95" class="footnote-ref" id="fnref95" role="doc-noteref"><sup>95</sup></a></li>
</ul></li>
<li>Royall
<ul>
<li>&#x201C;What do these data say?&#x201D;<a href="#fn96" class="footnote-ref" id="fnref96" role="doc-noteref"><sup>96</sup></a></li>
</ul></li>
</ul>
<p>Cranmer:</p>
<blockquote>
<p>Bayes&#x2019;s theorem is a theorem, so there&#x2019;s no debating it. It is not the case that Frequentists dispute whether Bayes&#x2019;s theorem is true. The debate is whether the necessary probabilities exist in the first place. If one can define the joint probability <span class="math inline">\(P (A, B)\)</span> in a frequentist way, then a Frequentist is perfectly happy using Bayes theorem. Thus, the debate starts at the very definition of probability.<a href="#fn97" class="footnote-ref" id="fnref97" role="doc-noteref"><sup>97</sup></a></p>
</blockquote>
<p>Neyman:</p>
<blockquote>
<p>Without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behaviour with regard to them, in following which we insure that, in the long run of experience, we shall not be too often wrong.<a href="#fn98" class="footnote-ref" id="fnref98" role="doc-noteref"><sup>98</sup></a></p>
</blockquote>
<figure>
<img src="img/Kruschke-bayes-freq-grid.png" id="fig:Kruschke-bayes-freq-grid" alt="Figure 8: From Kruschke." /><figcaption aria-hidden="true">Figure 8: From Kruschke.<a href="#fn99" class="footnote-ref" id="fnref99" role="doc-noteref"><sup>99</sup></a></figcaption>
</figure>
<h3 id="likelihood-principle">Likelihood principle</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Likelihood_principle">Likelihood principle</a></li>
<li>The likelihood principle is the proposition that, given a statistical model and a data sample, all the evidence relevant to model parameters is contained in the likelihood function.</li>
<li>The history of likelihood<a href="#fn100" class="footnote-ref" id="fnref100" role="doc-noteref"><sup>100</sup></a>
<ul>
<li>Allan Birnbaum proved that the likelihood principle follows from two more primitive and seemingly reasonable principles, the <a href="https://en.wikipedia.org/wiki/Conditionality_principle">conditionality principle</a> and the <a href="https://en.wikipedia.org/wiki/Sufficient_statistic">sufficiency principle</a>.<a href="#fn101" class="footnote-ref" id="fnref101" role="doc-noteref"><sup>101</sup></a></li>
<li>Hacking identified the &#x201C;law of likelihood.&#x201D;<a href="#fn102" class="footnote-ref" id="fnref102" role="doc-noteref"><sup>102</sup></a></li>
</ul></li>
<li>Berger &amp; Wolpert. (1988). <em>The Likelihood Principle</em>.<a href="#fn103" class="footnote-ref" id="fnref103" role="doc-noteref"><sup>103</sup></a></li>
</ul>
<p>O&#x2019;Hagan:</p>
<blockquote>
<p>The first key argument in favour of the Bayesian approach can be called the axiomatic argument. We can formulate systems of axioms of good inference, and under some persuasive axiom systems it can be proved that Bayesian inference is a consequence of adopting any of these systems&#x2026; If one adopts two principles known as ancillarity and sufficiency principles, then under some statement of these principles it follows that one must adopt another known as the likelihood principle. Bayesian inference conforms to the likelihood principle whereas classical inference does not. Classical procedures regularly violate the likelihood principle or one or more of the other axioms of good inference. There are no such arguments in favour of classical inference.<a href="#fn104" class="footnote-ref" id="fnref104" role="doc-noteref"><sup>104</sup></a></p>
</blockquote>
<ul>
<li>Gandenberger
<ul>
<li>&#x201C;A new proof of the likelihood principle&#x201D;<a href="#fn105" class="footnote-ref" id="fnref105" role="doc-noteref"><sup>105</sup></a></li>
<li>Thesis: <a href="http://d-scholarship.pitt.edu/24634/"><em>Two Principles of Evidence and Their Implications for the Philosophy of Scientific Method</em></a> (2015)</li>
<li><a href="http://gandenberger.org/research/">gandenberger.org/research</a></li>
<li><a href="http://gandenberger.org/2014/04/28/do-frequentist-methods-violate-lp/">Do frequentist methods violate the likelihood principle?</a></li>
</ul></li>
<li>Criticisms:
<ul>
<li>Evans<a href="#fn106" class="footnote-ref" id="fnref106" role="doc-noteref"><sup>106</sup></a></li>
<li>Mayo<a href="#fn107" class="footnote-ref" id="fnref107" role="doc-noteref"><sup>107</sup></a></li>
<li>Mayo: <a href="https://errorstatistics.com/2019/04/04/excursion-1-tour-ii-error-probing-tools-versus-logics-of-evidence-excerpt/">The law of likelihood and error statistics</a><a href="#fn108" class="footnote-ref" id="fnref108" role="doc-noteref"><sup>108</sup></a></li>
<li><a href="https://errorstatistics.com/2013/02/11/u-phil-mayos-response-to-hennig-and-gandenberger/">Mayo&#x2019;s response to Hennig and Gandenberger</a></li>
<li>Dawid, A.P. (2014). <a href="https://projecteuclid.org/journals/statistical-science/volume-29/issue-2/Discussion-of-On-the-Birnbaum-Argument-for-the-Strong-Likelihood/10.1214/14-STS470.full">Discussion of &#x201C;On the Birnbaum Argument for the Strong Likelihood Principle&#x201D;</a>.<a href="#fn109" class="footnote-ref" id="fnref109" role="doc-noteref"><sup>109</sup></a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Likelihoodist_statistics">Likelihoodist statistics</a></li>
</ul>
<p>Mayo:</p>
<blockquote>
<p>Likelihoods are vital to all statistical accounts, but they are often misunderstood because the data are fixed and the hypothesis varies. Likelihoods of hypotheses should not be confused with their probabilities. &#x2026; [T]he same phenomenon may be perfectly predicted or explained by two rival theories; so both theories are equally likely on the data, even though they cannot both be true.<a href="#fn110" class="footnote-ref" id="fnref110" role="doc-noteref"><sup>110</sup></a></p>
</blockquote>
<h3 id="discussion">Discussion</h3>
<p>Lyons:</p>
<blockquote>
<p>Particle Physicists tend to favor a frequentist method. This is because we really do consider that our data are representative as samples drawn according to the model we are using (decay time distributions often are exponential; the counts in repeated time intervals do follow a Poisson distribution, etc.), and hence we want to use a statistical approach that allows the data &#x201C;to speak for themselves,&#x201D; rather than our analysis being dominated by our assumptions and beliefs, as embodied in Bayesian priors.<a href="#fn111" class="footnote-ref" id="fnref111" role="doc-noteref"><sup>111</sup></a></p>
</blockquote>
<ul>
<li>Carnap
<ul>
<li>Sznajder on the alleged evolution of Carnap&#x2019;s views of inductive logic<a href="#fn112" class="footnote-ref" id="fnref112" role="doc-noteref"><sup>112</sup></a></li>
<li>Carnap, R. (1952). <a href="https://www.phil.cmu.edu/projects/carnap/editorial/latex_pdf/1952-1.pdf"><em>The Continuum of Inductive Methods</em></a>.<a href="#fn113" class="footnote-ref" id="fnref113" role="doc-noteref"><sup>113</sup></a></li>
</ul></li>
<li>David Cox</li>
<li>Ian Hacking
<ul>
<li><em>Logic of Statistical Inference</em><a href="#fn114" class="footnote-ref" id="fnref114" role="doc-noteref"><sup>114</sup></a></li>
</ul></li>
<li>Neyman
<ul>
<li>&#x201C;Frequentist probability and frequentist statistics&#x201D;<a href="#fn115" class="footnote-ref" id="fnref115" role="doc-noteref"><sup>115</sup></a></li>
</ul></li>
<li>Rozeboom
<ul>
<li>Rozeboom, W.W. (1960). The fallacy of the null-hypothesis significance test.<a href="#fn116" class="footnote-ref" id="fnref116" role="doc-noteref"><sup>116</sup></a></li>
</ul></li>
<li>Meehl
<ul>
<li>Meehl, P. E. (1978). Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology.<a href="#fn117" class="footnote-ref" id="fnref117" role="doc-noteref"><sup>117</sup></a></li>
</ul></li>
<li>Zech
<ul>
<li>&#x201C;Comparing statistical data to Monte Carlo simulation&#x201D;<a href="#fn118" class="footnote-ref" id="fnref118" role="doc-noteref"><sup>118</sup></a></li>
</ul></li>
<li>Richard Royall
<ul>
<li><em>Statistical Evidence: A likelihood paradigm</em><a href="#fn119" class="footnote-ref" id="fnref119" role="doc-noteref"><sup>119</sup></a></li>
</ul></li>
<li>Jim Berger
<ul>
<li>&#x201C;Could Fisher, Jeffreys, and Neyman have agreed on testing?&#x201D;<a href="#fn120" class="footnote-ref" id="fnref120" role="doc-noteref"><sup>120</sup></a></li>
</ul></li>
<li>Deborah Mayo
<ul>
<li>&#x201C;In defense of the Neyman-Pearson theory of confidence intervals&#x201D;<a href="#fn121" class="footnote-ref" id="fnref121" role="doc-noteref"><sup>121</sup></a></li>
<li>Concept of &#x201C;Learning from error&#x201D; in <em>Error and the Growth of Experimental Knowledge</em><a href="#fn122" class="footnote-ref" id="fnref122" role="doc-noteref"><sup>122</sup></a></li>
<li>&#x201C;Severe testing as a basic concept in a Neyman-Pearson philosophy of induction&#x201D;<a href="#fn123" class="footnote-ref" id="fnref123" role="doc-noteref"><sup>123</sup></a></li>
<li>&#x201C;Error statistics&#x201D;<a href="#fn124" class="footnote-ref" id="fnref124" role="doc-noteref"><sup>124</sup></a></li>
<li><em>Statistical Inference as Severe Testing</em><a href="#fn125" class="footnote-ref" id="fnref125" role="doc-noteref"><sup>125</sup></a></li>
<li><a href="https://blog.apaonline.org/2019/03/07/interview-with-deborah-mayo/">Statistics Wars: Interview with Deborah Mayo</a> - APA blog</li>
<li><a href="https://ndpr.nd.edu/news/statistical-inference-as-severe-testing-how-to-get-beyond-the-statistics-wars/">Review of <em>SIST</em> by Prasanta S. Bandyopadhyay</a></li>
<li><a href="https://phil-stat-wars.com/2020/05/22/lse-research-seminar-ph500-may-15/">LSE Research Seminar: Current Controversies in Phil Stat</a> (May 21, 2020)
<ul>
<li><a href="https://phil-stat-wars.com/2020/06/11/meeting-5-june-18/">Meeting 5</a> (June 18, 2020)</li>
</ul></li>
<li>Slides: <a href="https://philstatwars.files.wordpress.com/2022/09/deborah-g.-mayo-final-21-september-.pdf">The Statistics Wars and Their Casualties</a></li>
</ul></li>
<li>Andrew Gelman
<ul>
<li><a href="https://statmodeling.stat.columbia.edu/2014/09/05/confirmationist-falsificationist-paradigms-science/">Confirmationist and falsificationist paradigms of science</a> - Sept.&#xA0;5, 2014</li>
<li>Beyond subjective and objective in statistics<a href="#fn126" class="footnote-ref" id="fnref126" role="doc-noteref"><sup>126</sup></a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/03/20/retire-statistical-significance-the-discussion/">Retire Statistical Significance: The discussion</a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/09/11/exchange-with-deborah-mayo-on-abandoning-statistical-significance/">Exchange with Deborah Mayo on abandoning statistical significance</a></li>
<li><a href="https://statmodeling.stat.columbia.edu/2019/04/12/several-reviews-of-deborah-mayos-new-book-statistical-inference-as-severe-testing-how-to-get-beyond-the-statistics-wars/">Several reviews of <em>SIST</em></a></li>
</ul></li>
<li>Larry Wasserman
<ul>
<li><a href="https://normaldeviate.wordpress.com/2012/07/28/statistical-principles/">Statistical Principles?</a></li>
</ul></li>
<li>Kevin Murphy
<ul>
<li>Pathologies of frequentist statistics<a href="#fn127" class="footnote-ref" id="fnref127" role="doc-noteref"><sup>127</sup></a></li>
<li><span class="math inline">\(p\)</span>-values considered harmful<a href="#fn128" class="footnote-ref" id="fnref128" role="doc-noteref"><sup>128</sup></a></li>
</ul></li>
<li>Greg Gandenberger
<ul>
<li><a href="http://gandenberger.org/2014/07/21/intro-to-statistical-methods/">An introduction to likelihoodist, bayesian, and frequentist methods (1/3)</a></li>
<li>As Neyman and Pearson put it in their original presentation of the frequentist approach, &#x201C;without hoping to know whether each separate hypothesis is true or false, we may search for rules to govern our behavior with regard to them, in the following which we insure that, in the long run of experience, we shall not too often be wrong&#x201D; (1933, 291).</li>
<li><a href="http://gandenberger.org/2014/07/28/intro-to-statistical-methods-2/">An introduction to likelihoodist, bayesian, and frequentist methods (2/3)</a></li>
<li><a href="http://gandenberger.org/2014/08/26/intro-to-statistical-methods-3/">An introduction to likelihoodist, bayesian, and frequentist methods (3/3)</a></li>
<li><a href="http://gandenberger.org/2013/10/21/against-likelihoodist-methods/">An argument against likelihoodist methods as genuine alternatives to bayesian and frequentist methods</a></li>
<li>&#x201C;Why I am not a likelihoodist&#x201D;<a href="#fn129" class="footnote-ref" id="fnref129" role="doc-noteref"><sup>129</sup></a></li>
</ul></li>
<li>Jon Wakefield
<ul>
<li><em>Bayesian and Frequentist Regression Methods</em><a href="#fn130" class="footnote-ref" id="fnref130" role="doc-noteref"><sup>130</sup></a></li>
</ul></li>
<li>Efron &amp; Hastie
<ul>
<li>&#x201C;Flaws in Frequentist Inference&#x201D;<a href="#fn131" class="footnote-ref" id="fnref131" role="doc-noteref"><sup>131</sup></a></li>
</ul></li>
<li>Kruschke &amp; Liddel<a href="#fn132" class="footnote-ref" id="fnref132" role="doc-noteref"><sup>132</sup></a></li>
<li>Steinhardt, J. (2012). <a href="https://jsteinhardt.stat.berkeley.edu/files/stats-essay.pdf">Beyond Bayesians and frequentists</a>.<a href="#fn133" class="footnote-ref" id="fnref133" role="doc-noteref"><sup>133</sup></a></li>
<li>VanderPlas, J. (2014). <a href="https://jakevdp.github.io/blog/2014/06/12/frequentism-and-bayesianism-3-confidence-credibility/">Frequentism and Bayesianism III: Confidence, credibility, and why frequentism and science do not mix</a>.</li>
<li>Kent, B. (2021). <a href="https://www.crosstab.io/articles/confidence-interval-interpretation">No, your confidence interval is not a worst-case analysis</a>.</li>
</ul>
<figure>
<img src="img/gandenberger-thesis-venn-diagram.png" id="fig:gandenberger-thesis-venn-diagram" alt="Figure 9: The major virtues and vices of Bayesian, frequentist, and likelihoodist approaches to statistical inference (gandenberger.org/research/, 2015)." /><figcaption aria-hidden="true">Figure 9: The major virtues and vices of Bayesian, frequentist, and likelihoodist approaches to statistical inference (<a href="http://gandenberger.org/research/">gandenberger.org/research/</a>, 2015).</figcaption>
</figure>
<p>Goodman:</p>
<blockquote>
<p>The idea that the <span class="math inline">\(P\)</span> value can play both of these roles is based on a fallacy: that an event can be viewed simultaneously both from a long-run and a short-run perspective. In the long-run perspective, which is error-based and deductive, we group the observed result together with other outcomes that might have occurred in hypothetical repetitions of the experiment. In the &#x201C;short run&#x201D; perspective, which is evidential and inductive, we try to evaluate the meaning of the observed result from a single experiment. If we could combine these perspectives, it would mean that inductive ends (drawing scientific conclusions) could be served with purely deductive methods (objective probability calculations).<a href="#fn134" class="footnote-ref" id="fnref134" role="doc-noteref"><sup>134</sup></a></p>
</blockquote>
<h2 id="replication-crisis">Replication crisis</h2>
<h3 id="introduction-5">Introduction</h3>
<ul>
<li>Ioannidis, J.P. (2005). Why most published research findings are false.<a href="#fn135" class="footnote-ref" id="fnref135" role="doc-noteref"><sup>135</sup></a></li>
</ul>
<h3 id="p-value-controversy"><em>p</em>-value controversy</h3>
<ul>
<li>Wasserstein, R.L. &amp; Lazar, N.A. (2016). The ASA&#x2019;s statement on <span class="math inline">\(p\)</span>-values: Context, process, and purpose.<a href="#fn136" class="footnote-ref" id="fnref136" role="doc-noteref"><sup>136</sup></a></li>
<li>Wasserstein, R.L., Allen, L.S., &amp; Lazar, N.A. (2019). Moving to a World Beyond &#x201C;p&lt;0.05.&#x201D;<a href="#fn137" class="footnote-ref" id="fnref137" role="doc-noteref"><sup>137</sup></a></li>
<li><a href="http://www.nature.com/news/big-names-in-statistics-want-to-shake-up-much-maligned-p-value-1.22375">Big names in statistics want to shake up much-maligned P value</a><a href="#fn138" class="footnote-ref" id="fnref138" role="doc-noteref"><sup>138</sup></a></li>
<li><a href="https://hiphination.org/episodes/episode-7-hackademics-ii-the-hackers/">Hi-Phi Nation, episode 7</a></li>
<li>Fisher:</li>
</ul>
<blockquote>
<p>[N]o isolated experiment, however significant in itself, can suffice for the experimental demonstration of any natural phenomenon; for the &#x201C;one chance in a million&#x201D; will undoubtedly occur, with no less and no more than its appropriate frequency, however surprised we may be that it should occur to us. In order to assert that a natural phenomenon is experimentally demonstrable we need, not an isolated record, but a reliable method of procedure. In relation to the test of significance, we may say that a phenomenon is experimentally demonstrable when we know how to conduct an experiment which will rarely fail to give us a statistically significant result.<a href="#fn139" class="footnote-ref" id="fnref139" role="doc-noteref"><sup>139</sup></a></p>
</blockquote>
<ul>
<li>Relationship to the LEE</li>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Uncomfortable_science">Uncomfortable science</a></li>
</ul></li>
<li>Wasserman
<ul>
<li><a href="https://normaldeviate.wordpress.com/2012/07/11/the-higgs-boson-and-the-p-value-police/">The Higgs boson and the p-value police</a></li>
</ul></li>
<li>Rao &amp; Lovric
<ul>
<li>Rao, C.R. &amp; Lovric, M.M. (2016). <a href="http://digitalcommons.wayne.edu/jmasm/vol15/iss2/3">Testing point null hypothesis of a normal mean and the truth: 21st century perspective</a>.<a href="#fn140" class="footnote-ref" id="fnref140" role="doc-noteref"><sup>140</sup></a></li>
</ul></li>
<li>Mayo
<ul>
<li><a href="https://errorstatistics.com/2019/12/13/les-stats-cest-moi-we-take-that-step-here-adopt-our-fav-word-or-phil-stat/">&#x201C;Les stats, c&#x2019;est moi: We take that step here!&#x201D;</a></li>
<li>&#x201C;Significance tests: Vitiated or vindicated by the replication crisis in psychology?&#x201D;<a href="#fn141" class="footnote-ref" id="fnref141" role="doc-noteref"><sup>141</sup></a></li>
<li><a href="https://errorstatistics.com/2021/06/20/at-long-last-the-asa-presidents-task-force-statement-on-statistical-significance-and-replicability/">At long last! The ASA President&#x2019;s Task Force Statement on Statistical Significance and Replicability</a></li>
</ul></li>
<li>Gorard &amp; Gorard. (2016). What to do instead of significance testing.<a href="#fn142" class="footnote-ref" id="fnref142" role="doc-noteref"><sup>142</sup></a></li>
<li>Vox: <a href="https://www.vox.com/science-and-health/2017/7/31/16021654/p-values-statistical-significance-redefine-0005">What a nerdy debate about p-values shows about science&#x2013;and how to fix it</a></li>
<li>Karen Kafadar: <a href="https://magazine.amstat.org/blog/2019/12/01/kk_dec2019/">The Year in Review &#x2026; And More to Come</a></li>
<li><a href="https://jasa-acs.github.io/repro-guide/">The JASA Reproducibility Guide</a></li>
</ul>
<p>From &#x201C;The ASA president&#x2019;s task force statement on statistical significance and replicability&#x201D;:</p>
<blockquote>
<p><em>P</em>-values are valid statistical measures that provide convenient conventions for communicating the uncertainty inherent in quantitative results. Indeed, <em>P</em>-values and significance tests are among the most studied and best understood statistical procedures in the statistics literature. They are important tools that have advanced science through their proper application.<a href="#fn143" class="footnote-ref" id="fnref143" role="doc-noteref"><sup>143</sup></a></p>
</blockquote>
<h2 id="classical-machine-learning">Classical machine learning</h2>
<h3 id="introduction-6">Introduction</h3>
<ul>
<li>Classification vs regression</li>
<li>Supervised and unsupervised learning
<ul>
<li>Classification = supervised; clustering = unsupervised</li>
</ul></li>
<li>Hastie, Tibshirani, &amp; Friedman<a href="#fn144" class="footnote-ref" id="fnref144" role="doc-noteref"><sup>144</sup></a></li>
<li><em>Information Theory, Inference, and Learning</em><a href="#fn145" class="footnote-ref" id="fnref145" role="doc-noteref"><sup>145</sup></a></li>
<li>Murphy, K.P. (2012). <em>Machine Learning: A probabilistic perspective</em>. MIT Press.<a href="#fn146" class="footnote-ref" id="fnref146" role="doc-noteref"><sup>146</sup></a></li>
<li>Murphy, K.P. (2022). <em>Probabilistic Machine Learning: An introduction</em>. MIT Press.<a href="#fn147" class="footnote-ref" id="fnref147" role="doc-noteref"><sup>147</sup></a></li>
<li>Shalev-Shwarz, S. &amp; Ben-David, S. (2014). <a href="https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf"><em>Understanding Machine Learning: From Theory to Algorithms</em></a>.<a href="#fn148" class="footnote-ref" id="fnref148" role="doc-noteref"><sup>148</sup></a></li>
<li>VC-dimension
<ul>
<li>Vapnik (1994)<a href="#fn149" class="footnote-ref" id="fnref149" role="doc-noteref"><sup>149</sup></a></li>
<li>Shalev-Shwarz, S. &amp; Ben-David, S. (2014).<a href="#fn150" class="footnote-ref" id="fnref150" role="doc-noteref"><sup>150</sup></a></li>
</ul></li>
</ul>
<h3 id="history">History</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Arthur_Samuel">Arthur Samuel</a> (1901-1990)</li>
<li><a href="https://en.wikipedia.org/wiki/Dartmouth_workshop">Dartmouth workshop</a> (1956)
<ul>
<li>McCarthy, J., Minsky, M.L., Rochester, N., &amp; Shannon, C.E. (1955). <a href="http://www-formal.stanford.edu/jmc/history/dartmouth.pdf">A proposal for the Dartmouth Summer Research Project on Artificial Intelligence</a>.<a href="#fn151" class="footnote-ref" id="fnref151" role="doc-noteref"><sup>151</sup></a></li>
<li>Solomonoff, G. (2016). <a href="http://raysolomonoff.com/dartmouth/dartray.pdf">Ray Solomonoff and the Dartmouth Summer Research Project in Artificial Intelligence, 1956</a>.<a href="#fn152" class="footnote-ref" id="fnref152" role="doc-noteref"><sup>152</sup></a></li>
</ul></li>
<li>Kardum, M. (2020). Rudolf Carnap&#x2013;The grandfather of artificial neural networks: The influence of Carnap&#x2019;s philosophy on Walter Pitts.<a href="#fn153" class="footnote-ref" id="fnref153" role="doc-noteref"><sup>153</sup></a></li>
<li>Bright, L.K. (2022). <a href="https://sootyempiric.blogspot.com/2022/10/carnaps-contributions.html">Carnap&#x2019;s contributions</a>.</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="naturalism.html#honorific-reinterpretation">Honorific reinterpretation</a> of scientism</li>
</ul>
<h3 id="logistic-regression">Logistic regression</h3>
<p>From a probabilistic point of view,<a href="#fn154" class="footnote-ref" id="fnref154" role="doc-noteref"><sup>154</sup></a> logistic regression can be derived from doing maximum likelihood estimation of a vector of model parameters, <span class="math inline">\(\vec{w}\)</span>, in a dot product with the input features, <span class="math inline">\(\vec{x}\)</span>, and squashed with a logistic function that yields the probability, <span class="math inline">\(\mu\)</span>, of a Bernoulli random variable, <span class="math inline">\(y \in \{0, 1\}\)</span>.</p>
<p><span class="math display">\[ p(y | \vec{x}, \vec{w}) = \mathrm{Ber}(y | \mu(\vec{x}, \vec{w})) = \mu(\vec{x}, \vec{w})^y \: (1-\mu(\vec{x}, \vec{w}))^{(1-y)} \]</span></p>
<p>The negative log-likelihood of multiple trials is</p>
<p><span class="math display">\[\begin{align}
\mathrm{NLL}
    &amp;= - \sum_i \log p(y_i | \vec{x}_i, \vec{w}) \nonumber \\
    &amp;= - \sum_i \log\left( \mu(\vec{x}_i, \vec{w})^{y_i} \: (1-\mu(\vec{x}_i, \vec{w}))^{(1-y_i)} \right) \nonumber \\
    &amp;= - \sum_i \log\left( \mu_i^{y_i} \: (1-\mu_i)^{(1-y_i)} \right) \nonumber \\
    &amp;= - \sum_i \big( y_i \, \log \mu_i + (1-y_i) \log(1-\mu_i) \big) \label{eq:cross_entropy_loss0}
\end{align}\]</span></p>
<p>which is the <strong>cross entropy loss</strong>. Note that the first term is non-zero only when the true target is <span class="math inline">\(y_i=1\)</span>, and similarly the second term is non-zero only when <span class="math inline">\(y_i=0\)</span>.<a href="#fn155" class="footnote-ref" id="fnref155" role="doc-noteref"><sup>155</sup></a> Therefore, we can reparametrize the target <span class="math inline">\(y_i\)</span> in favor of <span class="math inline">\(t_{ki}\)</span> that is one-hot in an index <span class="math inline">\(k\)</span> over classes.</p>
<p><span class="math display">\[ \mathrm{CEL} = \mathrm{NLL} = - \sum_i \sum_k \big( t_{ki} \, \log \mu_{ki} \big) \label{eq:cross_entropy_loss1} \]</span></p>
<p>where</p>
<p><span class="math display">\[ t_{ki} = \begin{cases} 1 &amp; \mathrm{if}\ (k = y_i = 0)\ \mathrm{or}\ (k = y_i = 1) \\ 0 &amp; \mathrm{otherwise} \end{cases} \]</span></p>
<p>and</p>
<p><span class="math display">\[ \mu_{ki} = \begin{cases} 1-\mu_i &amp; \mathrm{if}\ k = 0 \\ \mu_i &amp; \mathrm{if}\ k =1 \end{cases} \]</span></p>
<p>This readily generalizes from binary classification to classification over many classes as we will discuss more below. Note that in the sum over classes, <span class="math inline">\(k\)</span>, only one term for the true class contributes.</p>
<p><span class="math display">\[ \mathrm{CEL} = - \left. \sum_i \log \mu_{ki} \right|_{k\ \mathrm{is\ such\ that}\ y_k=1} \label{eq:cross_entropy_loss2} \]</span></p>
<p>Logistic regression uses the <strong>logit function</strong>,<a href="#fn156" class="footnote-ref" id="fnref156" role="doc-noteref"><sup>156</sup></a> which is the logarithm of the odds&#x2014;the ratio of the chance of success to failure. Let <span class="math inline">\(\mu\)</span> be the probability of success in a Bernoulli trial, then the logit function is defined as</p>
<p><span class="math display">\[ \mathrm{logit}(\mu) \equiv \log\left(\frac{\mu}{1-\mu}\right) \label{eq:logit} \]</span></p>
<p>Logistic regression assumes that the logit function is a linear function of the explanatory variable, <span class="math inline">\(x\)</span>.</p>
<p><span class="math display">\[ \log\left(\frac{\mu}{1-\mu}\right) = \beta_0 + \beta_1 x \]</span></p>
<p>where <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are trainable parameters. (TODO: Why would we assume this?) This can be generalized to a vector of multiple input variables, <span class="math inline">\(\vec{x}\)</span>, where the input vector has a 1 prepended to be its zeroth component in order to conveniently include the bias, <span class="math inline">\(\beta_0\)</span>, in a dot product.</p>
<p><span class="math display">\[ \vec{x} = (1, x_1, x_2, \ldots, x_n)^{\mathsf{T}}\]</span></p>
<p><span class="math display">\[ \vec{w} = (\beta_0, \beta_1, \beta_2, \ldots, \beta_n)^{\mathsf{T}}\]</span></p>
<p><span class="math display">\[ \log\left(\frac{\mu}{1-\mu}\right) = \vec{w}^{\mathsf{T}}\vec{x} \]</span></p>
<p>For the moment, let <span class="math inline">\(z \equiv \vec{w}^{\mathsf{T}}\vec{x}\)</span>. Exponentiating and solving for <span class="math inline">\(\mu\)</span> gives</p>
<p><span class="math display">\[ \mu = \frac{ e^z }{ 1 + e^z } = \frac{ 1 }{ 1 + e^{-z} } \]</span></p>
<p>This function is called the <strong>logistic or sigmoid function</strong>.</p>
<p><span class="math display">\[ \mathrm{logistic}(z) \equiv \mathrm{sigm}(z) \equiv \frac{ 1 }{ 1 + e^{-z} }  \label{eq:logistic} \]</span></p>
<p>Since we inverted the logit function by solving for <span class="math inline">\(\mu\)</span>, the inverse of the logit function is the logistic or sigmoid.</p>
<p><span class="math display">\[ \mathrm{logit}^{-1}(z) = \mathrm{logistic}(z) = \mathrm{sigm}(z) \]</span></p>
<p>And therefore,</p>
<p><span class="math display">\[ \mu = \mathrm{sigm}(z) = \mathrm{sigm}(\vec{w}^{\mathsf{T}}\vec{x}) \]</span></p>
<figure>
<img src="img/logistic-regression.png" id="fig:logistic-regression" alt="Figure 10: Logistic regression." /><figcaption aria-hidden="true">Figure 10: Logistic regression.</figcaption>
</figure>
<p>See also:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Logistic_regression">Logistic regression</a></li>
<li>Harlan, W.S. (2007). <a href="http://www.billharlan.com/pub/papers/logistic/logistic.html">Bounded geometric growth: motivation for the logistic function</a>.</li>
<li>Heesch, D. <a href="http://www.daniel-heesch.com/static/softmax_regression.pdf">A short intro to logistic regression</a>.</li>
<li>Roelants, P. (2019). <a href="https://peterroelants.github.io/posts/cross-entropy-logistic/">Logistic classification with cross-entropy</a>.</li>
</ul>
<h3 id="softmax-regression">Softmax regression</h3>
<p>Again, from a probabilistic point of view, we can derive the use of multi-class cross entropy loss by starting with the Bernoulli distribution, generalizing it to multiple classes (indexed by <span class="math inline">\(k\)</span>) as</p>
<p><span class="math display">\[ p(y_k | \mu) = \mathrm{Cat}(y_k | \mu_k) = \prod_k {\mu_k}^{y_k} \label{eq:categorical_distribution} \]</span></p>
<p>which is the categorical or multinoulli distribution. The negative-log likelihood of multiple independent trials is</p>
<p><span class="math display">\[ \mathrm{NLL} = - \sum_i \log \left(\prod_k {\mu_{ki}}^{y_{ki}}\right) = - \sum_i \sum_k y_{ki} \: \log \mu_{ki} \label{eq:nll_multinomial} \]</span></p>
<p>Noting again that <span class="math inline">\(y_{ki} = 1\)</span> only when <span class="math inline">\(k\)</span> is the true class, and is 0 otherwise, this simplifies to eq.&#xA0;<span class="math inline">\(\eqref{eq:cross_entropy_loss2}\)</span>.</p>
<p>See also:</p>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Multinomial_logistic_regression">Multinomial logistic regression</a></li>
<li>McFadden<a href="#fn157" class="footnote-ref" id="fnref157" role="doc-noteref"><sup>157</sup></a></li>
<li>Softmax is really a soft argmax. TODO: find ref.</li>
<li>Softmax is not unique. There are other squashing functions.<a href="#fn158" class="footnote-ref" id="fnref158" role="doc-noteref"><sup>158</sup></a>
<ul>
<li><a href="https://twitter.com/PreetumNakkiran/status/1422752757920845825">Tweet from PreetumNakkiran</a></li>
<li><a href="https://arxiv.org/abs/2009.08092">Distributional generalization: A new kind of generalization</a></li>
<li><a href="https://arxiv.org/abs/2106.05932">Early-stopped neural networks are consistent</a></li>
</ul></li>
<li>Roelants, P. (2019). <a href="https://peterroelants.github.io/posts/cross-entropy-softmax/">Softmax classification with cross-entropy</a>.</li>
<li>Gradients from backprop through a softmax</li>
<li>Goodfellow et al.&#xA0;point out that <em>any</em> negative log-likelihood is a cross entropy between the training data and the probability distribution predicted by the model.<a href="#fn159" class="footnote-ref" id="fnref159" role="doc-noteref"><sup>159</sup></a></li>
</ul>
<h3 id="decision-trees">Decision trees</h3>
<ul>
<li>Freund, Y. &amp; Schapire, R.E. (1997). <a href="https://doi.org/10.1006/jcss.1997.1504">A decision-theoretic generalization of on-line learning and an application to boosting</a>. (AdaBoost)<a href="#fn160" class="footnote-ref" id="fnref160" role="doc-noteref"><sup>160</sup></a></li>
<li>Chen, T. &amp; Guestrin, C. (2016). <a href="https://arxiv.org/abs/1603.02754">Xgboost: A scalable tree boosting system</a>.<a href="#fn161" class="footnote-ref" id="fnref161" role="doc-noteref"><sup>161</sup></a></li>
<li>Aytekin, C. (2022). <a href="https://arxiv.org/abs/2210.05189">Neural networks are decision trees</a>.<a href="#fn162" class="footnote-ref" id="fnref162" role="doc-noteref"><sup>162</sup></a></li>
<li>Grinsztajn, L., Oyallon, E., &amp; Varoquaux, G. (2022). <a href="https://arxiv.org/abs/2207.08815">Why do tree-based models still outperform deep learning on tabular data?</a><a href="#fn163" class="footnote-ref" id="fnref163" role="doc-noteref"><sup>163</sup></a></li>
<li>Coadou, Y. (2022). <a href="https://arxiv.org/abs/2206.09645">Boosted decision trees</a>.<a href="#fn164" class="footnote-ref" id="fnref164" role="doc-noteref"><sup>164</sup></a></li>
</ul>
<h3 id="clustering">Clustering</h3>
<ul>
<li>unsupervised</li>
<li>Gaussian Mixture Models (GMMs)
<ul>
<li>Gaussian discriminant analysis</li>
<li><span class="math inline">\(\chi^2\)</span></li>
</ul></li>
<li>Generalized Linear Models (GLMs)
<ul>
<li>Exponential family of PDFs</li>
<li>Multinoulli <span class="math inline">\(\mathrm{Cat}(x|\mu)\)</span></li>
<li>GLMs</li>
</ul></li>
<li>EM algorithm
<ul>
<li><span class="math inline">\(k\)</span>-means</li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Clustering_high-dimensional_data">Clustering high-dimensional data</a>
<ul>
<li><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"><em>t</em>-distributed stochastic neighbor embedding (t-SNE)</a></li>
<li>Slonim, N., Atwal, G.S., Tkacik, G. &amp; Bialek, W. (2005). <a href="https://arxiv.org/abs/q-bio/0511043">Information-based clustering</a>.<a href="#fn165" class="footnote-ref" id="fnref165" role="doc-noteref"><sup>165</sup></a></li>
</ul></li>
<li><a href="https://en.wikipedia.org/wiki/Topological_data_analysis">Topological data analysis</a>
<ul>
<li>Dindin, M. (2018). <a href="https://towardsdatascience.com/tda-to-rule-them-all-tomato-clustering-878e03394a1">TDA To Rule Them All: ToMATo Clustering</a>.</li>
</ul></li>
<li>Relationship of clustering and autoencoding
<ul>
<li>Olah, C. (2014). <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural networks, manifolds, and topology</a>.</li>
<li>Batson et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2102.08380">Topological obstructions to autoencoding</a>.<a href="#fn166" class="footnote-ref" id="fnref166" role="doc-noteref"><sup>166</sup></a></li>
</ul></li>
<li>&#x201C;What are the true clusters?&#x201D;<a href="#fn167" class="footnote-ref" id="fnref167" role="doc-noteref"><sup>167</sup></a>
<ul>
<li>See also:
<ul>
<li><a href="#algorithmic-information-theory">Algorithmic information theory</a></li>
<li><a href="scientific-realism.html#constructive-empiricism">Constructive empiricism</a></li>
</ul></li>
</ul></li>
<li>Lauc, D. (2020). Machine learning and the philosophical problems of induction.<a href="#fn168" class="footnote-ref" id="fnref168" role="doc-noteref"><sup>168</sup></a></li>
<li>Ronen, M., Finder, S.E., &amp; Freifeld, O. (2022). <a href="https://arxiv.org/abs/2203.14309">DeepDPM: Deep clustering with an unknown number of clusters</a>.<a href="#fn169" class="footnote-ref" id="fnref169" role="doc-noteref"><sup>169</sup></a></li>
<li>Fang, Z. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2210.14707">Is out-of-distribution detection learnable?</a>.<a href="#fn170" class="footnote-ref" id="fnref170" role="doc-noteref"><sup>170</sup></a></li>
</ul>
<p>See also:</p>
<ul>
<li><a href="#curse-of-dimensionality">Curse of dimensionality</a></li>
<li><a href="#surrogate-models">Surrogate models</a></li>
</ul>
<h2 id="deep-learning">Deep learning</h2>
<h3 id="introduction-7">Introduction</h3>
<ul>
<li>Conceptual reviews of deep learning
<ul>
<li>Lower to higher level representations<a href="#fn171" class="footnote-ref" id="fnref171" role="doc-noteref"><sup>171</sup></a></li>
<li>LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Review: Deep learning.<a href="#fn172" class="footnote-ref" id="fnref172" role="doc-noteref"><sup>172</sup></a></li>
<li>Sutskever, I. (2015). <a href="https://web.archive.org/web/20220728224752/http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">A brief overview of deep learning</a>.<a href="#fn173" class="footnote-ref" id="fnref173" role="doc-noteref"><sup>173</sup></a></li>
<li><em>Deep Learning</em><a href="#fn174" class="footnote-ref" id="fnref174" role="doc-noteref"><sup>174</sup></a></li>
<li>Kaplan, J. (2019). <a href="https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf">Notes on contemporary machine learning</a>.<a href="#fn175" class="footnote-ref" id="fnref175" role="doc-noteref"><sup>175</sup></a></li>
<li>Raissi, M. (2017). <a href="https://maziarraissi.github.io/teaching/1_deep_learning_tutorial/">Deep learning tutorial</a>.</li>
</ul></li>
<li>Backpropagation
<ul>
<li>Rumelhart, D.E., Hinton, G.E., &amp; Williams, R.J. (1986). <a href="https://www.nature.com/articles/323533a0.pdf">Learning representations by back-propagating errors</a>.<a href="#fn176" class="footnote-ref" id="fnref176" role="doc-noteref"><sup>176</sup></a></li>
<li>Schmidhuber&#x2019;s <a href="http://people.idsia.ch/~juergen/critique-honda-prize-hinton.html">Critique of Honda Prize for Dr.&#xA0;Hinton</a>.</li>
<li>Schmidhuber: <a href="http://people.idsia.ch/~juergen/who-invented-backpropagation.html">Who invented backpropagation?</a></li>
<li>Scmidhuber: <a href="https://people.idsia.ch/~juergen/most-cited-neural-nets.html">The most cited neural networks all build on work done in my labs</a>.</li>
<li>LeCun, Y. &amp; Bottou, L. (1998). <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">Efficient BackProp</a>.<a href="#fn177" class="footnote-ref" id="fnref177" role="doc-noteref"><sup>177</sup></a></li>
</ul></li>
<li>Pedagogy
<ul>
<li>Bekman, S. (2023). <a href="https://github.com/stas00/ml-engineering">Machine Learning Engineering Open Book</a>.</li>
<li>Labonne, M. (2023). <a href="https://github.com/mlabonne/llm-course">Large Language Model Course</a>.</li>
<li>Microsoft. (2023). <a href="https://github.com/microsoft/generative-ai-for-beginners">Generative AI for Beginners</a>.</li>
</ul></li>
<li>Practical guides
<ul>
<li>Bottou, L. (1998). <a href="https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/">Stochastic gradient descent tricks</a>.<a href="#fn178" class="footnote-ref" id="fnref178" role="doc-noteref"><sup>178</sup></a></li>
<li>Bengio, Y. (2012). <a href="https://arxiv.org/abs/1206.5533">Practical recommendations for gradient-based training of deep architectures</a>.</li>
<li>Hao, L. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1712.09913">Visualizing the loss landscape of neural nets</a>.</li>
</ul></li>
<li>Discussion
<ul>
<li>Norvig, P. (2011). <a href="https://norvig.com/chomsky.html">On Chomsky and the Two Cultures of Statistical Learning</a>.<a href="#fn179" class="footnote-ref" id="fnref179" role="doc-noteref"><sup>179</sup></a></li>
<li>Sutton, R. (2019). <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">The bitter lesson</a>.<a href="#fn180" class="footnote-ref" id="fnref180" role="doc-noteref"><sup>180</sup></a></li>
<li>Frankle, J. &amp; Carbin, M. (2018). <a href="https://arxiv.org/abs/1803.03635">The lottery ticket hypothesis: Finding sparse, trainable neural networks</a>.<a href="#fn181" class="footnote-ref" id="fnref181" role="doc-noteref"><sup>181</sup></a></li>
<li><a href="https://www.aimyths.org/">AIMyths.com</a></li>
</ul></li>
</ul>
<figure>
<img src="img/bengio-raw-to-higher-rep.png" id="fig:bengio-raw-to-higher-rep" alt="Figure 11: Raw input image is transformed into gradually higher levels of representation." /><figcaption aria-hidden="true">Figure 11: Raw input image is transformed into gradually higher levels of representation.<a href="#fn182" class="footnote-ref" id="fnref182" role="doc-noteref"><sup>182</sup></a></figcaption>
</figure>
<h3 id="deep-double-descent">Deep double descent</h3>
<ul>
<li>Bias and variance trade-off. See <a href="#bias-and-variance">Bias and variance</a>.</li>
<li>MSE vs model capacity</li>
</ul>
<p>Papers:</p>
<ul>
<li>Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S. (2019). <a href="https://arxiv.org/abs/1812.11118">Reconciling modern machine-learning practice and the classical bias-variance trade-off</a>.<a href="#fn183" class="footnote-ref" id="fnref183" role="doc-noteref"><sup>183</sup></a></li>
<li>Muthukumar, V., Vodrahalli, K., Subramanian, V., &amp; Sahai, A. (2019). <a href="https://arxiv.org/abs/1903.09139">Harmless interpolation of noisy data in regression</a>.<a href="#fn184" class="footnote-ref" id="fnref184" role="doc-noteref"><sup>184</sup></a></li>
<li>Nakkiran, P., Kaplun, G., Bansal, Y., Yang, T., Barak, B., &amp; Sutskever, I. (2019). <a href="https://arxiv.org/abs/1912.02292">Deep double descent: Where bigger models and more data hurt</a>.<a href="#fn185" class="footnote-ref" id="fnref185" role="doc-noteref"><sup>185</sup></a></li>
<li>Chang, X., Li, Y., Oymak, S., &amp; Thrampoulidis, C. (2020). <a href="https://arxiv.org/abs/2012.08749">Provable benefits of overparameterization in model compression: From double descent to pruning neural networks</a>.<a href="#fn186" class="footnote-ref" id="fnref186" role="doc-noteref"><sup>186</sup></a></li>
<li>Holzm&#xFC;ller, D. (2020). <a href="https://arxiv.org/abs/2010.01851">On the universality of the double descent peak in ridgeless regression</a>.<a href="#fn187" class="footnote-ref" id="fnref187" role="doc-noteref"><sup>187</sup></a></li>
<li>Dar, Y., Muthukumar, V., &amp; Baraniuk, R.G. (2021). <a href="https://arxiv.org/abs/2109.02355">A farewell to the bias-variance tradeoff? An overview of the theory of overparameterized machine learning</a>.<a href="#fn188" class="footnote-ref" id="fnref188" role="doc-noteref"><sup>188</sup></a></li>
<li>Balestriero, R., Pesenti, J., &amp; LeCun, Y. (2021). <a href="https://arxiv.org/abs/2110.09485">Learning in high dimension always amounts to extrapolation</a>.<a href="#fn189" class="footnote-ref" id="fnref189" role="doc-noteref"><sup>189</sup></a></li>
<li>Belkin, M. (2021). <a href="https://arxiv.org/abs/2105.14368">Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation</a>.<a href="#fn190" class="footnote-ref" id="fnref190" role="doc-noteref"><sup>190</sup></a></li>
<li>Nagarajan, V. (2021). <a href="https://arxiv.org/abs/2110.08922">Explaining generalization in deep learning: progress and fundamental limits</a>.<a href="#fn191" class="footnote-ref" id="fnref191" role="doc-noteref"><sup>191</sup></a></li>
<li>Bach, F. (2022). <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf"><em>Learning Theory from First Principles</em></a>.<a href="#fn192" class="footnote-ref" id="fnref192" role="doc-noteref"><sup>192</sup></a></li>
<li>Barak, B. (2022). <a href="https://windowsontheory.org/2022/06/20/the-uneasy-relationship-between-deep-learning-and-classical-statistics/">The uneasy relationship between deep learning and (classical) statistics</a>.</li>
<li>Ghosh, N. &amp; Belkin, M. (2022). <a href="https://arxiv.org/abs/2207.11621">A universal trade-off between the model size, test loss, and training loss of linear predictors</a>.<a href="#fn193" class="footnote-ref" id="fnref193" role="doc-noteref"><sup>193</sup></a></li>
<li>Singh, S.P., Lucchi, A., Hofmann, T., &amp; Sch&#xF6;lkopf, B. (2022). <a href="https://arxiv.org/abs/2203.07337">Phenomenology of double descent in finite-width neural networks</a>.<a href="#fn194" class="footnote-ref" id="fnref194" role="doc-noteref"><sup>194</sup></a></li>
<li>Hastie, T., Montanari, A., Rosset, S., &amp; Tibshirani, R. J. (2022). <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9481183/">Surprises in high-dimensional ridgeless least squares interpolation</a>.<a href="#fn195" class="footnote-ref" id="fnref195" role="doc-noteref"><sup>195</sup></a></li>
<li>Bubeck, S. &amp; Sellke, M. (2023). <a href="https://dl.acm.org/doi/full/10.1145/3578580">A universal law of robustness via isoperimetry</a>.<a href="#fn196" class="footnote-ref" id="fnref196" role="doc-noteref"><sup>196</sup></a></li>
<li>Gamba, M., Englesson, E., Bj&#xF6;rkman, M., &amp; Azizpour, H. (2022). <a href="https://arxiv.org/abs/2209.10080">Deep double descent via smooth interpolation</a>.<a href="#fn197" class="footnote-ref" id="fnref197" role="doc-noteref"><sup>197</sup></a></li>
<li>Schaeffer, R. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2303.14151">Double descent demystified: Identifying, interpreting &amp; ablating the sources of a deep learning puzzle</a>.<a href="#fn198" class="footnote-ref" id="fnref198" role="doc-noteref"><sup>198</sup></a></li>
<li>Yang, T. &amp; Suzuki, J. (2023). <a href="https://arxiv.org/abs/2305.16179">Dropout drops double descent</a>.<a href="#fn199" class="footnote-ref" id="fnref199" role="doc-noteref"><sup>199</sup></a></li>
<li>Maddox, W.J., Benton, G., &amp; Wilson, A.G. (2023). <a href="https://arxiv.org/abs/2003.02139">Rethinking parameter counting in deep models: Effective dimensionality revisited</a>.<a href="#fn200" class="footnote-ref" id="fnref200" role="doc-noteref"><sup>200</sup></a></li>
</ul>
<p>Blogs:</p>
<ul>
<li>Hubinger, E. (2019). <a href="https://www.lesswrong.com/posts/FRv7ryoqtvSuqBxuT/understanding-deep-double-descent">Understanding deep double descent</a>. <em>LessWrong</em>.</li>
<li>OpenAI. (2019). <a href="https://openai.com/blog/deep-double-descent/">Deep double descent</a>.</li>
<li>Steinhardt, J. (2022). <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">More is different for AI</a>.<a href="#fn201" class="footnote-ref" id="fnref201" role="doc-noteref"><sup>201</sup></a></li>
<li>Henighan, T. et al.&#xA0;(2023). <a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html">Superposition, memorization, and double descent</a>.<a href="#fn202" class="footnote-ref" id="fnref202" role="doc-noteref"><sup>202</sup></a></li>
</ul>
<p>Twitter threads:</p>
<ul>
<li>Daniela Witten. (2020). Twitter thread: <a href="https://twitter.com/daniela_witten/status/1292293104855158784">The bias-variance trade-off &amp; double descent</a>.</li>
<li>Fran&#xE7;ois Fleuret. (2020). Twitter thread: <a href="https://twitter.com/francoisfleuret/status/1269301689095503872">The double descent with polynomial regression</a>.</li>
<li>adad8m. (2022). Twitter thread: <a href="https://twitter.com/adad8m/status/1582231644223987712">The double descent with polynomial regression</a>.</li>
<li>Peyman Milanfar. (2022). Twitter thread: <a href="https://twitter.com/docmilanfar/status/1477838376996769792">The perpetually undervalued least-squares</a>.</li>
<li>Pierre Ablin. (2023). Twitter thread: <a href="https://twitter.com/PierreAblin/status/1678689910683836416">The double descent with polynomial regression</a>.</li>
</ul>
<h3 id="regularization">Regularization</h3>
<p>Regularization = any change we make to the training algorithm in order to reduce the generalization error but not the training error.<a href="#fn203" class="footnote-ref" id="fnref203" role="doc-noteref"><sup>203</sup></a></p>
<p>Most common regularizations:</p>
<ul>
<li>L2 Regularization</li>
<li>L1 Regularization</li>
<li>Data Augmentation</li>
<li>Dropout</li>
<li>Early Stopping</li>
</ul>
<p>Papers:</p>
<ul>
<li>Loshchilov, I. &amp; Hutter, F. (2019). <a href="https://arxiv.org/abs/1711.05101">Decoupled weight decay regularization</a>.</li>
<li>Chen, S., Dobriban, E., &amp; Lee, J. H. (2020). <a href="https://arxiv.org/abs/1907.10905">A group theoretic framework for data augmentation</a>.<a href="#fn204" class="footnote-ref" id="fnref204" role="doc-noteref"><sup>204</sup></a></li>
</ul>
<h3 id="batch-size-vs-learning-rate">Batch size vs learning rate</h3>
<p>Papers:</p>
<ol type="1">
<li>Keskar, N.S. et al.&#xA0;(2016). <a href="https://arxiv.org/abs/1609.04836">On large-batch training for deep learning: Generalization gap and sharp minima</a>.</li>
</ol>
<blockquote>
<p>[L]arge-batch methods tend to converge to sharp minimizers of the training and testing functions&#x2014;and as is well known&#x2014;sharp minima lead to poorer generalization. In contrast, small-batch methods consistently converge to flat minimizers, and our experiments support a commonly held view that this is due to the inherent noise in the gradient estimation.</p>
</blockquote>
<ol start="2" type="1">
<li><p>Hoffer, E. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1705.08741">Train longer, generalize better: closing the generalization gap in large batch training of neural networks</a>.</p>
<ul>
<li><span class="math inline">\(\eta \propto \sqrt{m}\)</span></li>
</ul></li>
<li><p>Goyal, P. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1706.02677">Accurate large minibatch SGD: Training ImageNet in 1 hour</a>.</p>
<ul>
<li><span class="math inline">\(\eta \propto m\)</span></li>
</ul></li>
<li><p>You, Y. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1708.03888">Large batch training of convolutional networks</a>.</p>
<ul>
<li>Layer-wise Adaptive Rate Scaling (LARS)</li>
</ul></li>
<li><p>You, Y. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1709.05011">ImageNet training in minutes</a>.</p>
<ul>
<li>Layer-wise Adaptive Rate Scaling (LARS)</li>
</ul></li>
<li><p>Jastrzebski, S. (2018). <a href="https://arxiv.org/abs/1711.04623">Three factors influencing minima in SGD</a>.</p>
<ul>
<li><span class="math inline">\(\eta \propto m\)</span></li>
</ul></li>
<li><p>Smith, S.L. &amp; Le, Q.V. (2018). <a href="https://arxiv.org/abs/1710.06451">A Bayesian Perspective on Generalization and Stochastic Gradient Descent</a>.</p></li>
<li><p>Smith, S.L. et al.&#xA0;(2018). <a href="https://arxiv.org/abs/1711.00489">Don&#x2019;t decay the learning rate, increase the batch size</a>.</p>
<ul>
<li><span class="math inline">\(m \propto \eta\)</span></li>
</ul></li>
<li><p>Masters, D. &amp; Luschi, C. (2018). <a href="https://arxiv.org/abs/1804.07612">Revisiting small batch training for deep neural networks</a>.</p></li>
</ol>
<blockquote>
<p>This linear scaling rule has been widely adopted, e.g., in Krizhevsky (2014), Chen et al.&#xA0;(2016), Bottou et al.&#xA0;(2016), Smith et al.&#xA0;(2017) and Jastrzebski et al.&#xA0;(2017).</p>
<p>On the other hand, as shown in Hoffer et al.&#xA0;(2017), when <span class="math inline">\(m \ll M\)</span>, the covariance matrix of the weight update <span class="math inline">\(\mathrm{Cov(\eta \Delta\theta)}\)</span> scales linearly with the quantity <span class="math inline">\(\eta^2/m\)</span>.</p>
<p>This implies that, adopting the linear scaling rule, an increase in the batch size would also result in a linear increase in the covariance matrix of the weight update <span class="math inline">\(\eta \Delta\theta\)</span>. Conversely, to keep the scaling of the covariance of the weight update vector <span class="math inline">\(\eta \Delta\theta\)</span> constant would require scaling <span class="math inline">\(\eta\)</span> with the square root of the batch size <span class="math inline">\(m\)</span> (Krizhevsky, 2014; Hoffer et al., 2017).</p>
</blockquote>
<ol start="10" type="1">
<li><p>Lin, T. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/1808.07217">Don&#x2019;t use large mini-batches, use local SGD</a>.<br />
- Post-local SGD.</p></li>
<li><p>Golmant, N. et al.&#xA0;(2018). <a href="https://arxiv.org/abs/1811.12941">On the computational inefficiency of large batch sizes for stochastic gradient descent</a>.</p></li>
</ol>
<blockquote>
<p>Scaling the learning rate as <span class="math inline">\(\eta \propto \sqrt{m}\)</span> attempts to keep the weight increment length statistics constant, but the distance between SGD iterates is governed more by properties of the objective function than the ratio of learning rate to batch size. This rule has also been found to be empirically sub-optimal in various problem domains. &#x2026; There does not seem to be a simple training heuristic to improve large batch performance in general.</p>
</blockquote>
<ol start="12" type="1">
<li>McCandlish, S. et al.&#xA0;(2018). <a href="https://arxiv.org/abs/1812.06162">An empirical model of large-batch training</a>.
<ul>
<li>Critical batch size</li>
</ul></li>
<li>Shallue, C.J. et al.&#xA0;(2018). <a href="https://arxiv.org/abs/1811.03600">Measuring the effects of data parallelism on neural network training</a>.</li>
</ol>
<blockquote>
<p>In all cases, as the batch size grows, there is an initial period of perfect scaling (<span class="math inline">\(b\)</span>-fold benefit, indicated with a dashed line on the plots) where the steps needed to achieve the error goal halves for each doubling of the batch size. However, for all problems, this is followed by a region of diminishing returns that eventually leads to a regime of maximal data parallelism where additional parallelism provides no benefit whatsoever.</p>
</blockquote>
<ol start="14" type="1">
<li>Jastrzebski, S. et al.&#xA0;(2018). <a href="https://link.springer.com/chapter/10.1007/978-3-030-01424-7_39">Width of minima reached by stochastic gradient descent is influenced by learning rate to batch size ratio</a>.
<ul>
<li><span class="math inline">\(\eta \propto m\)</span></li>
</ul></li>
</ol>
<blockquote>
<p>We show this experimentally in Fig. 5, where similar learning dynamics and final performance can be observed when simultaneously multiplying the learning rate and batch size by a factor up to a certain limit.</p>
</blockquote>
<ol start="15" type="1">
<li>You, Y. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1901.08256">Large-batch training for LSTM and beyond</a>.
<ul>
<li>Warmup and use <span class="math inline">\(\eta \propto m\)</span></li>
</ul></li>
</ol>
<blockquote>
<p>[W]e propose linear-epoch gradual-warmup approach in this paper. We call this approach Leg-Warmup (LEGW). LEGW enables a Sqrt Scaling scheme in practice and as a result we achieve much better performance than the previous Linear Scaling learning rate scheme. For the GNMT application (Seq2Seq) with LSTM, we are able to scale the batch size by a factor of 16 without losing accuracy and without tuning the hyper-parameters mentioned above.</p>
</blockquote>
<ol start="16" type="1">
<li>You, Y. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1904.00962">Large batch optimization for deep learning: Training BERT in 76 minutes</a>.
<ul>
<li>LARS and LAMB</li>
</ul></li>
<li>Zhang, G. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1907.04164">Which algorithmic choices matter at which batch sizes? Insights from a Noisy Quadratic Model</a>.</li>
</ol>
<blockquote>
<p>Consistent with the empirical results of Shallue et al.&#xA0;(2018), each optimizer shows two distinct regimes: a small-batch (stochastic) regime with perfect linear scaling, and a large-batch (deterministic) regime insensitive to batch size. We call the phase transition between these regimes the critical batch size.</p>
</blockquote>
<ol start="18" type="1">
<li>Li, Y., Wei, C., &amp; Ma, T. (2019). <a href="https://arxiv.org/abs/1907.04595">Towards explaining the regularization effect of initial large learning rate in training neural networks</a>.</li>
</ol>
<blockquote>
<p>Our analysis reveals that more SGD noise, or larger learning rate, biases the model towards learning &#x201C;generalizing&#x201D; kernels rather than &#x201C;memorizing&#x201D; kernels.</p>
</blockquote>
<ol start="19" type="1">
<li><p>Kaplan, J. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2001.08361">Scaling laws for neural language models</a>.</p></li>
<li><p>Jastrzebski, S. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2002.09572">The break-even point on optimization trajectories of deep neural networks</a>.</p></li>
</ol>
<p>Blogs:</p>
<ul>
<li>Shen, K. (2018). <a href="https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e">Effect of batch size on training dynamics</a>.</li>
<li>Chang, D. (2020). <a href="https://medium.com/deep-learning-experiments/effect-of-batch-size-on-neural-net-training-c5ae8516e57">Effect of batch size on neural net training</a>.</li>
</ul>
<h3 id="normalization">Normalization</h3>
<ul>
<li>BatchNorm</li>
<li>LayerNorm, GroupNorm</li>
<li>OnlineNorm: Chiley, V. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1905.05894">Online normalization for training neural networks</a>.<a href="#fn205" class="footnote-ref" id="fnref205" role="doc-noteref"><sup>205</sup></a></li>
<li>Kiani, B., Balestriero, R., LeCun, Y., &amp; Lloyd, S. (2022). <a href="https://arxiv.org/abs/2203.05483">projUNN: efficient method for training deep networks with unitary matrices</a>.<a href="#fn206" class="footnote-ref" id="fnref206" role="doc-noteref"><sup>206</sup></a></li>
</ul>
<h3 id="computer-vision">Computer vision</h3>
<ul>
<li>Computer Vision (CV)</li>
<li>Fukushima: neocognitron<a href="#fn207" class="footnote-ref" id="fnref207" role="doc-noteref"><sup>207</sup></a></li>
<li>LeNet-5</li>
<li>LeCun, Y. (1989). Generalization and network design strategies.</li>
<li>LeCun: OCR with backpropagation<a href="#fn208" class="footnote-ref" id="fnref208" role="doc-noteref"><sup>208</sup></a></li>
<li>LeCun: LeNet-5<a href="#fn209" class="footnote-ref" id="fnref209" role="doc-noteref"><sup>209</sup></a></li>
<li>Ciresan: MCDNN<a href="#fn210" class="footnote-ref" id="fnref210" role="doc-noteref"><sup>210</sup></a></li>
<li>Krizhevsky, Sutskever, and Hinton: AlexNet<a href="#fn211" class="footnote-ref" id="fnref211" role="doc-noteref"><sup>211</sup></a></li>
<li>VGG<a href="#fn212" class="footnote-ref" id="fnref212" role="doc-noteref"><sup>212</sup></a></li>
<li>ResNet<a href="#fn213" class="footnote-ref" id="fnref213" role="doc-noteref"><sup>213</sup></a>
<ul>
<li>ResNet is performing a forward Euler discretisation of the ODE: <span class="math inline">\(\dot{x} = \sigma(F(x))\)</span>.<a href="#fn214" class="footnote-ref" id="fnref214" role="doc-noteref"><sup>214</sup></a></li>
</ul></li>
<li>MobileNet<a href="#fn215" class="footnote-ref" id="fnref215" role="doc-noteref"><sup>215</sup></a></li>
<li>Neural ODEs<a href="#fn216" class="footnote-ref" id="fnref216" role="doc-noteref"><sup>216</sup></a></li>
<li>EfficientNet<a href="#fn217" class="footnote-ref" id="fnref217" role="doc-noteref"><sup>217</sup></a></li>
<li>VisionTransformer<a href="#fn218" class="footnote-ref" id="fnref218" role="doc-noteref"><sup>218</sup></a></li>
<li>EfficientNetV2<a href="#fn219" class="footnote-ref" id="fnref219" role="doc-noteref"><sup>219</sup></a></li>
<li>gMLP<a href="#fn220" class="footnote-ref" id="fnref220" role="doc-noteref"><sup>220</sup></a></li>
<li>Dhariwal, P. &amp; Nichol, A. (2021). <a href="https://arxiv.org/abs/2105.05233">Diffusion models beat GANs on image synthesis</a>.<a href="#fn221" class="footnote-ref" id="fnref221" role="doc-noteref"><sup>221</sup></a></li>
<li>Liu, Y. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2111.06091">A survey of visual transformers</a>.<a href="#fn222" class="footnote-ref" id="fnref222" role="doc-noteref"><sup>222</sup></a></li>
<li>Ingrosso, A. &amp; Goldt, S. (2022). <a href="https://arxiv.org/abs/2202.00565">Data-driven emergence of convolutional structure in neural networks</a>.<a href="#fn223" class="footnote-ref" id="fnref223" role="doc-noteref"><sup>223</sup></a></li>
<li>Park, N. &amp; Kim, S. (2022). <a href="https://arxiv.org/abs/2202.06709">How do vision transformers work?</a><a href="#fn224" class="footnote-ref" id="fnref224" role="doc-noteref"><sup>224</sup></a></li>
</ul>
<p>Resources:</p>
<ul>
<li>Neptune.ai. (2021). <a href="https://neptune.ai/blog/object-detection-algorithms-and-libraries">Object detection algorithms and libraries</a>.</li>
<li><a href="https://github.com/facebookresearch/vissl">facebookresearch/vissl</a></li>
<li><a href="https://pytorch-geometric.readthedocs.io/en/latest/">PyTorch Geometric (PyG)</a></li>
</ul>
<h3 id="natural-language-processing">Natural language processing</h3>
<h4 id="introduction-8">Introduction</h4>
<ul>
<li>Natural Language Processing (NLP)</li>
<li>History
<ul>
<li>Firth, J.R. (1957): &#x201C;You shall know a word by the company it keeps.&#x201D;<a href="#fn225" class="footnote-ref" id="fnref225" role="doc-noteref"><sup>225</sup></a></li>
<li>Nirenburg, S. (1996). <a href="https://web.archive.org/web/20120414235847/http://ilit.umbc.edu/SergeiPub/bar-hillel.pdf">Bar Hillel and Machine Translation: Then and Now</a>.<a href="#fn226" class="footnote-ref" id="fnref226" role="doc-noteref"><sup>226</sup></a></li>
<li>Hutchins, J. (2000). <a href="https://web.archive.org/web/20190702192948/http://www.hutchinsweb.me.uk/Bar-Hillel-2000.pdf">Yehoshua Bar-Hillel: A philosophers&#x2019; contribution to machine translation</a>.<a href="#fn227" class="footnote-ref" id="fnref227" role="doc-noteref"><sup>227</sup></a></li>
</ul></li>
<li>Textbooks
<ul>
<li>Jurafsky, D. &amp; Martin, J.H. (2022). <a href="https://web.stanford.edu/~jurafsky/slp3/ed3book_jan122022.pdf"><em>Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition</em></a>.<a href="#fn228" class="footnote-ref" id="fnref228" role="doc-noteref"><sup>228</sup></a></li>
<li>Liu, Z., Lin, Y., &amp; Sun, M. (2023). <a href="https://link.springer.com/book/10.1007/978-981-99-1600-9"><em>Representation Learning for Natural Language Processing</em></a>.<a href="#fn229" class="footnote-ref" id="fnref229" role="doc-noteref"><sup>229</sup></a></li>
</ul></li>
</ul>
<h4 id="word2vec">word2vec</h4>
<ul>
<li>Mikolov<a href="#fn230" class="footnote-ref" id="fnref230" role="doc-noteref"><sup>230</sup></a></li>
<li>Julia Bazi&#x144;ska</li>
<li>Olah, C. (2014). <a href="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep learning, NLP, and representations</a>.</li>
<li>Alammar, J. (2019). <a href="https://jalammar.github.io/illustrated-word2vec/">The illustrated word2vec</a>.</li>
<li>Migdal, P. (2017). <a href="https://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html">king - man + woman is queen; but why?</a></li>
<li>Kun, J. (2018). <em>A Programmer&#x2019;s Introduction to Mathematics</em>.
<ul>
<li>Word vectors have semantic linear structure<a href="#fn231" class="footnote-ref" id="fnref231" role="doc-noteref"><sup>231</sup></a></li>
</ul></li>
<li>Ethayarajh, K. (2019). <a href="https://kawine.github.io/blog/nlp/2019/06/21/word-analogies.html">Word embedding analogies: Understanding King - Man + Woman = Queen</a>.</li>
<li>Allen, C. (2019). <a href="https://carl-allen.github.io/nlp/2019/07/01/explaining-analogies-explained.html">&#x201C;Analogies Explained&#x201D; &#x2026; Explained</a>.</li>
</ul>
<h4 id="rnns">RNNs</h4>
<ul>
<li>RNNs and LSTMs</li>
<li>Hochreiter, S. &amp; Schmidhuber, J. (1997). Long short-term memory.<a href="#fn232" class="footnote-ref" id="fnref232" role="doc-noteref"><sup>232</sup></a></li>
<li>Graves, A. (2013). <a href="https://arxiv.org/abs/1308.0850">Generating sequences with recurrent neural networks</a>.<a href="#fn233" class="footnote-ref" id="fnref233" role="doc-noteref"><sup>233</sup></a>
<ul>
<li>Auto-regressive language modeling</li>
</ul></li>
<li>Olah, C. (2015). <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM networks</a>.</li>
<li>Karpathy, A. (2015). <a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">The unreasonable effectiveness of recurrent neural networks</a>.</li>
</ul>
<p>Chain rule of language modeling (chain rule of probability):</p>
<p><span class="math display">\[ P(x_1, \ldots, x_T) = P(x_1, \ldots, x_{n-1}) \prod_{t=n}^{T} P(x_t | x_1 \ldots x_{t-1}) \label{eq:chain_rule_of_lm} \]</span></p>
<p>or for the whole sequence:</p>
<p><span class="math display">\[ P(x_1, \ldots, x_T) = \prod_{t=1}^{T} P(x_t | x_1 \ldots x_{t-1}) \label{eq:chain_rule_of_lm_2} \]</span></p>
<p><span class="math display">\[ = P(x_1) \: P(x_2 | x_1) \: P(x_3 | x_1 x_2) \: P(x_4 | x_1 x_2 x_3) \ldots \]</span></p>
<p>A <em>language model</em> (LM), predicts the next token given previous context. The output of the model is a vector of logits, which is given to a softmax to convert to probabilities for the next token.</p>
<p><span class="math display">\[ P(x_t | x_1 \ldots x_{t-1}) = \mathrm{softmax}\left( \mathrm{model}(x_1 \ldots x_{t-1}) \right) \]</span></p>
<p><em>Auto-regressive</em> inference follows this chain rule. If done with greedy search:</p>
<p><span class="math display">\[ \hat{x}_t = \underset{x_t \in V}{\mathrm{argmax}} \: P(x_t | x_1 \ldots x_{t-1}) \label{eq:greedy_search} \]</span></p>
<p>Beam search:</p>
<ul>
<li>Beam search as used in NLP is described in Sutskever.<a href="#fn234" class="footnote-ref" id="fnref234" role="doc-noteref"><sup>234</sup></a></li>
<li>Zhang, W. (1998). <a href="https://cdn.aaai.org/AAAI/1998/AAAI98-060.pdf">Complete anytime beam search</a>.<a href="#fn235" class="footnote-ref" id="fnref235" role="doc-noteref"><sup>235</sup></a></li>
<li>Zhou, R. &amp; Hansen, E. A. (2005). <a href="https://cdn.aaai.org/ICAPS/2005/ICAPS05-010.pdf">Beam-stack search: Integrating backtracking with beam search</a>.<a href="#fn236" class="footnote-ref" id="fnref236" role="doc-noteref"><sup>236</sup></a></li>
<li>Collobert, R., Hannun, A., &amp; Synnaeve, G. (2019). <a href="http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf">A fully differentiable beam search decoder</a>.<a href="#fn237" class="footnote-ref" id="fnref237" role="doc-noteref"><sup>237</sup></a></li>
</ul>
<p>Backpropagation through time (BPTT):</p>
<ul>
<li>Werbos, P.J. (1990). <a href="http://www.werbos.com/Neural/BTT.pdf">Backpropagation through time: what it does and how to do it</a>.<a href="#fn238" class="footnote-ref" id="fnref238" role="doc-noteref"><sup>238</sup></a></li>
</ul>
<p>Neural Machine Translation (NMT):</p>
<ul>
<li>Sutskever seq2seq<a href="#fn239" class="footnote-ref" id="fnref239" role="doc-noteref"><sup>239</sup></a></li>
<li>Bahdanau attention<a href="#fn240" class="footnote-ref" id="fnref240" role="doc-noteref"><sup>240</sup></a> and GNMT<a href="#fn241" class="footnote-ref" id="fnref241" role="doc-noteref"><sup>241</sup></a></li>
<li>Review by Stahlberg<a href="#fn242" class="footnote-ref" id="fnref242" role="doc-noteref"><sup>242</sup></a></li>
</ul>
<h4 id="transformers">Transformers</h4>
<figure>
<img src="img/transformer.png" id="fig:transformer" class="tallimg" alt="Figure 12: Diagram of the Transformer model (source: d2l.ai)." /><figcaption aria-hidden="true">Figure 12: Diagram of the Transformer model (source: <a href="https://d2l.ai/chapter_attention-mechanisms/transformer.html">d2l.ai</a>).</figcaption>
</figure>
<p><span class="math display">\[ \mathrm{attention}(Q, K, V) = \mathrm{softmax}\left(\frac{Q\, K^\intercal}{\sqrt{d_k}}\right) V \label{eq:attention} \]</span></p>
<ul>
<li>Attention and Transformers
<ul>
<li>Transformer<a href="#fn243" class="footnote-ref" id="fnref243" role="doc-noteref"><sup>243</sup></a></li>
<li>BERT<a href="#fn244" class="footnote-ref" id="fnref244" role="doc-noteref"><sup>244</sup></a></li>
<li>Alammar, J. (2018). <a href="https://jalammar.github.io/illustrated-bert/">The illustrated BERT</a>.</li>
<li>Horev, R. (2018). <a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270">BERT Explained: State of the art language model for NLP</a>.</li>
<li>Liu, Y. et al.&#xA0;(2019). RoBERTa: <a href="https://arxiv.org/abs/1907.11692">A robustly optimized BERT pretraining approach</a>.<a href="#fn245" class="footnote-ref" id="fnref245" role="doc-noteref"><sup>245</sup></a></li>
<li>Raffel, C. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1910.10683">Exploring the limits of transfer learning with a unified text-to-text transformer</a>. (T5 model)<a href="#fn246" class="footnote-ref" id="fnref246" role="doc-noteref"><sup>246</sup></a></li>
<li>Horan, C. (2021). <a href="https://neptune.ai/blog/bert-and-the-transformer-architecture-reshaping-the-ai-landscape">10 things you need to know about BERT and the transformer architecture that are reshaping the AI landscape</a>.</li>
<li>Video: <a href="https://www.youtube.com/watch?v=XSSTuhyAmnI&amp;t=92s&amp;ab_channel=AriSeff">What are transformer neural networks?</a></li>
<li>Video: <a href="https://www.youtube.com/watch?v=-9vVhYEXeyQ&amp;ab_channel=Peltarion">How to get meaning from text with language model BERT</a>.</li>
<li>ALBERT<a href="#fn247" class="footnote-ref" id="fnref247" role="doc-noteref"><sup>247</sup></a></li>
<li>BART<a href="#fn248" class="footnote-ref" id="fnref248" role="doc-noteref"><sup>248</sup></a></li>
<li>GPT-1,<a href="#fn249" class="footnote-ref" id="fnref249" role="doc-noteref"><sup>249</sup></a> 2,<a href="#fn250" class="footnote-ref" id="fnref250" role="doc-noteref"><sup>250</sup></a> 3<a href="#fn251" class="footnote-ref" id="fnref251" role="doc-noteref"><sup>251</sup></a></li>
<li>Alammar, J. (2019). <a href="https://jalammar.github.io/illustrated-gpt2/">The illustrated GPT-2</a>.</li>
<li>Yang, Z. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1906.08237">XLNet: Generalized autoregressive pretraining for language understanding</a>.<a href="#fn252" class="footnote-ref" id="fnref252" role="doc-noteref"><sup>252</sup></a></li>
<li>Daily Nous: <a href="https://dailynous.com/2020/07/30/philosophers-gpt-3/">Philosophers On GPT-3</a>.</li>
<li>DeepMind&#x2019;s blog posts for more details: <a href="https://deepmind.com/blog/article/AlphaFold-Using-AI-for-scientific-discovery">AlphaFold1</a>, <a href="https://deepmind.com/blog/article/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology">AlphaFold2</a> (2020). Slides from the CASP14 conference are publicly available <a href="https://predictioncenter.org/casp14/doc/presentations/2020_12_01_TS_predictor_AlphaFold2.pdf">here</a>.</li>
<li>Joshi, C. (2020). <a href="https://graphdeeplearning.github.io/post/transformers-are-gnns/">Transformers are GNNs</a>.</li>
<li>Lakshmanamoorthy, R. (2020). <a href="https://analyticsindiamag.com/a-complete-learning-path-to-transformers/">A complete learning path to transformers (with guide to 23 architectures)</a>.</li>
<li>Zaheer, M. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2007.14062">Big Bird: Transformers for longer sequences</a>.<a href="#fn253" class="footnote-ref" id="fnref253" role="doc-noteref"><sup>253</sup></a></li>
<li>Edelman, B.L., Goel, S., Kakade, S., &amp; Zhang, C. (2021). <a href="https://arxiv.org/abs/2110.10090">Inductive biases and variable creation in self-attention mechanisms</a>.<a href="#fn254" class="footnote-ref" id="fnref254" role="doc-noteref"><sup>254</sup></a></li>
<li>Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2022). <a href="https://arxiv.org/abs/2009.06732">Efficient transformers: A survey</a>.<a href="#fn255" class="footnote-ref" id="fnref255" role="doc-noteref"><sup>255</sup></a></li>
<li>Phuong, M. &amp; Hutter, M. (2022). <a href="https://arxiv.org/abs/2207.09238">Formal algorithms for transformers</a>.<a href="#fn256" class="footnote-ref" id="fnref256" role="doc-noteref"><sup>256</sup></a></li>
<li>Chowdhery, A. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2204.02311">PaLM: Scaling language modeling with pathways</a>.<a href="#fn257" class="footnote-ref" id="fnref257" role="doc-noteref"><sup>257</sup></a></li>
<li>Ouyang, L. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2203.02155">Training language models to follow instructions with human feedback</a>. (InstructGPT)<a href="#fn258" class="footnote-ref" id="fnref258" role="doc-noteref"><sup>258</sup></a></li>
<li>OpenAI. (2022). Blog: <a href="https://openai.com/blog/chatgpt/">ChatGPT: Optimizing Language Models for Dialogue</a>.</li>
<li>Wolfram, S. (2023). <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">What is ChatGPT doing&#x2014;and why does it work?</a><a href="#fn259" class="footnote-ref" id="fnref259" role="doc-noteref"><sup>259</sup></a></li>
<li>GPT-4<a href="#fn260" class="footnote-ref" id="fnref260" role="doc-noteref"><sup>260</sup></a></li>
<li>Mohamadi, S., Mujtaba, G., Le, N., Doretto, G., &amp; Adjeroh, D.A. (2023). <a href="https://arxiv.org/abs/2307.04251v1">ChatGPT in the age of generative AI and large language models: A concise survey</a>.<a href="#fn261" class="footnote-ref" id="fnref261" role="doc-noteref"><sup>261</sup></a></li>
<li>3Blue1Brown. (2024). Video: <a href="https://www.youtube.com/watch?v=wjZofJX0v4M">But what is a GPT? Visual intro to Transformers</a>.</li>
</ul></li>
</ul>
<figure>
<img src="img/bert-encoder-block.png" id="fig:bert-encoder-block" class="tallimg" alt="Figure 13: Diagram of the BERT model (source: peltarion.com)." /><figcaption aria-hidden="true">Figure 13: Diagram of the BERT model (source: <a href="https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/blocks/english-bert-encoder">peltarion.com</a>).</figcaption>
</figure>
<ul>
<li>Efficient transformers
<ul>
<li>Dao, T. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2205.14135">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</a>.<a href="#fn262" class="footnote-ref" id="fnref262" role="doc-noteref"><sup>262</sup></a></li>
</ul></li>
<li>What comes after Transformers?
<ul>
<li>Merrill, W. &amp; Sabharwal, A. (2022). <a href="https://arxiv.org/abs/2207.00729">The parallelism tradeoff: Limitations of log-precision transformers</a>.<a href="#fn263" class="footnote-ref" id="fnref263" role="doc-noteref"><sup>263</sup></a></li>
<li>Gu, A., Goel, K., &amp; R&#xE9;, C. (2021). <a href="https://arxiv.org/abs/2111.00396">Efficiently modeling long sequences with structured state spaces</a>.<a href="#fn264" class="footnote-ref" id="fnref264" role="doc-noteref"><sup>264</sup></a></li>
<li>Bulatov, A., Kuratov, Y., &amp; Burtsev, M.S. (2022). <a href="https://arxiv.org/abs/2207.06881">Recurrent memory transformer</a>.<a href="#fn265" class="footnote-ref" id="fnref265" role="doc-noteref"><sup>265</sup></a></li>
<li>Bulatov, A., Kuratov, Y., &amp; Burtsev, M.S. (2023). <a href="https://arxiv.org/abs/2304.11062">Scaling transformer to 1M tokens and beyond with RMT</a>.<a href="#fn266" class="footnote-ref" id="fnref266" role="doc-noteref"><sup>266</sup></a></li>
<li>Bertsch, A., Alon, U., Neubig, G., &amp; Gormley, M.R. (2023). <a href="https://arxiv.org/abs/2305.01625">Unlimiformer: Long-range transformers with unlimited length input</a>.<a href="#fn267" class="footnote-ref" id="fnref267" role="doc-noteref"><sup>267</sup></a></li>
<li>Mialon, G. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2302.07842">Augmented Language Models: a Survey</a>.<a href="#fn268" class="footnote-ref" id="fnref268" role="doc-noteref"><sup>268</sup></a></li>
<li>Peng, B. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2305.13048">RWKV: Reinventing RNNs for the Transformer Era</a>.<a href="#fn269" class="footnote-ref" id="fnref269" role="doc-noteref"><sup>269</sup></a></li>
<li>Sun, Y. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2307.08621">Retentive network: A successor to transformer for large language models</a>.<a href="#fn270" class="footnote-ref" id="fnref270" role="doc-noteref"><sup>270</sup></a></li>
<li>Gu, A. &amp; Dao, T. (2023). <a href="https://arxiv.org/abs/2312.00752">Mamba: Linear-time sequence modeling with selective state spaces</a>.<a href="#fn271" class="footnote-ref" id="fnref271" role="doc-noteref"><sup>271</sup></a></li>
<li>Wang, H. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2310.11453">BitNet: Scaling 1-bit transformers for large language models</a>.<a href="#fn272" class="footnote-ref" id="fnref272" role="doc-noteref"><sup>272</sup></a></li>
<li>Ma, S. et al.&#xA0;(2024). <a href="https://arxiv.org/abs/2402.17764">The era of 1-bit LLMs: All large language models are in 1.58 bits</a>.<a href="#fn273" class="footnote-ref" id="fnref273" role="doc-noteref"><sup>273</sup></a></li>
<li>Ma, X. et al.&#xA0;(2024). <a href="https://arxiv.org/abs/2404.08801">Megalodon: Efficient LLM pretraining and inference with unlimited context length</a>.<a href="#fn274" class="footnote-ref" id="fnref274" role="doc-noteref"><sup>274</sup></a></li>
<li>Bhargava, A., Witkowski, C., Shah, M., &amp; Thomson, M. (2023). <a href="https://arxiv.org/abs/2310.04444">What&#x2019;s the magic word? A control theory of LLM prompting</a>.<a href="#fn275" class="footnote-ref" id="fnref275" role="doc-noteref"><sup>275</sup></a></li>
</ul></li>
</ul>
<h4 id="scaling-laws-in-nlp">Scaling laws in NLP</h4>
<ul>
<li>Hestness, J. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1712.00409">Deep learning scaling is predictable, empirically</a>.<a href="#fn276" class="footnote-ref" id="fnref276" role="doc-noteref"><sup>276</sup></a></li>
<li>Church, K.W. &amp; Hestness, J. (2019). <a href="https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-25-years-of-evaluation/E4330FAEB9202EC490218E3220DDA291">Rationalism and empiricism in artificial intellegence: A survey of 25 years of evaluation [in NLP]</a>.<a href="#fn277" class="footnote-ref" id="fnref277" role="doc-noteref"><sup>277</sup></a></li>
<li>Kaplan, J. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2001.08361">Scaling laws for neural language models</a>.<a href="#fn278" class="footnote-ref" id="fnref278" role="doc-noteref"><sup>278</sup></a></li>
<li>Rae, J.W. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2112.11446">Scaling language models: Methods, analysis &amp; insights from training Gopher</a>.<a href="#fn279" class="footnote-ref" id="fnref279" role="doc-noteref"><sup>279</sup></a></li>
<li>Hoffmann, J. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2203.15556">Training compute-optimal large language models</a> (Chinchilla).<a href="#fn280" class="footnote-ref" id="fnref280" role="doc-noteref"><sup>280</sup></a></li>
<li>Constantin, S. (2023). <a href="https://sarahconstantin.substack.com/p/scaling-laws-for-ai-and-some-implications">&#x201C;Scaling Laws&#x201D; for AI and some implications</a>.</li>
<li>Muennighoff, N. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2305.16264">Scaling data-constrained language models</a>.<a href="#fn281" class="footnote-ref" id="fnref281" role="doc-noteref"><sup>281</sup></a></li>
</ul>
<h4 id="language-understanding">Language understanding</h4>
<ul>
<li>NLU</li>
<li>Mahowald, K. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2301.06627">Dissociating language and thought in large language models: a cognitive perspective</a>.<a href="#fn282" class="footnote-ref" id="fnref282" role="doc-noteref"><sup>282</sup></a></li>
<li>Kosinski, M. (2023). <a href="https://arxiv.org/abs/2302.02083">Theory of mind may have spontaneously emerged in large language models</a>.<a href="#fn283" class="footnote-ref" id="fnref283" role="doc-noteref"><sup>283</sup></a></li>
<li>Chitra, T. &amp; Prior, H. (2023). <a href="https://hackmd.io/@pinged/zk-and-llms">Do language models possess knowledge (soundness)?</a></li>
</ul>
<p>See also:</p>
<ul>
<li><a href="statistics.html#word-meanings">Word meanings</a></li>
</ul>
<h4 id="interpretability">Interpretability</h4>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Grandmother_cell">Grandmother cell</a></li>
<li>Watson, D. &amp; Floridi, L. (2019). The explanation game: A formal framework for interpretable machine learning.<a href="#fn284" class="footnote-ref" id="fnref284" role="doc-noteref"><sup>284</sup></a></li>
<li>Anthropic. (2021). <a href="https://transformer-circuits.pub/2021/framework/index.html">A mathematical framework for transformer circuits</a>.</li>
<li>Anthropic. (2022). <a href="https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html">In-context learning and induction heads</a>.</li>
<li>Gurnee, W. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2305.01610">Finding neurons in a haystack: Case studies with sparse probing</a>.<a href="#fn285" class="footnote-ref" id="fnref285" role="doc-noteref"><sup>285</sup></a></li>
<li>Meng, K., Bau, D., Andonian, A., &amp; Belinkov, Y. (2023). <a href="https://arxiv.org/abs/2202.05262">Locating and editing factual associations in GPT</a>.<a href="#fn286" class="footnote-ref" id="fnref286" role="doc-noteref"><sup>286</sup></a></li>
<li>McDougall, C., Conmy, A., Rushing, C., McGrath, T., &amp; Nanda, N. (2023). <a href="https://arxiv.org/abs/2310.04625">Copy suppression: Comprehensively understanding an attention head</a>.<a href="#fn287" class="footnote-ref" id="fnref287" role="doc-noteref"><sup>287</sup></a></li>
</ul>
<p>Linear probes:</p>
<ul>
<li>Alain, G. &amp; Bengio, Y. (2016). <a href="https://arxiv.org/abs/1610.01644">Understanding intermediate layers using linear classifier probes</a>.<a href="#fn288" class="footnote-ref" id="fnref288" role="doc-noteref"><sup>288</sup></a></li>
<li>Belinkov, Y. (2022). Probing classifiers: Promises, shortcomings, and advances.<a href="#fn289" class="footnote-ref" id="fnref289" role="doc-noteref"><sup>289</sup></a></li>
<li>Gurnee, W. &amp; Tegmark, M. (2023). <a href="https://arxiv.org/abs/2310.02207">Language models represent space and time</a>.<a href="#fn290" class="footnote-ref" id="fnref290" role="doc-noteref"><sup>290</sup></a></li>
</ul>
<h3 id="reinforcement-learning">Reinforcement learning</h3>
<ul>
<li>Reinforcement Learning (RL)</li>
<li><a href="https://en.wikipedia.org/wiki/Dynamic_programming">Dynamic programming</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bellman_equation">Bellman equation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Backward_induction">Backward induction</a>
<ul>
<li>John von Neumann &amp; Oskar Morgenstern. (1944). <em>Theory of Games and Economic Behavior</em>.</li>
</ul></li>
</ul>
<p>Pedagogy:</p>
<ul>
<li>Sutton &amp; Barto<a href="#fn291" class="footnote-ref" id="fnref291" role="doc-noteref"><sup>291</sup></a></li>
<li>Deep Reinforcement Learning: A Brief Survey<a href="#fn292" class="footnote-ref" id="fnref292" role="doc-noteref"><sup>292</sup></a></li>
<li>Cesa-Bianchi, N. &amp; Lugosi, G. (2006). <a href="https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf"><em>Prediction, Learning, and Games</em></a>.<a href="#fn293" class="footnote-ref" id="fnref293" role="doc-noteref"><sup>293</sup></a></li>
</ul>
<p>Tutorials:</p>
<ul>
<li><a href="https://www.youtube.com/watch?v=2pWv7GOvuf0&amp;list=PLqYmG7hTraZBiG_XpjnPrSNw-1XQaM_gB&amp;ab_channel=DeepMind">RL course by David Silver</a></li>
<li><a href="https://www.youtube.com/watch?v=FgzM3zpZ55o&amp;list=PLoROMvodv4rOSOPzutgyCTapiGlY2Nd8u&amp;ab_channel=stanfordonline">RL course by Emma Brunskill</a></li>
<li><a href="https://deepmind.com/learning-resources/reinforcement-learning-series-2021">DeepMind Reinforcement Learning Lecture Series 2021</a></li>
</ul>
<p>More:</p>
<ul>
<li><a href="https://spinningup.openai.com/en/latest/spinningup/keypapers.html">List by OpenAI of key RL papers</a></li>
<li><a href="https://github.com/datamllab/awesome-game-ai">List of game AI codes by DATA Lab</a></li>
<li>Chen, L. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2106.01345">Decision Transformer: Reinforcement learning via sequence modeling</a>.</li>
</ul>
<h4 id="q-learning">Q-learning</h4>
<ul>
<li>Q-learning and DQN</li>
<li>Uses the Markov Decision Process (MDP) framework</li>
<li>The Bellman equation<a href="#fn294" class="footnote-ref" id="fnref294" role="doc-noteref"><sup>294</sup></a></li>
<li>Q-learning is a values-based learning algorithm. Value based algorithms updates the value function based on an equation (particularly Bellman equation). Whereas the other type, policy-based estimates the value function with a greedy policy obtained from the last policy improvement (<a href="https://towardsdatascience.com/a-beginners-guide-to-q-learning-c3e2a30a653c">source: towardsdatascience.com</a>).</li>
<li>DQN masters Atari<a href="#fn295" class="footnote-ref" id="fnref295" role="doc-noteref"><sup>295</sup></a></li>
</ul>
<h4 id="alphazero">AlphaZero</h4>
<ul>
<li>AlphaGo Lee<a href="#fn296" class="footnote-ref" id="fnref296" role="doc-noteref"><sup>296</sup></a> &#x2192; AlphaGo Zero<a href="#fn297" class="footnote-ref" id="fnref297" role="doc-noteref"><sup>297</sup></a> &#x2192; AlphaZero<a href="#fn298" class="footnote-ref" id="fnref298" role="doc-noteref"><sup>298</sup></a></li>
<li><a href="https://openai.com/blog/openai-five/">OpenAI Five masters Dota2</a></li>
<li><a href="https://deepmind.com/blog/article/alphastar-mastering-real-time-strategy-game-starcraft-ii">AlphaStar masters StarCraftII</a></li>
<li>AlphaZero
<ul>
<li><span class="math inline">\(\pi(a|s)\)</span> and <span class="math inline">\(V(s)\)</span></li>
<li>Monte Carlo Tree Search (MCTS)</li>
</ul></li>
</ul>
<h4 id="regret-minimization">Regret minimization</h4>
<p><strong>Regret matching (RM)</strong></p>
<ul>
<li>Hart, S. &amp; Mas&#x2010;Colell, A. (2000). <a href="https://www.ma.imperial.ac.uk/~dturaev/Hart0.pdf">A simple adaptive procedure leading to correlated equilibrium</a>.<a href="#fn299" class="footnote-ref" id="fnref299" role="doc-noteref"><sup>299</sup></a></li>
</ul>
<p>Consider a game like rock-paper-scissors, where there is only one action per round. Let <span class="math inline">\(v^{t}(a)\)</span> be the value observed when playing action <span class="math inline">\(a\)</span> on iteration <span class="math inline">\(t\)</span>.</p>
<p>TODO: explain that the entire rewards vector, <span class="math inline">\(v^{t}(a)\)</span>, over <span class="math inline">\(a\)</span> is observable after the chosen action is played.</p>
<p>Let a strategy, <span class="math inline">\(\sigma^t\)</span>, be a probability distribution over actions, <span class="math inline">\(a \in A\)</span>. Then the value of a strategy, <span class="math inline">\(v^{t}(\sigma^{t})\)</span>, is the expectation of its value over actions.</p>
<p><span class="math display">\[ v^{t}(\sigma^{t}) = \sum_{a \in A} \sigma^{t}(a) \: v^{t}(a) \label{eq:value_of_strategy} \]</span></p>
<p>Regret, <span class="math inline">\(R^{T}\)</span>, measures how much better some sequence of strategies, <span class="math inline">\(\sigma&#39;\)</span>, would do compared to the chosen sequence of strategies, <span class="math inline">\(\sigma = \{\sigma^1, \sigma^2, \ldots \sigma^T\}\)</span>.</p>
<p><span class="math display">\[ R^{T} \equiv \sum_{t=1}^{T} \left( v^{t}({\sigma&#39;}^{t}) - v^{t}(\sigma^{t}) \right) \label{eq:regret} \]</span></p>
<p>External regret, <span class="math inline">\(R^{T}(a)\)</span>, measures the regret of the chosen sequence of strategies versus a hypothetical stategy where action <span class="math inline">\(a\)</span> is always chosen.</p>
<p><span class="math display">\[ R^{T}(a) \equiv \sum_{t=1}^{T} \left( v^{t}(a) - v^{t}(\sigma^{t}) \right) \label{eq:external_regret} \]</span></p>
<p>Regret Matching (RM) is a rule to determine the strategy for the next iteration:</p>
<p><span class="math display">\[ \sigma^{t+1}(a) \equiv \frac{ R^{t}_{+}(a) }{ \sum_{b \in A} R^{t}_{+}(b) } \label{eq:regret_matching} \]</span></p>
<p>where <span class="math inline">\(R_{+} \equiv \mathrm{max}(R, 0)\)</span>.</p>
<p>At the end of training, the resulting recommended strategy with convergence bounds is <em>not</em> the final strategy used in training, <span class="math inline">\(\sigma^{T}\)</span>, but the average strategy over all time steps:</p>
<p><span class="math display">\[ \bar{\sigma}^{T}(a) = \frac{1}{T} \sum_{t=1}^{T} \sigma^{t}(a) \]</span></p>
<p>TODO: explain the convergence of <span class="math inline">\(\bar{\sigma}^{t}\)</span> to an <span class="math inline">\(\varepsilon\)</span>-Nash equilibrium.</p>
<ul>
<li>Roughgarden, T. (2013). Video: <a href="https://www.youtube.com/watch?v=ssAEgJKRe9o">Twenty Lectures on Algorithmic Game Theory</a>.</li>
<li>Roughgarden, T. (2016). <em>Twenty Lectures on Algorithmic Game Theory</em>.<a href="#fn300" class="footnote-ref" id="fnref300" role="doc-noteref"><sup>300</sup></a>
<ul>
<li>TODO: Coarse correlated equilibria</li>
</ul></li>
</ul>
<p><strong>Counterfactual regret minimization (CFR)</strong></p>
<ul>
<li>CFR
<ul>
<li>Zinkevich, M., Johanson, M., Bowling, M., &amp; Piccione, C. (2007). <a href="https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf">Regret minimization in games with incomplete information</a>.<a href="#fn301" class="footnote-ref" id="fnref301" role="doc-noteref"><sup>301</sup></a></li>
<li>Counterfactual regret minimization (CFR) is an algorithm for extensive-form games that independently minimizes regret in each information set.<a href="#fn302" class="footnote-ref" id="fnref302" role="doc-noteref"><sup>302</sup></a></li>
<li>&#x201C;In other words, actions are selected in proportion to the amount of positive counterfactual regret for not playing that action.&#x201D;<a href="#fn303" class="footnote-ref" id="fnref303" role="doc-noteref"><sup>303</sup></a></li>
<li>CFR differs from traditional RL algorithms in that it does not try to maximize expected return. Instead, it minimizes exploitability. CFR does not use the MDP framework; instead, it uses extensive-form games (<a href="https://www.quora.com/What-are-the-connection-and-difference-between-reinforcement-learning-and-Counterfactual-Regret-Minimization">source: Quora</a>).</li>
<li><a href="https://www.quora.com/What-is-an-intuitive-explanation-of-counterfactual-regret-minimization">Johanson&#x2019;s explanation on Quora</a>.</li>
</ul></li>
<li>CFR+
<ul>
<li>Tammelin, O. (2014). <a href="https://arxiv.org/abs/1407.5042">Solving large imperfect information games using CFR+</a>.<a href="#fn304" class="footnote-ref" id="fnref304" role="doc-noteref"><sup>304</sup></a></li>
<li>Tammelin, O., Burch, N., Johanson, M., &amp; Bowling, M. (2015). <a href="http://johanson.ca/publications/poker/2015-ijcai-cfrplus/2015-ijcai-cfrplus.pdf">Solving heads-up limit texas hold&#x2019;em</a><a href="#fn305" class="footnote-ref" id="fnref305" role="doc-noteref"><sup>305</sup></a></li>
<li>Burch, N., Moravcik, M., &amp; Schmid, M. (2019). <a href="https://www.jair.org/index.php/jair/article/view/11370">Revisiting CFR+ and alternating updates</a>.<a href="#fn306" class="footnote-ref" id="fnref306" role="doc-noteref"><sup>306</sup></a></li>
<li>Brown, N. &amp; Sandholm, T. (2019). <a href="https://arxiv.org/abs/1809.04040">Solving imperfect-information games via discounted regret minimization</a>.<a href="#fn307" class="footnote-ref" id="fnref307" role="doc-noteref"><sup>307</sup></a>
<ul>
<li>LCFR+ is worse than CFR+ or LCFR.</li>
</ul></li>
</ul></li>
<li>Examples
<ul>
<li>Czarnog&#xF3;rski, K. (2018). <a href="https://int8.io/counterfactual-regret-minimization-for-poker-ai/">Counterfactual Regret Minimization: The core of poker AI beating professional players</a>.</li>
<li><a href="https://github.com/tt293/medium-poker-ai/blob/master/part_7/exploitability_two_player_kuhn_poker.py" class="uri">https://github.com/tt293/medium-poker-ai/blob/master/part_7/exploitability_two_player_kuhn_poker.py</a></li>
<li><a href="https://github.com/int8/counterfactual-regret-minimization" class="uri">https://github.com/int8/counterfactual-regret-minimization</a></li>
</ul></li>
</ul>
<p>TODO: explain extensive-form games.</p>
<p>A finite <em>extensive game</em> with imperfect information has the following components:<a href="#fn308" class="footnote-ref" id="fnref308" role="doc-noteref"><sup>308</sup></a></p>
<ul>
<li>A finite set <span class="math inline">\(N\)</span> of players. A finite set <span class="math inline">\(H\)</span> of sequences, the possible histories of actions, such that the empty sequence is in <span class="math inline">\(H\)</span> and every prefix of a sequence in <span class="math inline">\(H\)</span> is also in <span class="math inline">\(H\)</span>. Define <span class="math inline">\(h \sqsubseteq h&#39;\)</span> to mean <span class="math inline">\(h\)</span> is a prefix of <span class="math inline">\(h&#39;\)</span>. <span class="math inline">\(Z \subseteq H\)</span> are the terminal histories (those which are not a prefix of any other sequences). <span class="math inline">\(A(h) = \{a : ha \in H\}\)</span> are the actions available after a non-terminal history, <span class="math inline">\(h \in H \backslash Z\)</span>.</li>
<li>A function <span class="math inline">\(P\)</span> that assigns to each non-terminal history a member of <span class="math inline">\(N \cup \{c\}\)</span>. <span class="math inline">\(P\)</span> is the player function. <span class="math inline">\(P(h)\)</span> is the player who takes an action after the history <span class="math inline">\(h\)</span>. If <span class="math inline">\(P(h) = c\)</span> then chance determines the action taken after history <span class="math inline">\(h\)</span>.</li>
<li>For each player <span class="math inline">\(i \in N \cup \{c\}\)</span> a partition <span class="math inline">\(\mathcal{I}_i\)</span> of <span class="math inline">\(\{h \in H : P (h) = i\}\)</span> with the property that <span class="math inline">\(A(h) = A(h&#39;)\)</span> whenever <span class="math inline">\(h\)</span> and <span class="math inline">\(h&#39;\)</span> are in the same member of the partition. For <span class="math inline">\(I \in \mathcal{I}_i\)</span> we denote by <span class="math inline">\(A(I_i)\)</span> the set <span class="math inline">\(A(h)\)</span> and by <span class="math inline">\(P(I_i)\)</span> the player <span class="math inline">\(P(h)\)</span> for any <span class="math inline">\(h \in I_i\)</span> . <span class="math inline">\(\mathcal{I}_i\)</span> is the information partition of player <span class="math inline">\(i\)</span>; a set <span class="math inline">\(I_i \in \mathcal{I}_i\)</span> is an information set of player <span class="math inline">\(i\)</span>.</li>
<li>A function <span class="math inline">\(f_c\)</span> that associates with every information set <span class="math inline">\(I\)</span> where <span class="math inline">\(P(I) = c\)</span>, a probability measure <span class="math inline">\(f_c(a|I)\)</span> on <span class="math inline">\(A(h)\)</span>; <span class="math inline">\(f_c(a|I)\)</span> is the probability that <span class="math inline">\(a\)</span> occurs given some <span class="math inline">\(h \in I\)</span>, where each such probability measure is independent of every other such measure.</li>
<li>For each player <span class="math inline">\(i \in N\)</span> there is a utility function <span class="math inline">\(u_i\)</span> from the terminal states <span class="math inline">\(Z\)</span> to the reals <span class="math inline">\(\mathbb{R}\)</span>. If <span class="math inline">\(N = \{1, 2\}\)</span> and <span class="math inline">\(u_1 = -u_2\)</span>, it is a zero-sum extensive game. Define <span class="math inline">\(\Delta_{u,i} \equiv \mathrm{max}_z \: u_i(z) - \mathrm{min}_z \: u_i(z)\)</span> to be the range of utilities to player <span class="math inline">\(i\)</span>.</li>
</ul>
<p>The <em>player reach</em>, <span class="math inline">\(\pi^{\sigma}_{i}(h)\)</span>, of a history <span class="math inline">\(h\)</span> is the product of the probabilities for all agent <span class="math inline">\(i\)</span> actions leading to <span class="math inline">\(h\)</span>. Formally,<a href="#fn309" class="footnote-ref" id="fnref309" role="doc-noteref"><sup>309</sup></a></p>
<p><span class="math display">\[ \pi^{\sigma}_{i}(h) \equiv \prod_{h&#39; \cdot a&#39; \sqsubseteq h | P(h&#39;) = i} \sigma_{i}(h&#39;, a&#39;) \label{eq:player_reach} \]</span></p>
<p>Due to perfect recall, any two histories in infoset <span class="math inline">\(I_i\)</span> have the same player reach for player <span class="math inline">\(i\)</span>. Thus, we similarly define the player reach <span class="math inline">\(\pi^{\sigma}_{i}(I_i)\)</span> of infoset <span class="math inline">\(I_i\)</span> as</p>
<p><span class="math display">\[ \pi^{\sigma}_{i}(I_i) \equiv \prod_{ {I&#39;}_{i} \cdot a&#39; \sqsubseteq I_i | P(I_i) = i } \sigma_{i}({I&#39;}_{i}, a&#39;) = \left.\pi^{\sigma}_{i}(h)\right|_{h \in I_i} \label{eq:player_reach_from_infoset} \]</span></p>
<p>The <em>external reach</em> AKA <em>opponent reach</em>, <span class="math inline">\(\pi^{\sigma}_{-i}(h)\)</span>, of a history <span class="math inline">\(h\)</span> is the contribution of chance and all other players than <span class="math inline">\(i\)</span>. Formally,</p>
<p><span class="math display">\[ \pi^{\sigma}_{-i}(h) \equiv \prod_{h&#39; \cdot a&#39; \sqsubseteq h | P(h&#39;) \neq i} \sigma_{i}(h&#39;, a&#39;) \label{eq:external_reach} \]</span></p>
<p>We also define the external reach of an infoset as</p>
<p><span class="math display">\[ \pi^{\sigma}_{-i}(I_i) \equiv \sum_{h \in I_{i}} \pi^{\sigma}_{-i}(h) \label{eq:external_reach_from_infoset} \]</span></p>
<p>The <em>counterfactual value</em> of an infoset <span class="math inline">\(I\)</span> is the expected utility to player <span class="math inline">\(i\)</span> given that <span class="math inline">\(I\)</span> has been reached, weighted by the external reach of <span class="math inline">\(I\)</span> for player <span class="math inline">\(i\)</span>. Formally,<a href="#fn310" class="footnote-ref" id="fnref310" role="doc-noteref"><sup>310</sup></a></p>
<p><span class="math display">\[ v(I) = \sum_{h \in I} \pi^{\sigma}_{-i}(h) \sum_{z \in Z} \pi^{\sigma}(h, z) \: u_{i}(z) \label{eq:counter_factual_value} \]</span></p>
<p>The counterfactual value of an action, <span class="math inline">\(a\)</span>, is</p>
<p><span class="math display">\[ v(I, a) = \sum_{h \in I} \pi^{\sigma}_{-i}(h) \sum_{z \in Z} \pi^{\sigma}(h \cdot a, z) \: u_{i}(z) \label{eq:counter_factual_value_of_a} \]</span></p>
<p>Let&#x2019;s consider the case where, like in NLHE, our two private hole cards each make a single unique history <span class="math inline">\(h\)</span>, and we form infosets with a single hand, so <span class="math inline">\(I=h\)</span>. Then</p>
<p><span class="math display">\[ v(h) = \pi^{\sigma}_{-i}(h) \sum_{z \in Z} \pi^{\sigma}(h, z) \: u_{i}(z) \]</span></p>
<p>making explicit the player reach and the external reach,</p>
<p><span class="math display">\[ v(h) = \pi^{\sigma}_{-i}(h) \sum_{z \in Z} \pi_{i}^{\sigma}(h, z) \: \pi_{-i}^{\sigma}(h, z) \: u_{i}(z) \]</span></p>
<p>At a leaf node where we finally calculate the rewards,</p>
<p><span class="math display">\[ v(z) = \pi^{\sigma}_{-i}(z) \: u_{i}(z) \]</span></p>
<p>TODO: explain CFR.</p>
<p>The instantaneous regret is</p>
<p><span class="math display">\[ r^{t}(I, a) = v^{\sigma^t}(I, a) - v^{\sigma^t}(I) \]</span></p>
<p>The (cummulative) counterfactual regret</p>
<p><span class="math display">\[ R^{t}(I, a) = \sum_{t=1}^{T} r^{t}(I, a) \]</span></p>
<p>Similar to the single-node game discussed above, eq.&#xA0;<span class="math inline">\(\eqref{eq:regret_matching}\)</span>, applying <em>regret matching</em> during training means to update strategies according to the following rule.</p>
<p><span class="math display">\[ \sigma^{t+1}(I, a) \equiv \frac{ R^{t}_{+}(I, a) }{ \sum_{b \in A} R^{t}_{+}(I, b) } \label{eq:regret_matching_cfr} \]</span></p>
<p>The average strategy is</p>
<p><span class="math display">\[ \bar{\sigma}^{T}(I, a) = \sum_{t=1}^{T} \frac{\pi^{t}_{i}(I) \: \sigma^{t}(I, a) }{\pi^{t}_{i}(I)} \]</span></p>
<p><strong>Monte Carlo Counterfactual Regret Minimization (MCCFR)</strong></p>
<ul>
<li>Lanctot, M. (2009). <a href="https://proceedings.neurips.cc/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf">Monte Carlo sampling for regret minimization</a>.<a href="#fn311" class="footnote-ref" id="fnref311" role="doc-noteref"><sup>311</sup></a></li>
<li>Neller, T.W. &amp; Lanctot, M. (2013). <a href="http://modelai.gettysburg.edu/2013/cfr/cfr.pdf">An introduction to counterfactual regret minimization</a>.<a href="#fn312" class="footnote-ref" id="fnref312" role="doc-noteref"><sup>312</sup></a></li>
<li>Vectorized and sampling variants
<ul>
<li>Burch, N., Lanctot, M., Szafron, D., &amp; Gibson, R. (2012). <a href="https://proceedings.neurips.cc/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf">Efficient Monte Carlo counterfactual regret minimization in games with many player actions</a>.<a href="#fn313" class="footnote-ref" id="fnref313" role="doc-noteref"><sup>313</sup></a></li>
<li>Johanson, M., Bard, N., Lanctot, M., Gibson, R.G., &amp; Bowling, M. (2012). <a href="https://www.idi.ntnu.no/emner/it3105/materials/poker/monte-carlo-cfm-2012.pdf">Efficient Nash equilibrium approximation through Monte Carlo counterfactual regret minimization</a>.<a href="#fn314" class="footnote-ref" id="fnref314" role="doc-noteref"><sup>314</sup></a>
<ul>
<li>Variants of chance sampling with single or vector opponent actions.</li>
</ul></li>
<li>Schmid, M. et al.&#xA0;(2019). <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4048/3926">Variance reduction in Monte Carlo counterfactual regret minimization (VR-MCCFR) for extensive form games using baselines</a>.<a href="#fn315" class="footnote-ref" id="fnref315" role="doc-noteref"><sup>315</sup></a></li>
<li>Li, H. et al.&#xA0;(2020). <a href="http://aaai-rlg.mlanctot.info/2020/papers/AAAI20-RLG_paper_14.pdf">Regret minimization via novel vectorized sampling policies and exploration</a>.<a href="#fn316" class="footnote-ref" id="fnref316" role="doc-noteref"><sup>316</sup></a></li>
<li>Habara, K., Fukuda, E.H., &amp; Yamashita, N. (2023). <a href="https://arxiv.org/abs/2303.11046">Convergence analysis and acceleration of the smoothing methods for solving extensive-form games</a>.<a href="#fn317" class="footnote-ref" id="fnref317" role="doc-noteref"><sup>317</sup></a></li>
</ul></li>
<li>MCCFR Ph.D.&#xA0;theses
<ul>
<li>Lanctot, M. (2013). <a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf"><em>Monte Carlo Sample and Regret Minimization for Equilibrium Computation and Decision-Making in Large Extensive Form Games</em></a>.<a href="#fn318" class="footnote-ref" id="fnref318" role="doc-noteref"><sup>318</sup></a></li>
<li>Gibson, R. (2014). <a href="https://era.library.ualberta.ca/items/15d28cbf-49d4-42e5-a9c9-fc55b1d816af/view/5ee708c7-6b8b-4b96-b1f5-23cdd95b6a46/Gibson_Richard_Spring-202014.pdf"><em>Regret minimization in games and the development of champion multiplayer computer poker-playing agents</em></a>.<a href="#fn319" class="footnote-ref" id="fnref319" role="doc-noteref"><sup>319</sup></a></li>
<li>Johanson, M. (2016). <a href="http://johanson.ca/publications/theses/2016-johanson-phd-thesis/2016-johanson-phd-thesis.pdf"><em>Robust Strategies and Counter-Strategies: From Superhuman to Optimal Play</em></a>.<a href="#fn320" class="footnote-ref" id="fnref320" role="doc-noteref"><sup>320</sup></a></li>
<li>Burch, N. (2018). <a href="https://era.library.ualberta.ca/items/db44409f-b373-427d-be83-cace67d33c41/view/bcb00dca-39e6-4c43-9ec2-65026a50135e/Burch_Neil_E_201712_PhD.pdf"><em>Time and Space: Why imperfect information games are hard</em></a>.<a href="#fn321" class="footnote-ref" id="fnref321" role="doc-noteref"><sup>321</sup></a></li>
<li>Horacek, M. (2022). <a href="https://is.muni.cz/th/ydbvx/thesis.pdf"><em>Risk-Aversion in Algorithms for Poker</em></a>.</li>
</ul></li>
</ul>
<p>TODO: explain MCCFR.</p>
<p>External sampling MCCFR:</p>
<p><span class="math display">\[ \tilde{v}^{\sigma}_{i}(I) = \sum_{z \in Q} u_{i}(z) \: \pi^{\sigma}_{i}(z[I] \rightarrow z) \label{eq:external_sample_mccfr} \]</span></p>
<p><strong>Best response and exploitability</strong></p>
<p>Best response:</p>
<p><span class="math display">\[ \mathrm{BR}(\sigma_{-i}) = \underset{\sigma_{i}^{\prime}}{\mathrm{argmax}} \: u_{i}(\sigma_{i}^{\prime}, \sigma_{-i}) \label{eq:best_response} \]</span></p>
<p>TODO: Local Best Response (LBR).<a href="#fn322" class="footnote-ref" id="fnref322" role="doc-noteref"><sup>322</sup></a></p>
<p>Exploitability:</p>
<p><span class="math display">\[ \varepsilon_{i}(\sigma) = u_{i}(\mathrm{BR}(\sigma_{-i}), \sigma_{-i}) - u_{i}(\sigma_{i}, \mathrm{BR}(\sigma_{i})) \label{eq:exploitability} \]</span></p>
<p>NashConv<a href="#fn323" class="footnote-ref" id="fnref323" role="doc-noteref"><sup>323</sup></a> exploitability uses the convention:</p>
<p><span class="math display">\[ \varepsilon_{i}(\sigma) = u_{i}(\mathrm{BR}(\sigma_{-i}), \sigma_{-i}) - u_{i}(\sigma_{i}, \sigma_{-i}) \label{eq:nc_exploitability} \]</span></p>
<p>The average exploitability per player is</p>
<p><span class="math display">\[ \varepsilon(\sigma) = \frac{1}{n} \sum_{i}^{n} \varepsilon_{i}(\sigma) \]</span></p>
<p>Note that in zero-sum games, when summing over players, the second terms in NashConv sum to zero.<a href="#fn324" class="footnote-ref" id="fnref324" role="doc-noteref"><sup>324</sup></a></p>
<p><span class="math display">\[ \varepsilon(\sigma) = \frac{1}{n} \sum_{i}^{n} u_{i}(\mathrm{BR}(\sigma_{-i}), \sigma_{-i})  \label{eq:average_exploitability} \]</span></p>
<p>In two-player games:</p>
<p><span class="math display">\[ \varepsilon(\sigma) = \frac{1}{2} \Big( u_{1}(\mathrm{BR}(\sigma_{2}), \sigma_{2}) + u_{2}(\sigma_{1}, \mathrm{BR}(\sigma_{1})) \Big)  \label{eq:average_exploitability_two_player} \]</span></p>
<ul>
<li>Johanson, M., Waugh, K., Bowling, M., &amp; Zinkevich, M. (2011). <a href="http://www.cs.cmu.edu/~kwaugh/publications/johanson11.pdf">Accelerating best response calculation in large extensive games</a>.<a href="#fn325" class="footnote-ref" id="fnref325" role="doc-noteref"><sup>325</sup></a>
<ul>
<li>Evaluates range vs range rewards in <span class="math inline">\(O(n \log n)\)</span> + <span class="math inline">\(O(n)\)</span> instead of <span class="math inline">\(O(n^2)\)</span>.</li>
</ul></li>
<li>Ponsen, M., De Jong, S., &amp; Lanctot, M. (2011). <a href="https://arxiv.org/abs/1401.4591">Computing approximate Nash equilibria and robust best-responses using sampling</a>.<a href="#fn326" class="footnote-ref" id="fnref326" role="doc-noteref"><sup>326</sup></a></li>
<li>Lisy, V. &amp; Bowling, M. (2016). <a href="https://arxiv.org/abs/1612.07547">Equilibrium approximation quality of current no-limit poker bots</a>.<a href="#fn327" class="footnote-ref" id="fnref327" role="doc-noteref"><sup>327</sup></a></li>
<li>Timbers, F. (2020). <a href="https://arxiv.org/abs/2004.09677">Approximate exploitability: Learning a best response in large games</a>.<a href="#fn328" class="footnote-ref" id="fnref328" role="doc-noteref"><sup>328</sup></a></li>
</ul>
<h4 id="solving-poker">Solving poker</h4>
<ul>
<li>Simplified toy pokers
<ul>
<li>Kuhn poker<a href="#fn329" class="footnote-ref" id="fnref329" role="doc-noteref"><sup>329</sup></a></li>
<li>Leduc poker<a href="#fn330" class="footnote-ref" id="fnref330" role="doc-noteref"><sup>330</sup></a></li>
</ul></li>
<li>Earlier poker work
<ul>
<li>Billings, D., Davidson, A., Schaeffer, J., &amp; Szafron, D. (2002). <a href="https://doi.org/10.1016/S0004-3702(01)00130-8">The challenge of poker</a>.<a href="#fn331" class="footnote-ref" id="fnref331" role="doc-noteref"><sup>331</sup></a></li>
<li>Billings, D. et al.&#xA0;(2003). <a href="http://webdocs.cs.ualberta.ca/~duane/publications/pdf/2003ijcai.pdf">Approximating game-theoretic optimal strategies for full-scale poker</a>.<a href="#fn332" class="footnote-ref" id="fnref332" role="doc-noteref"><sup>332</sup></a></li>
<li>Johanson, M. (2013). <a href="https://arxiv.org/abs/1302.7008">Measuring the size of large no-limit poker games</a>.<a href="#fn333" class="footnote-ref" id="fnref333" role="doc-noteref"><sup>333</sup></a></li>
<li>Bowling, M., Burch, N., Johanson, M., &amp; Tammelin, O. (2015). <a href="https://www.science.org/doi/10.1126/science.1259433">Heads-up limit hold&#x2019;em poker is solved</a>.<a href="#fn334" class="footnote-ref" id="fnref334" role="doc-noteref"><sup>334</sup></a>
<ul>
<li>CFR+</li>
</ul></li>
<li>Heinrich &amp; Silver. (2016). <a href="https://arxiv.org/abs/1603.01121">Deep reinforcement learning from self play in imperfect-information games</a>.<a href="#fn335" class="footnote-ref" id="fnref335" role="doc-noteref"><sup>335</sup></a>
<ul>
<li>Q-learning</li>
</ul></li>
<li>Moravcik, M. et al.&#xA0;(2017). <a href="https://arxiv.org/abs/1701.01724">DeepStack: Expert-level artificial intelligence in heads-up no-limit poker</a>.<a href="#fn336" class="footnote-ref" id="fnref336" role="doc-noteref"><sup>336</sup></a></li>
</ul></li>
<li>Libratus
<ul>
<li>Brown, N. &amp; Sandholm, T. (2018). <a href="https://www.science.org/doi/10.1126/science.aao1733">Superhuman AI for heads-up no-limit poker: Libratus beats top professionals</a>.<a href="#fn337" class="footnote-ref" id="fnref337" role="doc-noteref"><sup>337</sup></a>
<ul>
<li>bet and card abstraction</li>
<li>MCCFR used to find a solution of the abstracted game: blueprint</li>
</ul></li>
<li>Brown, N. &amp; Sandholm, T. (2019). <a href="https://arxiv.org/abs/1809.04040">Solving imperfect-information games via discounted regret minimization</a>.<a href="#fn338" class="footnote-ref" id="fnref338" role="doc-noteref"><sup>338</sup></a></li>
<li>Brown, N., Lerer, A., Gross, S., &amp; Sandholm, T. (2019). <a href="https://arxiv.org/abs/1811.00164">Deep counterfactual regret minimization</a>.<a href="#fn339" class="footnote-ref" id="fnref339" role="doc-noteref"><sup>339</sup></a></li>
</ul></li>
<li>Pluribus
<ul>
<li>Brown, N. &amp; Sandholm, T. (2019). <a href="https://www.science.org/doi/10.1126/science.aay2400">Superhuman AI for multiplayer poker</a>.<a href="#fn340" class="footnote-ref" id="fnref340" role="doc-noteref"><sup>340</sup></a></li>
<li>Brown, N. (2019). <a href="https://ai.facebook.com/blog/pluribus-first-ai-to-beat-pros-in-6-player-poker/">Facebook, Carnegie Mellon build first AI that beats pros in 6-player poker</a>.</li>
<li><a href="https://www.nature.com/articles/d41586-019-02156-9">No limit: AI poker bot is first to beat professionals at multiplayer game</a></li>
</ul></li>
<li>ReBeL
<ul>
<li>Brown, N. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2007.13544">Combining deep reinforcement learning and search</a>.<a href="#fn341" class="footnote-ref" id="fnref341" role="doc-noteref"><sup>341</sup></a></li>
<li><a href="https://ai.facebook.com/blog/rebel-a-general-game-playing-ai-bot-that-excels-at-poker-and-more/">ReBeL: A general game-playing AI bot that excels at poker and more</a></li>
<li>YouTube by Brown: <a href="https://www.youtube.com/watch?v=mCldyXOYNok">Combining deep reinforcement learning and search for imperfect-information games</a>.</li>
<li>Brown, N. (2020). <a href="http://www.cs.cmu.edu/~noamb/thesis.pdf"><em>Equilibrium finding for large adversarial imperfect-information games</em></a>.<a href="#fn342" class="footnote-ref" id="fnref342" role="doc-noteref"><sup>342</sup></a></li>
</ul></li>
<li>Player of Games
<ul>
<li>Schmid, M. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2112.03178">Player of games</a>.<a href="#fn343" class="footnote-ref" id="fnref343" role="doc-noteref"><sup>343</sup></a></li>
</ul></li>
<li>More
<ul>
<li>Kovarik, V. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/1906.11110">Rethinking formal models of partially observable multiagent decision making</a>.<a href="#fn344" class="footnote-ref" id="fnref344" role="doc-noteref"><sup>344</sup></a></li>
</ul></li>
</ul>
<h3 id="applications-in-physics">Applications in physics</h3>
<ul>
<li><a href="https://iml-wg.github.io/HEPML-LivingReview/">HEPML-LivingReview: A Living Review of Machine Learning for Particle Physics</a></li>
<li>Spears, B.K. et al.&#xA0;(2018). Deep learning: A guide for practitioners in the physical sciences.<a href="#fn345" class="footnote-ref" id="fnref345" role="doc-noteref"><sup>345</sup></a></li>
<li>Cranmer, K., Seljak, U., &amp; Terao, K. (2021). Machine learning (Review in the <a href="https://pdg.lbl.gov/2021-rev/2021/reviews/contents_sports.html">PDG</a>).<a href="#fn346" class="footnote-ref" id="fnref346" role="doc-noteref"><sup>346</sup></a></li>
</ul>
<p>See also:</p>
<ul>
<li><a href="statistics.html#surrogate-models">Surrogate models</a></li>
</ul>
<h2 id="theoretical-machine-learning">Theoretical machine learning</h2>
<h3 id="algorithmic-information-theory">Algorithmic information theory</h3>
<ul>
<li>Ray Solomonoff (1926-2009)</li>
<li>Solomonoff induction
<ul>
<li>Naturally formalizes Occam&#x2019;s razor</li>
<li>Incomputable</li>
</ul></li>
<li>Cilibrasi, R. &amp; Vitanyi, P.M.B. (2005). Clustering by compression.<a href="#fn347" class="footnote-ref" id="fnref347" role="doc-noteref"><sup>347</sup></a></li>
<li>Hutter, M. (2007). <a href="http://www.hutter1.net/ai/aixigentle.htm">Universal Algorithmic Intelligence: A mathematical top-down approach</a>.<a href="#fn348" class="footnote-ref" id="fnref348" role="doc-noteref"><sup>348</sup></a></li>
<li>Rathmanner, S. &amp; Hutter, M. (2011). <a href="https://www.mdpi.com/1099-4300/13/6/1076">A philosophical treatise of universal induction</a>.<a href="#fn349" class="footnote-ref" id="fnref349" role="doc-noteref"><sup>349</sup></a></li>
<li>Hutter, M. (2022). Talk: <a href="https://www.youtube.com/watch?v=8Q2G2OGHm1c">Introduction to Algorithmic Information Theory and University Learning</a>.</li>
</ul>
<h3 id="no-free-lunch-theorems">No free lunch theorems</h3>
<ul>
<li>David Wolpert and William G. Macready
<ul>
<li>No free lunch theorems for search (1995)<a href="#fn350" class="footnote-ref" id="fnref350" role="doc-noteref"><sup>350</sup></a></li>
<li>The lack of a priori distinctions between learning algorithms (1996)<a href="#fn351" class="footnote-ref" id="fnref351" role="doc-noteref"><sup>351</sup></a></li>
<li>No free lunch theorems for optimization (1997)<a href="#fn352" class="footnote-ref" id="fnref352" role="doc-noteref"><sup>352</sup></a></li>
<li>Shalev-Shwarz, S. &amp; Ben-David, S. (2014).<a href="#fn353" class="footnote-ref" id="fnref353" role="doc-noteref"><sup>353</sup></a></li>
<li>McDermott, J. (2019). <a href="https://arxiv.org/abs/1906.03280">When and why metaheuristics researchers can ignore &#x201C;no free lunch&#x201D; theorems</a>.<a href="#fn354" class="footnote-ref" id="fnref354" role="doc-noteref"><sup>354</sup></a></li>
<li>Wolpert, D.H. (2007). <a href="https://arxiv.org/abs/0708.1362">Physical limits of inference</a>.<a href="#fn355" class="footnote-ref" id="fnref355" role="doc-noteref"><sup>355</sup></a></li>
<li>Wolpert, D.H. &amp; Kinney, D. (2020). <a href="https://arxiv.org/abs/2012.08298">Noisy deductive reasoning: How humans construct math, and how math constructs universes</a>.<a href="#fn356" class="footnote-ref" id="fnref356" role="doc-noteref"><sup>356</sup></a></li>
</ul></li>
<li>Blogs:
<ul>
<li>Fedden, L. (2017). <a href="https://medium.com/@LeonFedden/the-no-free-lunch-theorem-62ae2c3ed10c">The no free lunch theorem</a>.</li>
<li>Lokesh, M. (2020). <a href="https://towardsdatascience.com/intuitions-behind-no-free-lunch-theorem-1d160f754513">The intuition behind the no free lunch theorem</a>.</li>
<li>Mueller, A. (2019). <a href="https://peekaboo-vision.blogspot.com/2019/07/dont-cite-no-free-lunch-theorem.html">Don&#x2019;t cite the no free lunch theorem</a>.</li>
<li><a href="https://www.quora.com/If-Wolperts-No-free-lunch-theorem-is-true-how-come-most-Kaggle-competitions-winners-use-either-XGBoost-for-structured-data-or-NN-for-unstructured/answer/Luis-Argerich">Quora answer by Luis Argerich</a></li>
</ul></li>
<li>Inductive bias
<ul>
<li>Yudkowsky, E. (2007). <a href="https://www.lesswrong.com/posts/H59YqogX94z5jb8xx/inductive-bias">Inductive bias</a>. <em>LessWrong</em>.</li>
<li><a href="https://en.wikipedia.org/wiki/Ugly_duckling_theorem">Ugly duckling theorem</a></li>
<li>Hamilton, L.D. (2014). <a href="http://www.lauradhamilton.com/inductive-biases-various-machine-learning-algorithms">The inductive biases of various machine learning algorithms</a>.</li>
<li>Mitchell, T.M. (1980). <a href="http://www.cs.cmu.edu/afs/cs/usr/mitchell/ftp/pubs/NeedForBias_1980.pdf">The need for biases in learning generalizations</a>.<a href="#fn357" class="footnote-ref" id="fnref357" role="doc-noteref"><sup>357</sup></a></li>
</ul></li>
<li>Gerhard Schurz
<ul>
<li>See also: <a href="scientific-method.html#meta-induction-as-a-solution-to-the-problem-of-induction">Meta-induction as a solution to the problem of induction</a></li>
</ul></li>
<li>Dan A. Roberts. (2021). <a href="https://arxiv.org/abs/2104.00008">Why is AI hard and physics simple?</a><a href="#fn358" class="footnote-ref" id="fnref358" role="doc-noteref"><sup>358</sup></a>
<ul>
<li>See also: <a href="math.html#unreasonable-effectiveness">Unreasonable effectiveness</a></li>
</ul></li>
<li>More
<ul>
<li>Goldreich, O. &amp; Ron, D. (1997). <a href="https://www.wisdom.weizmann.ac.il/~oded/p_ul.html">On universal learning algorithms</a>.<a href="#fn359" class="footnote-ref" id="fnref359" role="doc-noteref"><sup>359</sup></a></li>
<li>Joyce, T. &amp; Herrmann, J.M. (2017). A review of no free lunch theorems, and their implications for metaheuristic optimisation.<a href="#fn360" class="footnote-ref" id="fnref360" role="doc-noteref"><sup>360</sup></a></li>
<li>Lauc, D. (2020). Machine learning and the philosophical problems of induction.<a href="#fn361" class="footnote-ref" id="fnref361" role="doc-noteref"><sup>361</sup></a></li>
<li>Nakkiran, P. (2021). <a href="https://arxiv.org/abs/2111.05321">Turing-universal learners with optimal scaling laws</a>.<a href="#fn362" class="footnote-ref" id="fnref362" role="doc-noteref"><sup>362</sup></a></li>
<li>Bousquet, O., Hanneke, S., Moran, S., Van Handel, R., &amp; Yehudayoff, A. (2021). <a href="https://dl.acm.org/doi/pdf/10.1145/3406325.3451087">A theory of universal learning</a>.<a href="#fn363" class="footnote-ref" id="fnref363" role="doc-noteref"><sup>363</sup></a></li>
<li>Andrews, M. (2023). <a href="https://philsci-archive.pitt.edu/22690/1/ML_Atheoreticity.pdf">The devil in the data: Machine learning &amp; the theory-free ideal</a>.<a href="#fn364" class="footnote-ref" id="fnref364" role="doc-noteref"><sup>364</sup></a></li>
</ul></li>
</ul>
<p>Raissi et al.:</p>
<blockquote>
<p>encoding such structured information into a learning algorithm results in amplifying the information content of the data that the algorithm sees, enabling it to quickly steer itself towards the right solution and generalize well even when only a few training examples are available.<a href="#fn365" class="footnote-ref" id="fnref365" role="doc-noteref"><sup>365</sup></a></p>
</blockquote>
<p>Roberts:</p>
<blockquote>
<p>From an algorithmic complexity standpoint it is somewhat miraculous that we can compress our huge look-up table of experiment/outcome into such an efficient description. In many senses, this type of compression is precisely what we mean when we say that physics enables us to understand a given phenomenon.<a href="#fn366" class="footnote-ref" id="fnref366" role="doc-noteref"><sup>366</sup></a></p>
</blockquote>
<ul>
<li>TODO: Note Dennett&#x2019;s discussion of compression.<a href="#fn367" class="footnote-ref" id="fnref367" role="doc-noteref"><sup>367</sup></a></li>
</ul>
<h3 id="graphical-tensor-notation">Graphical tensor notation</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Penrose_graphical_notation">Penrose graphical notation</a></li>
<li>Predrag Cvitanovic</li>
<li><a href="https://www.math3ma.com/blog/matrices-as-tensor-network-diagrams">Matrices as Tensor Network Diagrams</a></li>
<li>Multi-layer perceptions</li>
</ul>
<h3 id="universal-approximation-theorem">Universal approximation theorem</h3>
<ul>
<li>Minsky, M. &amp; Papert, S. (1969). <em>Perceptrons: An Introduction to Computational Geometry</em>.<a href="#fn368" class="footnote-ref" id="fnref368" role="doc-noteref"><sup>368</sup></a></li>
<li>Hornik, K., Stinchcombe, M., &amp; White, H. (1989). <a href="https://cognitivemedium.com/magic_paper/assets/Hornik.pdf">Multilayer feedforward networks are universal approximators</a>.<a href="#fn369" class="footnote-ref" id="fnref369" role="doc-noteref"><sup>369</sup></a></li>
<li>Lu, Z., Pu, H., Wang, F., Hu, Z., &amp; Wang, L. (2017). <a href="https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf">The expressive power of neural networks: A view from the width</a>.<a href="#fn370" class="footnote-ref" id="fnref370" role="doc-noteref"><sup>370</sup></a></li>
<li>Lin, H. &amp; Jegelka, S. (2018). <a href="https://arxiv.org/abs/1806.10909">ResNet with one-neuron hidden layers is a universal approximator</a>.<a href="#fn371" class="footnote-ref" id="fnref371" role="doc-noteref"><sup>371</sup></a></li>
<li>Ismailov, V. (2020). <a href="https://arxiv.org/abs/2012.03016">A three layer neural network can represent any multivariate function</a>.<a href="#fn372" class="footnote-ref" id="fnref372" role="doc-noteref"><sup>372</sup></a></li>
<li>Multi-layer perceptions with two or more layers are universal approximators.<a href="#fn373" class="footnote-ref" id="fnref373" role="doc-noteref"><sup>373</sup></a></li>
<li>Seemed to slow the interest in deeper networks?</li>
</ul>
<h3 id="relationship-to-statistical-mechanics">Relationship to statistical mechanics</h3>
<ul>
<li>Logistic/softmax and Boltzman factors</li>
<li>Opper, M. &amp; Kinzel, W. (1996). <a href="https://gwern.net/doc/ai/nn/1996-opper.pdf">Statistical mechanics of generalization</a>.<a href="#fn374" class="footnote-ref" id="fnref374" role="doc-noteref"><sup>374</sup></a></li>
<li>Opper, M. (2001). <a href="https://t.co/3UYIwKvKeu">Learning to generalize</a>.<a href="#fn375" class="footnote-ref" id="fnref375" role="doc-noteref"><sup>375</sup></a></li>
<li>Wang, L. (2018). <a href="https://wangleiphy.github.io/lectures/PILtutorial.pdf">Generative models for physicists</a>.</li>
<li>Bahri<a href="#fn376" class="footnote-ref" id="fnref376" role="doc-noteref"><sup>376</sup></a></li>
<li>Halverson<a href="#fn377" class="footnote-ref" id="fnref377" role="doc-noteref"><sup>377</sup></a></li>
<li>Canatar, A., Bordelon, B., &amp; Pehlevan, C. (2020). <a href="https://arxiv.org/abs/2006.13198">Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks</a>.<a href="#fn378" class="footnote-ref" id="fnref378" role="doc-noteref"><sup>378</sup></a></li>
<li>Roberts, Yaida, &amp; Hanin. (2021). <a href="https://deeplearningtheory.com/PDLT.pdf"><em>The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks</em></a>.<a href="#fn379" class="footnote-ref" id="fnref379" role="doc-noteref"><sup>379</sup></a>
<ul>
<li>Introduced in <a href="https://ai.facebook.com/blog/advancing-ai-theory-with-a-first-principles-understanding-of-deep-neural-networks/">Facebook AI&#x2019;s blog</a></li>
</ul></li>
<li>Cantwell, G.T. (2022). <a href="https://arxiv.org/abs/2209.10423">Approximate sampling and estimation of partition functions using neural networks</a>.<a href="#fn380" class="footnote-ref" id="fnref380" role="doc-noteref"><sup>380</sup></a></li>
<li>Wang, L. (2022). <a href="https://drive.google.com/file/d/1D1bL--Zmv0J7n-QEqRvRb3erf5abV1o0/view">Generative AI for science</a>.</li>
<li>Dinan, E., Yaida, S., &amp; Zhang, S. (2023). <a href="https://arxiv.org/abs/2304.02034">Effective theory of transformers at initialization</a>.<a href="#fn381" class="footnote-ref" id="fnref381" role="doc-noteref"><sup>381</sup></a></li>
<li>Sohl-Dickstein, J. (2020). <a href="https://arxiv.org/abs/2005.06553">Two equalities expressing the determinant of a matrix in terms of expectations over matrix-vector products</a>.<a href="#fn382" class="footnote-ref" id="fnref382" role="doc-noteref"><sup>382</sup></a></li>
<li>Aifer, M. et al.&#xA0;(2023). <a href="https://arxiv.org/abs/2308.05660">Thermodynamic linear algebra</a>.<a href="#fn383" class="footnote-ref" id="fnref383" role="doc-noteref"><sup>383</sup></a></li>
<li>Geshkovski, B., Letrouit, C., Polyanskiy, Y., &amp; Rigollet, P. (2023). <a href="https://arxiv.org/abs/2312.10794">A mathematical perspective on Transformers</a><a href="#fn384" class="footnote-ref" id="fnref384" role="doc-noteref"><sup>384</sup></a></li>
</ul>
<h3 id="relationship-to-gauge-theory">Relationship to gauge theory</h3>
<ul>
<li>Cohen &amp; Welling. (2016). Group equivariant convolutional networks.<a href="#fn385" class="footnote-ref" id="fnref385" role="doc-noteref"><sup>385</sup></a></li>
<li>Gauge equivariant convolutional networks and the icosahedral CNN (2019)<a href="#fn386" class="footnote-ref" id="fnref386" role="doc-noteref"><sup>386</sup></a></li>
<li>Pavlus, J. (2020). <a href="https://www.quantamagazine.org/an-idea-from-physics-helps-ai-see-in-higher-dimensions-20200109/">An idea from physics helps AI see in higher dimensions</a>.</li>
<li>SE(3)-Transformers<a href="#fn387" class="footnote-ref" id="fnref387" role="doc-noteref"><sup>387</sup></a> and <a href="https://fabianfuchsml.github.io/alphafold2/">blog post</a>.</li>
<li>Bogatskiy, A., Hoffman, T., Miller, D.W., Offermann, J.T., &amp; Liu, X. (2023). <a href="https://arxiv.org/abs/2307.16506">Explainable equivariant neural networks for particle physics: PELICAN</a>.<a href="#fn388" class="footnote-ref" id="fnref388" role="doc-noteref"><sup>388</sup></a></li>
<li><a href="https://e3nn.org/">e3nn: a modular PyTorch framework for Euclidean neural networks</a></li>
<li><a href="https://github.com/Chen-Cai-OSU/awesome-equivariant-network">List of papers: Chen-Cai-OSU/awesome-equivariant-network</a></li>
<li>Marchetti, G.L., Hillar, C., Kragic, D., &amp; Sanborn. S. (2023). <a href="https://arxiv.org/abs/2312.08550">Harmonics of learning: Universal fourier features emerge in invariant networks</a>.<a href="#fn389" class="footnote-ref" id="fnref389" role="doc-noteref"><sup>389</sup></a></li>
</ul>
<h3 id="thermodynamics-of-computation">Thermodynamics of computation</h3>
<ul>
<li>Wolpert, D. (2018). <a href="https://blogs.scientificamerican.com/observations/why-do-computers-use-so-much-energy/">Why do computers use so much energy?</a></li>
<li><a href="https://centre.santafe.edu/thermocomp/Santa_Fe_Institute_Collaboration_Platform:Thermodynamics_of_Computation_Wiki">Sante Fe Institute: Thermodynamics of Computation</a></li>
</ul>
<h2 id="information-geometry">Information geometry</h2>
<h3 id="introduction-9">Introduction</h3>
<ul>
<li>Smith, L. (2019). <a href="http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html">A gentle introduction to information geometry</a>.<a href="#fn390" class="footnote-ref" id="fnref390" role="doc-noteref"><sup>390</sup></a></li>
<li>Nielsen, F. (2020). <a href="https://arxiv.org/abs/1808.08271">An elementary introduction to information geometry</a>.<a href="#fn391" class="footnote-ref" id="fnref391" role="doc-noteref"><sup>391</sup></a></li>
<li>Amari, S. (1998). Natural gradient works efficiently in learning.<a href="#fn392" class="footnote-ref" id="fnref392" role="doc-noteref"><sup>392</sup></a></li>
<li>Amari, S. (2016). <em>Information Geometry and Its Applications</em>.<a href="#fn393" class="footnote-ref" id="fnref393" role="doc-noteref"><sup>393</sup></a></li>
<li>Geomstats tutorial: <a href="https://geomstats.github.io/notebooks/06_information_geometry.html">Information geometry</a></li>
</ul>
<h3 id="geometric-understanding-of-classical-statistics">Geometric understanding of classical statistics</h3>
<ul>
<li>Balasubramanian, V. (1996). <a href="https://arxiv.org/abs/adap-org/9601001">A geometric formulation of Occam&#x2019;s razor for inference of parametric distributions</a>.<a href="#fn394" class="footnote-ref" id="fnref394" role="doc-noteref"><sup>394</sup></a></li>
<li>Balasubramanian, V. (1996). <a href="https://arxiv.org/abs/cond-mat/9601030">Statistical inference, Occam&#x2019;s razor and statistical mechanics on the space of probability distributions</a>.<a href="#fn395" class="footnote-ref" id="fnref395" role="doc-noteref"><sup>395</sup></a></li>
<li>Calin, O. &amp; Udriste, C. (2014). <em>Geometric Modeling in Probability and Statistics</em>.<a href="#fn396" class="footnote-ref" id="fnref396" role="doc-noteref"><sup>396</sup></a></li>
<li>de Carvalho, M., Page, G.L., &amp; Barney, B.J. (2019). <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-4/On-the-Geometry-of-Bayesian-Inference/10.1214/18-BA1112.full">On the geometry of Bayesian inference</a>.<a href="#fn397" class="footnote-ref" id="fnref397" role="doc-noteref"><sup>397</sup></a></li>
<li>Cranmer: <a href="http://theoryandpractice.org/stats-ds-book/statistics/information-geometry.html">Information geometry</a> (coming soon?)</li>
</ul>
<h3 id="geometric-understanding-of-deep-learning">Geometric understanding of deep learning</h3>
<ul>
<li>Lei, N. et al.&#xA0;(2018). Geometric understanding of deep learning.<a href="#fn398" class="footnote-ref" id="fnref398" role="doc-noteref"><sup>398</sup></a></li>
<li>Gao, Y. &amp; Chaudhari, P. (2020). <a href="https://arxiv.org/abs/2011.00613">An information-geometric distance on the space of tasks</a>.<a href="#fn399" class="footnote-ref" id="fnref399" role="doc-noteref"><sup>399</sup></a></li>
<li>Bronstein, M.M. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2104.13478">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</a>.<a href="#fn400" class="footnote-ref" id="fnref400" role="doc-noteref"><sup>400</sup></a></li>
</ul>
<h2 id="automation">Automation</h2>
<h3 id="automl">AutoML</h3>
<ul>
<li>Neural Architecture Search (NAS)</li>
<li>AutoML frameworks</li>
<li>RL-driven NAS</li>
<li>learned sparsity</li>
</ul>
<h3 id="surrogate-models">Surrogate models</h3>
<ul>
<li>Autoencoders, latent variables
<ul>
<li>The manifold hypothesis</li>
<li>Olah, C. (2014). <a href="https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/">Neural networks, manifolds, and topology</a>.</li>
<li>Fefferman, C., Mitter, S., &amp; Narayanan, H. (2016). <a href="https://www.ams.org/journals/jams/2016-29-04/S0894-0347-2016-00852-4/S0894-0347-2016-00852-4.pdf">Testing the manifold hypothesis</a>.<a href="#fn401" class="footnote-ref" id="fnref401" role="doc-noteref"><sup>401</sup></a></li>
</ul></li>
<li>Physical constraints in loss functions
<ul>
<li>Raissi, M., Perdikaris, P., &amp; Karniadakis, G.E. (2017). <a href="https://arxiv.org/abs/1711.10561">Physics informed deep learning (Part I)</a> and <a href="https://arxiv.org/abs/1711.10566">(Part II)</a>.<a href="#fn402" class="footnote-ref" id="fnref402" role="doc-noteref"><sup>402</sup></a></li>
<li>Karniadakis, G.E. et al.&#xA0;(2021). <a href="https://www.nature.com/articles/s42254-021-00314-5">Physics-informed machine learning</a>.<a href="#fn403" class="footnote-ref" id="fnref403" role="doc-noteref"><sup>403</sup></a></li>
<li>Howard, J.N. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2101.08944">Foundations of a fast, data-driven, machine-learned simulator</a>.<a href="#fn404" class="footnote-ref" id="fnref404" role="doc-noteref"><sup>404</sup></a></li>
<li>Thuerey, N. et al.&#xA0;(2021). <a href="https://arxiv.org/abs/2109.05237">Physics-based deep learning</a>.<a href="#fn405" class="footnote-ref" id="fnref405" role="doc-noteref"><sup>405</sup></a></li>
<li><a href="https://physicsbaseddeeplearning.org/intro.html">physicsbaseddeeplearning.org</a></li>
</ul></li>
<li>Simulation-based inference
<ul>
<li>Cranmer, K., Pavez, J., &amp; Louppe, G. (2015). <a href="https://arxiv.org/abs/1506.02169">Approximating likelihood ratios with calibrated discriminative classifiers</a>.<a href="#fn406" class="footnote-ref" id="fnref406" role="doc-noteref"><sup>406</sup></a></li>
<li>Cranmer, K., Brehmer, J., &amp; Louppe, G. (2019). <a href="https://arxiv.org/abs/1911.01429">The frontier of simulation-based inference</a>.<a href="#fn407" class="footnote-ref" id="fnref407" role="doc-noteref"><sup>407</sup></a></li>
<li>Baydin, A.G. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1907.03382">Etalumis: Bringing probabilistic programming to scientific simulators at scale</a>.<a href="#fn408" class="footnote-ref" id="fnref408" role="doc-noteref"><sup>408</sup></a></li>
</ul></li>
</ul>
<p>Lectures:</p>
<ul>
<li>Paul Hand. (2020). <a href="https://www.youtube.com/watch?v=IpbeIwSr7r0">Invertible neural networks and inverse problems</a>.</li>
</ul>
<h3 id="autoscience">AutoScience</h3>
<ul>
<li>Automated discovery
<ul>
<li>Anderson, C. (2008). <a href="https://www.wired.com/2008/06/pb-theory/">The End of Theory: The data deluge makes the scientific method obsolete</a>.<a href="#fn409" class="footnote-ref" id="fnref409" role="doc-noteref"><sup>409</sup></a></li>
<li>Cranmer, K. (2017). <a href="https://github.com/cranmer/active_sciencing">Active sciencing</a>.</li>
<li>Asch, M. et al.&#xA0;(2018). Big data and extreme-scale computing: Pathways to Convergence-Toward a shaping strategy for a future software and data ecosystem for scientific inquiry.<a href="#fn410" class="footnote-ref" id="fnref410" role="doc-noteref"><sup>410</sup></a></li>
<li>D&#x2019;Agnolo, R.T. &amp; Wulzer, A. (2019). <a href="https://arxiv.org/abs/1806.02350">Learning New Physics from a Machine</a>.<a href="#fn411" class="footnote-ref" id="fnref411" role="doc-noteref"><sup>411</sup></a>
<ul>
<li>Note that this description of abduction is missing that it is normative (i.e.&#xA0;&#x201C;best-fit&#x201D;).</li>
</ul></li>
<li>Krenn, M. et al.&#xA0;(2022). <a href="https://arxiv.org/abs/2204.01467">On scientific understanding with artificial intelligence</a>.<a href="#fn412" class="footnote-ref" id="fnref412" role="doc-noteref"><sup>412</sup></a></li>
</ul></li>
<li>Symbolic regression
<ul>
<li>Udrescu, S. &amp; Tegmark, M. (2020). <a href="https://arxiv.org/abs/2005.11212">Symbolic pregression: Discovering physical laws from raw distorted video</a>.<a href="#fn413" class="footnote-ref" id="fnref413" role="doc-noteref"><sup>413</sup></a></li>
<li>Cranmer, M. et al.&#xA0;(2020). <a href="https://arxiv.org/abs/2006.11287">Discovering symbolic models from deep learning with inductive biases</a>.<a href="#fn414" class="footnote-ref" id="fnref414" role="doc-noteref"><sup>414</sup></a>
<ul>
<li>Video: <a href="https://www.youtube.com/watch?v=LMb5tvW-UoQ">Discussion by Yannic Kilcher</a></li>
</ul></li>
<li>Liu, Z., Madhavan, V., &amp; Tegmark, M. (2022). <a href="https://arxiv.org/abs/2203.12610">AI Poincare 2.0: Machine learning conservation laws from differential equations</a>.<a href="#fn415" class="footnote-ref" id="fnref415" role="doc-noteref"><sup>415</sup></a></li>
<li>Cranmer, M. (2024). Video of seminar: <a href="https://www.youtube.com/watch?v=fk2r8y5TfNY">The next great scientific theory is hiding inside a neural network</a>.</li>
</ul></li>
</ul>
<figure>
<img src="img/BDEC-scientific-method.png" id="fig:BDEC-scientific-method" alt="Figure 14: The inference cycle for the process of scientific inquiry. The three distinct forms of inference (abduction, deduction, and induction) facilitate an all-encompassing vision, enabling HPC and HDA to converge in a rational and structured manner. HPC: high- performance computing; HDA: high-end data analysis." /><figcaption aria-hidden="true">Figure 14: The inference cycle for the process of scientific inquiry. The three distinct forms of inference (abduction, deduction, and induction) facilitate an all-encompassing vision, enabling HPC and HDA to converge in a rational and structured manner. HPC: high- performance computing; HDA: high-end data analysis.<a href="#fn416" class="footnote-ref" id="fnref416" role="doc-noteref"><sup>416</sup></a></figcaption>
</figure>
<p>See also:</p>
<ul>
<li><a href="http://rreece.github.io/outline-of-philosophy/future.html#artificial-intelligence">Artificial intelligence</a> in the <a href="http://rreece.github.io/outline-of-philosophy/future.html">Outline of futures studies</a>.</li>
</ul>
<h2 id="implications-for-the-realism-debate">Implications for the realism debate</h2>
<h3 id="introduction-10">Introduction</h3>
<ul>
<li>Korb<a href="#fn417" class="footnote-ref" id="fnref417" role="doc-noteref"><sup>417</sup></a></li>
<li>Williamson<a href="#fn418" class="footnote-ref" id="fnref418" role="doc-noteref"><sup>418</sup></a></li>
<li>Bensusan<a href="#fn419" class="footnote-ref" id="fnref419" role="doc-noteref"><sup>419</sup></a></li>
</ul>
<p>See also:</p>
<ul>
<li><a href="scientific-realism.html">Outline on scientific realism</a></li>
</ul>
<h3 id="real-clusters">Real clusters</h3>
<ul>
<li>Nope: Hennig</li>
</ul>
<p>See also:</p>
<ul>
<li><a href="#clustering">Clustering</a></li>
<li><a href="naturalism.html#natural-kinds">Natural kinds</a></li>
</ul>
<h3 id="word-meanings">Word meanings</h3>
<ul>
<li>Note that NLP has implications to the philosophy of language and realism</li>
<li>Olah, C. (2014). <a href="https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/">Deep learning, NLP, and representations</a>.</li>
<li>Perone, C.S. (2018). <a href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/">NLP word representations and the Wittgenstein philosophy of language</a>.<a href="#fn420" class="footnote-ref" id="fnref420" role="doc-noteref"><sup>420</sup></a></li>
<li>Belloni, M. (2019). <a href="https://towardsdatascience.com/neural-networks-and-philosophy-of-language-31c34c0796da">Neural networks and philosophy of language: Why Wittgenstein&#x2019;s theories are the basis of all modern NLP</a>.</li>
<li>Goldhill, O. (2019). <a href="https://qz.com/1549212/google-translate-is-a-manifestation-of-wittgensteins-theory-of-language">Google Translate is a manifestation of Wittgenstein&#x2019;s theory of language</a>.</li>
<li>Tenney, I. et al.&#xA0;(2019). <a href="https://arxiv.org/abs/1905.06316">What do you learn from context? Probing for sentence structure in contextualized word representations</a>.<a href="#fn421" class="footnote-ref" id="fnref421" role="doc-noteref"><sup>421</sup></a></li>
<li>Nissim, M., van Noord, R., &amp; van der Goot, R. (2019). Fair is better than sensational: Man is to doctor as woman is to doctor.<a href="#fn422" class="footnote-ref" id="fnref422" role="doc-noteref"><sup>422</sup></a></li>
<li>Skelac, I. &amp; Jandric, A. (2020). Meaning as use: From Wittgenstein to Google&#x2019;s Word2vec.<a href="#fn423" class="footnote-ref" id="fnref423" role="doc-noteref"><sup>423</sup></a></li>
<li>Boccelli, D. (2022). <a href="https://towardsdatascience.com/word-embeddings-align-with-kandinskys-theory-of-color-26288b864834">Word embeddings align with Kandinsky&#x2019;s theory of color</a>.</li>
<li>Patel, R. &amp; Pavlick, E. (2022). <a href="https://openreview.net/pdf?id=gJcEM8sxHK">Mapping language models to grounded conceptual spaces</a>.<a href="#fn424" class="footnote-ref" id="fnref424" role="doc-noteref"><sup>424</sup></a></li>
<li>Lovering, C. &amp; Pavlick, E. (2022). <a href="https://aclanthology.org/2022.tacl-1.69/">Unit testing for concepts in neural networks</a>.<a href="#fn425" class="footnote-ref" id="fnref425" role="doc-noteref"><sup>425</sup></a></li>
<li><a href="https://twitter.com/Plinz/status/1639629419881873410">Tweet by Joscha Bach, Mar 25, 2023</a></li>
<li><a href="https://www.youtube.com/watch?v=x10964w00zk">Debate: Do language models need sensory grounding for meaning and understanding?</a> (NYU).</li>
<li>Piantadosi, S. (2023). Talk: <a href="https://www.youtube.com/watch?v=lA19zXgObKA">Meaning in the age of large language models</a>.</li>
</ul>
<p>Wittgenstein in <em>PI</em>:</p>
<blockquote>
<p>The meaning of a word is its use in the language.<a href="#fn426" class="footnote-ref" id="fnref426" role="doc-noteref"><sup>426</sup></a></p>
</blockquote>
<p>and</p>
<blockquote>
<p>One cannot guess how a word functions. One has to look at its use, and learn from that.<a href="#fn427" class="footnote-ref" id="fnref427" role="doc-noteref"><sup>427</sup></a></p>
</blockquote>
<p>Piantadosi:</p>
<blockquote>
<p>Modern large language models integrate syntax and semantics in the underlying representations: encoding words as vectors in a high-dimensional space, without an effort to separate out e.g.&#xA0;part of speech categories from semantic representations, or even predict at any level of analysis other than the literal word. Part of making these models work well was in determining how to encode semantic properties into vectors, and in fact initializing word vectors via encodings of distribution semantics from e.g.&#xA0;Mikolov et al.&#xA0;2013 (Radford et al.&#xA0;2019). Thus, an assumption of the autonomy of syntax is not required to make models that predict syntactic material and may well hinder it.<a href="#fn428" class="footnote-ref" id="fnref428" role="doc-noteref"><sup>428</sup></a></p>
</blockquote>
<p>See also:</p>
<ul>
<li><a href="statistics.html#natural-language-processing">Natural language processing</a></li>
<li><a href="scientific-realism.html#ordinary-language-philosophy">Ordinary language philosophy</a></li>
</ul>
<h2 id="my-thoughts">My thoughts</h2>
<p>My docs:</p>
<ul>
<li><a href="http://rreece.github.io/publications/pdf/2009.Reece.Derivation-of-the-Cramer-Rao-Bound.pdf">Derivation of the Cram&#xE9;r-Rao Bound</a></li>
</ul>
<p>My talks:</p>
<ul>
<li><a href="http://rreece.github.io/talks/pdf/2009-09-29-RReece-Likelihood-functions-for-SUSY.pdf">Likelihood functions for supersymmetric observables</a></li>
<li><a href="http://rreece.github.io/talks/pdf/2017-03-16-RReece-Machine-learning-and-realism.pdf">Machine learning and realism</a></li>
<li><a href="http://rreece.github.io/talks/pdf/2018-02-16-RReece-statistics-workshop-insight.pdf">Primer on statistics: MLE, confidence intervals, and hypothesis testing</a></li>
</ul>
<h2 id="annotated-bibliography">Annotated bibliography</h2>
<div class="clickmore">
<a id="link:annotated_bibliography" class="closed" onclick="toggle_more('annotated_bibliography')"> Click to show annotated bibliography </a>
</div>
<div id="annotated_bibliography" class="more">
<h3 id="mayo-d.g.-1996.-error-and-the-growth-of-experimental-knowledge.">Mayo, D.G. (1996). <em>Error and the Growth of Experimental Knowledge</em>.</h3>
<ul>
<li><span class="citation" data-cites="Mayo_1996_Error_and_the_Growth_of_Experimental_Knowledge">Mayo (1996)</span></li>
</ul>
<h4 id="my-thoughts-1">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cowan-g.-1998.-statistical-data-analysis.">Cowan, G. (1998). <em>Statistical Data Analysis</em>.</h3>
<ul>
<li><span class="citation" data-cites="Cowan_1998_Statistical_Data_Analysis">Cowan (1998)</span> and <span class="citation" data-cites="Cowan_2016_StatisticsIn_CPatrignani_et_alParticle_Data">Cowan (2016)</span></li>
</ul>
<h4 id="my-thoughts-2">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="james-f.-2006.-statistical-methods-in-experimental-physics.">James, F. (2006). <em>Statistical Methods in Experimental Physics</em>.</h3>
<ul>
<li><span class="citation" data-cites="James_2006_Statistical_Methods_in_Experimental_Particle">F. James (2006)</span></li>
</ul>
<h4 id="my-thoughts-3">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cowan-g.-et-al.-2011.-asymptotic-formulae-for-likelihood-based-tests-of-new-physics.">Cowan, G. <em>et al.</em> (2011). Asymptotic formulae for likelihood-based tests of new physics.</h3>
<ul>
<li><span class="citation" data-cites="Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">Cowan et al. (2011)</span></li>
<li>Glen Cowan, Kyle Cranmer, Eilam Gross, Ofer Vitells</li>
</ul>
<h4 id="my-thoughts-4">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="atlas-collaboration.-2012.-combined-search-for-the-standard-model-higgs-boson.">ATLAS Collaboration. (2012). Combined search for the Standard Model Higgs boson.</h3>
<ul>
<li><span class="citation" data-cites="ATLAS_2012_Combined_search_for_the_Standard_Model_Higgs_boson">ATLAS Collaboration (2012)</span></li>
<li><a href="http://arxiv.org/abs/1207.0319">arxiv:1207.0319</a></li>
</ul>
<h4 id="my-thoughts-5">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="cranmer-k.-2015.-practical-statistics-for-the-lhc.">Cranmer, K. (2015). Practical statistics for the LHC.</h3>
<ul>
<li><span class="citation" data-cites="Cranmer_2015_Practical_statistics_for_the_LHC">Cranmer (2015)</span></li>
</ul>
<h4 id="my-thoughts-6">My thoughts</h4>
<ul>
<li>TODO</li>
</ul>
<hr />
<h3 id="more-articles-to-do">More articles to do</h3>
<ul>
<li><em>All of Statistics</em><a href="#fn429" class="footnote-ref" id="fnref429" role="doc-noteref"><sup>429</sup></a></li>
<li><em>The Foundations of Statistics</em><a href="#fn430" class="footnote-ref" id="fnref430" role="doc-noteref"><sup>430</sup></a></li>
</ul>
</div>
<h2 id="links-and-encyclopedia-articles">Links and encyclopedia articles</h2>
<div class="clickmore">
<a id="link:encyclopedia_articles" class="closed" onclick="toggle_more('encyclopedia_articles')"> Click to show links </a>
</div>
<div id="encyclopedia_articles" class="more">
<h3 id="sep">SEP</h3>
<ul>
<li><a href="http://plato.stanford.edu/entries/abduction/">Abduction</a></li>
<li><a href="http://plato.stanford.edu/entries/knowledge-analysis/">Analysis of knowledge</a></li>
<li><a href="https://plato.stanford.edu/entries/bayes-theorem/">Bayes&#x2019; theorem</a></li>
<li><a href="https://plato.stanford.edu/entries/epistemology-bayesian/">Bayesian epistemology</a></li>
<li><a href="https://plato.stanford.edu/entries/carnap/">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="https://plato.stanford.edu/entries/causal-models/">Causal models</a></li>
<li><a href="http://plato.stanford.edu/entries/causation-process/">Causal processes</a></li>
<li><a href="https://plato.stanford.edu/entries/confirmation/">Confirmation</a></li>
<li><a href="https://plato.stanford.edu/entries/dutch-book/">Dutch book arguments</a></li>
<li><a href="http://plato.stanford.edu/entries/epistemology/">Epistemology</a></li>
<li><a href="http://plato.stanford.edu/entries/justep-foundational/">Foundationalist Theories of Epistemic Justification</a></li>
<li><a href="http://plato.stanford.edu/entries/hume/">Hume, David (1711-1776)</a></li>
<li><a href="http://plato.stanford.edu/entries/identity-indiscernible/">Identity of indiscernibles</a></li>
<li><a href="http://plato.stanford.edu/entries/induction-problem/">Induction, The problem of</a></li>
<li><a href="https://plato.stanford.edu/entries/logic-probability/">Logic and Probability</a></li>
<li><a href="http://plato.stanford.edu/entries/epistemology-naturalized/">Naturalized epistemology</a></li>
<li><a href="https://plato.stanford.edu/entries/peirce/">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="http://plato.stanford.edu/entries/popper/">Popper, Karl (1902-1994)</a></li>
<li><a href="http://plato.stanford.edu/entries/sufficient-reason/">Principle of sufficient reason</a></li>
<li><a href="https://plato.stanford.edu/entries/probability-interpret/">Probability, Interpretations of</a></li>
<li><a href="https://plato.stanford.edu/entries/causation-probabilistic/">Probabilistic pausation</a></li>
<li><a href="https://plato.stanford.edu/entries/reichenbach/">Reichenbach, Hans (1891-1953)</a></li>
<li><a href="http://plato.stanford.edu/entries/scientific-explanation/">Scientific explanation</a></li>
<li><a href="https://plato.stanford.edu/entries/science-big-data/">Scientific research and big data</a></li>
<li><a href="http://plato.stanford.edu/entries/statistics/">Statistics, Philosophy of</a></li>
</ul>
<h3 id="iep">IEP</h3>
<ul>
<li><a href="http://www.iep.utm.edu/carnap/">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="http://www.iep.utm.edu/epistemo/">Epistemology</a></li>
<li><a href="http://www.iep.utm.edu/hempel/">Hempel, Carl Gustav (1905-1997)</a></li>
<li><a href="http://www.iep.utm.edu/hume-cau/">Hume, David (1711-1776)</a></li>
<li><a href="http://www.iep.utm.edu/naturali/">Naturalism</a></li>
<li><a href="http://www.iep.utm.edu/nat-epis/">Naturalistic Epistemology</a></li>
<li><a href="https://www.iep.utm.edu/peircebi/">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="http://www.iep.utm.edu/red-ism/">Reductionism</a></li>
<li><a href="http://www.iep.utm.edu/safety-c/">Safety Condition for Knowledge, The</a></li>
<li><a href="http://www.iep.utm.edu/simplici/">Simplicity in the philosophy of science</a></li>
<li><a href="http://www.iep.utm.edu/ockham/">William of Ockham (1280-1349)</a></li>
</ul>
<h3 id="scholarpedia">Scholarpedia</h3>
<ul>
<li><a href="http://www.scholarpedia.org/article/Algorithmic_information_theory">Algorithmic information theory</a></li>
<li><a href="http://www.scholarpedia.org/article/Algorithmic_probability">Algorithmic probability</a></li>
<li><a href="http://www.scholarpedia.org/article/Universal_search">Universal search</a></li>
</ul>
<h3 id="wikipedia">Wikipedia</h3>
<ul>
<li><a href="http://en.wikipedia.org/wiki/Abductive_reasoning">Abductive reasoning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Akaike_information_criterion">Akaike_information_criterion</a></li>
<li><a href="https://en.wikipedia.org/wiki/Algorithmic_information_theory">Algorithmic information theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Algorithmic_probability">Algorithmic probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/Analysis_of_variance">Analysis of variance</a></li>
<li><a href="https://en.wikipedia.org/wiki/Aumann%27s_agreement_theorem">Aumann&#x2019;s agreement theorem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Thomas_Bayes">Bayes, Thomas (1701-1761)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Bayesian_inference">Bayesian inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jacob_Bernoulli">Bernoulli, Jacob (1655-1705)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Allan_Birnbaum">Birnbaum, Allan (1923-1976)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Bootstrapping_(statistics)">Bootstrapping</a></li>
<li><a href="http://en.wikipedia.org/wiki/Rudolf_Carnap">Carnap, Rudolf (1891-1970)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Confidence_interval">Confidence interval</a></li>
<li><a href="https://en.wikipedia.org/wiki/Cosmic_variance">Cosmic variance</a></li>
<li><a href="http://en.wikipedia.org/wiki/Cram%C3%A9r%E2%80%93Rao_bound">Cram&#xE9;r-Rao bound</a></li>
<li><a href="https://en.wikipedia.org/wiki/Harald_Cram%C3%A9r">Cram&#xE9;r, Harald (1893-1985)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Data_science">Data science</a></li>
<li><a href="https://en.wikipedia.org/wiki/Decision_theory">Decision theory</a></li>
<li><a href="https://en.wikipedia.org/wiki/Deductive-nomological_model">Deductive-nomological model</a></li>
<li><a href="http://en.wikipedia.org/wiki/Empiricism">Empiricism</a></li>
<li><a href="http://en.wikipedia.org/wiki/Epistemology">Epistemology</a></li>
<li><a href="https://en.wikipedia.org/wiki/Exploratory_data_analysis">Exploratory data analysis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ronald_Fisher">Fisher, Ronald (1890-1962)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Frequentist_inference">Frequentist inference</a></li>
<li><a href="https://en.wikipedia.org/wiki/Foundations_of_statistics">Foundations of statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/Gauss">Gauss, Carl Friedrich (1777-1855)</a></li>
<li><a href="https://en.wikipedia.org/wiki/German_tank_problem">German tank problem</a></li>
<li><a href="https://en.wikipedia.org/wiki/William_Sealy_Gosset">Gosset, William Sealy (1876-1937)</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Graunt">Graunt, John (1620-1674)</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_probability">History of probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/History_of_statistics">History of statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/David_Hume">Hume, David (1711-1776)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Problem_of_induction">Induction, The problem of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inductive_reasoning">Inductive reasoning</a></li>
<li><a href="http://en.wikipedia.org/wiki/Interval_estimation">Interval estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inverse_probability">Inverse probability</a></li>
<li><a href="https://en.wikipedia.org/wiki/Inverse_problem">Inverse problem</a></li>
<li><a href="https://en.wikipedia.org/wiki/Alexey_Ivakhnenko">Ivakhnenko, Alexey (1913-2007)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Richard_Jeffrey">Jeffrey, Richard (1926-2002)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Harold_Jeffreys">Jeffreys, Harold (1891-1989)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jeffreys_prior">Jeffreys prior</a></li>
<li><a href="https://en.wikipedia.org/wiki/Andrey_Kolmogorov">Kolmogorov, Andrey (1903-1987)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Kolmogorov_complexity">Kolmogorov complexity</a></li>
<li><a href="https://en.wikipedia.org/wiki/Lady_tasting_tea">Lady tasting tea</a></li>
<li><a href="https://en.wikipedia.org/wiki/Pierre-Simon_Laplace">Laplace, Pierre-Simon (1749-1827)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Likelihood_principle">Likelihood principle</a></li>
<li><a href="https://en.wikipedia.org/wiki/Likelihoodist_statistics">Likelihoodist statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/List_of_important_publications_in_statistics">List of important publications in statistics</a></li>
<li><a href="https://en.wikipedia.org/wiki/Machine_learning">Machine learning</a></li>
<li><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Maximum likelihood estimation</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Stuart_Mill">Mill, John Stuart (1806-1873)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Misuse_of_p-values">Misuse of p-values</a></li>
<li><a href="https://en.wikipedia.org/wiki/Jerzy_Neyman">Neyman, Jerzy (1894-1981)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Neyman_construction">Neyman construction</a></li>
<li><a href="http://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma">Neyman-Pearson lemma</a></li>
<li><a href="http://en.wikipedia.org/wiki/William_of_Ockham">Ockham, William of (1287-1347)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Egon_Pearson">Pearson, Egon (1895-1980)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Pearson">Pearson, Karl (1857-1936)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Charles_Sanders_Peirce">Peirce, Charles Sanders (1839-1914)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Sim%C3%A9on_Denis_Poisson">Poisson, Sim&#xE9;on Denis (1781-1840)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Karl_Popper">Popper, Karl (1902-1994)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Principle_of_sufficient_reason">Principle of sufficient reason</a></li>
<li><a href="https://en.wikipedia.org/wiki/Precision_and_recall">Precision and recall</a></li>
<li><a href="https://en.wikipedia.org/wiki/Proteus_phenomenon">Proteus phenomenon</a></li>
<li><a href="https://en.wikipedia.org/wiki/P-value">P-value</a></li>
<li><a href="https://en.wikipedia.org/wiki/C._R._Rao">Rao, C.R. (b. 1920)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Replication_crisis">Replication crisis</a></li>
<li><a href="https://en.wikipedia.org/wiki/Rule_of_three_(statistics)">Rule of three</a></li>
<li><a href="https://en.wikipedia.org/wiki/Leonard_Jimmie_Savage">Savage, Leonard Jimmie (1917-1971)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Ray_Solomonoff">Solomonoff, Ray (1926-2009)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Solomonoff%27s_theory_of_inductive_inference">Solomonoff&#x2019;s theory of inductive inference</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_classification">Statistical classification</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_hypothesis_testing">Statistical hypothesis testing</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_inference">Statistical inference</a></li>
<li><a href="http://en.wikipedia.org/wiki/Sensitivity_and_specificity">Statistical sensitivity and specificity</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistical_significance">Statistical significance</a></li>
<li><a href="http://en.wikipedia.org/wiki/Statistics">Statistics</a></li>
<li><a href="http://en.wikipedia.org/wiki/Founders_of_statistics">Statistics, Founders of</a></li>
<li><a href="http://en.wikipedia.org/wiki/History_of_statistics">Statistics, History of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Mathematical_statistics">Statistics, Mathematical</a></li>
<li><a href="http://en.wikipedia.org/wiki/Outline_of_statistics">Statistics, Outline of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Philosophy_of_statistics">Statistics, Philosophy of</a></li>
<li><a href="https://en.wikipedia.org/wiki/Student%27s_t-test">Student&#x2019;s t-test</a></li>
<li><a href="http://en.wikipedia.org/wiki/Systematic_error">Systematic error</a></li>
<li><a href="https://en.wikipedia.org/wiki/Edward_O._Thorp">Thorp, Edward O. (b. 1932)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Trial_and_error">Trial and error</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Tukey">Tukey, John (1915-2000)</a></li>
<li><a href="http://en.wikipedia.org/wiki/Type_I_and_type_II_errors">Type-I and type-II errors</a></li>
<li><a href="https://en.wikipedia.org/wiki/Uncomfortable_science">Uncomfortable science</a></li>
<li><a href="http://en.wikipedia.org/wiki/Uniformitarianism">Uniformitarianism</a></li>
<li><a href="http://en.wikipedia.org/wiki/List_of_unsolved_problems_in_statistics">Unsolved problems in statistics, List of</a></li>
<li><a href="https://en.wikipedia.org/wiki/John_Venn">Venn, John (1834-1923)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Samuel_S._Wilks">Wilks, S.S. (1906-1964)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Wilks%27_theorem">Wilks&#x2019;s theorem</a></li>
</ul>
<h3 id="others">Others</h3>
<ul>
<li><a href="http://people.idsia.ch/~juergen/deep-learning-miraculous-year-1990-1991.html">Deep Learning: Our Miraculous Year 1990-1991</a> - Schmidhuber</li>
<li><a href="http://errorstatistics.com/">errorstatistics.com</a> - Deborah Mayo&#x2019;s blog</li>
<li><a href="http://statprob.com/encyclopedia/JohnGRAUNT.html">Graunt, John (1620-1674)</a> - statprob.com</li>
<li>Peng, R. (2016). <a href="http://simplystatistics.org/2016/08/24/replication-crisis/">A Simple Explanation for the Replication Crisis in Science.</a> - simplystatistics.org</li>
<li><a href="https://stats.stackexchange.com/questions/240138/why-is-binary-classification-not-a-hypothesis-test">Why is binary classification not a hypothesis test?</a> - stackexchange.com</li>
<li><a href="https://stats.stackexchange.com/questions/40856/if-the-likelihood-principle-clashes-with-frequentist-probability-then-do-we-disc">If the likelihood principle clashes with frequentist probability then do we discard one of them?</a> - stackexchange.com</li>
<li><a href="https://stephens999.github.io/fiveMinuteStats/wilks.html">Wilks&#x2019;s theorem</a> - fiveMinuteStats</li>
<li>Dallal, G.E. (2012). <a href="http://www.jerrydallal.com/LHSP/LHSP.HTM">The Little Handbook of Statistical Practice</a>.</li>
</ul>
</div>
<!-- REFERENCES -->
<h2 id="references">References</h2>
<p></p>
<div id="refs" class="references csl-bib-body hanging-indent" data-line-spacing="2" role="doc-bibliography">
<div id="ref-Agresti_1998_Approximate_is_better_than_exact_for_interval" class="csl-entry" role="doc-biblioentry">
Agresti, A. &amp; Coull, B. A. (1998). <span class="nocase">Approximate is better than "exact" for interval estimation of binomial proportions.</span> <em>The American Statistician</em>, <em><span>52</span></em>, 119&#x2013;126.
</div>
<div id="ref-Aifer_2023_Thermodynamic_linear_algebra" class="csl-entry" role="doc-biblioentry">
Aifer, M. et al. (2023). <span class="nocase">Thermodynamic linear algebra</span>. <a href="https://arxiv.org/abs/2308.05660">https://arxiv.org/abs/2308.05660</a>
</div>
<div id="ref-Alain_2016_Understanding_intermediate_layers_using_linear" class="csl-entry" role="doc-biblioentry">
Alain, G. &amp; Bengio, Y. (2016). <span class="nocase">Understanding intermediate layers using linear classifier probes</span>. <a href="https://arxiv.org/abs/1610.01644">https://arxiv.org/abs/1610.01644</a>
</div>
<div id="ref-Aldrich_1997_RAFisher_and_the_making_of_maximum_likelihood" class="csl-entry" role="doc-biblioentry">
Aldrich, J. (1997). <span class="nocase">R. A. Fisher and the making of maximum likelihood 1912-1922.</span> <em>Statistical Science</em>, <em><span>12</span></em>, 162&#x2013;176.
</div>
<div id="ref-Amari_1998_Natural_gradient_works_efficiently_in_learning" class="csl-entry" role="doc-biblioentry">
Amari, S. (1998). <span class="nocase">Natural gradient works efficiently in learning.</span> <em>Neural Computation</em>, <em><span>10</span></em>, 251&#x2013;276.
</div>
<div id="ref-Amari_2016_Information_Geometry_and_Its_Applications" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2016). <em><span class="nocase">Information Geometry and Its Applications</span></em>. <span>Springer Japan</span>.
</div>
<div id="ref-Anderson_2008_The_End_of_Theory_The_data_deluge_makes" class="csl-entry" role="doc-biblioentry">
Anderson, C. (2008). <span class="nocase">The End of Theory: The data deluge makes the scientific method obsolete.</span> <em>Wired</em>. June 23, 2008. <a href="https://www.wired.com/2008/06/pb-theory/">https://www.wired.com/2008/06/pb-theory/</a>
</div>
<div id="ref-Andrews_2023_The_devil_in_the_data_Machine_learning" class="csl-entry" role="doc-biblioentry">
Andrews, M. (2023). <span class="nocase">The devil in the data: Machine learning &amp; the theory-free ideal</span>. <a href="https://philsci-archive.pitt.edu/22690/1/ML_Atheoreticity.pdf">https://philsci-archive.pitt.edu/22690/1/ML_Atheoreticity.pdf</a>
</div>
<div id="ref-Arras_1998_An_introduction_to_error_propagation_Derivation" class="csl-entry" role="doc-biblioentry">
Arras, K. O. (1998). <span class="nocase">An introduction to error propagation: Derivation, meaning and examples of <span class="math inline">\(C_y= F_x C_x F_{x}^{\top}\)</span></span>. EPFL-ASL-TR-98-01 R3. <a href="http://srl.informatik.uni-freiburg.de/papers/arrasTR98.pdf">http://srl.informatik.uni-freiburg.de/papers/arrasTR98.pdf</a>
</div>
<div id="ref-Arulkumaran_2017_Deep_Reinforcement_Learning_A_Brief_Survey" class="csl-entry" role="doc-biblioentry">
Arulkumaran, K., Deisenroth, M. P., Brundage, M., &amp; Bharath, A. A. (2017). <span>Deep Reinforcement Learning: A Brief Survey.</span> <em>IEEE Signal Processing Magazine</em>, <em><span>34</span></em>, 26&#x2013;38.
</div>
<div id="ref-Asch_2018_Big_data_and_extreme_scale_computing_Pathways" class="csl-entry" role="doc-biblioentry">
Asch, M. et al. (2018). <span class="nocase">Big data and extreme-scale computing: Pathways to Convergence-Toward a shaping strategy for a future software and data ecosystem for scientific inquiry.</span> <em>The International Journal of High Performance Computing Applications</em>, <em><span>32</span></em>, 435&#x2013;479.
</div>
<div id="ref-ATLAS_2011_Procedure_for_the_LHC_Higgs_boson_search" class="csl-entry" role="doc-biblioentry">
ATLAS and CMS Collaborations. (2011). <span class="nocase">Procedure for the LHC Higgs boson search combination in Summer 2011</span>. CMS-NOTE-2011-005, ATL-PHYS-PUB-2011-11. <a href="http://cds.cern.ch/record/1379837">http://cds.cern.ch/record/1379837</a>
</div>
<div id="ref-ATLAS_2012_Combined_search_for_the_Standard_Model_Higgs_boson" class="csl-entry" role="doc-biblioentry">
ATLAS Collaboration. (2012). <span class="nocase">Combined search for the Standard Model Higgs boson in <span class="math inline">\(pp\)</span> collisions at <span class="math inline">\(\sqrt{s}\)</span> = 7 TeV with the ATLAS detector.</span> <em>Physical Review D</em>, <em><span>86</span></em>, 032003. <a href="https://arxiv.org/abs/1207.0319">https://arxiv.org/abs/1207.0319</a>
</div>
<div id="ref-ATLAS_2011_The_CLs_method_Information_for_conference" class="csl-entry" role="doc-biblioentry">
ATLAS Statistics Forum. (2011). <span class="nocase">The CLs method: Information for conference speakers</span>. <a href="http://www.pp.rhul.ac.uk/~cowan/stat/cls/CLsInfo.pdf">http://www.pp.rhul.ac.uk/~cowan/stat/cls/CLsInfo.pdf</a>
</div>
<div id="ref-Aytekin_2022_Neural_networks_are_decision_trees" class="csl-entry" role="doc-biblioentry">
Aytekin, C. (2022). <span class="nocase">Neural networks are decision trees</span>. <a href="https://arxiv.org/abs/2210.05189">https://arxiv.org/abs/2210.05189</a>
</div>
<div id="ref-Bach_2022_Learning_Theory_from_First_Principles" class="csl-entry" role="doc-biblioentry">
Bach, F. (2022). <em><span class="nocase">Learning Theory from First Principles</span></em>. (Draft). <a href="https://www.di.ens.fr/~fbach/ltfp_book.pdf">https://www.di.ens.fr/~fbach/ltfp_book.pdf</a>
</div>
<div id="ref-Bahdanau_2015_Neural_machine_translation_by_jointly_learning" class="csl-entry" role="doc-biblioentry">
Bahdanau, D., Cho, K., &amp; Bengio, Y. (2015). <span class="nocase">Neural machine translation by jointly learning to align and translate.</span> <em>International Conference on Learning Representations, 3rd</em>, <em><span>2015</span></em>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-Bahri_2020_Statistical_mechanics_of_deep_learning" class="csl-entry" role="doc-biblioentry">
Bahri, Y. et al. (2020). <span class="nocase">Statistical mechanics of deep learning.</span> <em>Annual Review of Condensed Matter Physics</em>, <em><span>11</span></em>, 501&#x2013;528.
</div>
<div id="ref-Balasubramanian_1996_A_geometric_formulation_of_Occams_razor" class="csl-entry" role="doc-biblioentry">
Balasubramanian, V. (1996a). <span class="nocase">A geometric formulation of Occam&#x2019;s razor for inference of parametric distributions</span>. <a href="https://arxiv.org/abs/adap-org/9601001">https://arxiv.org/abs/adap-org/9601001</a>
</div>
<div id="ref-Balasubramanian_1996_Statistical_inference_Occams_razor" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1996b). <span class="nocase">Statistical inference, Occam&#x2019;s razor and statistical mechanics on the space of probability distributions</span>. <a href="https://arxiv.org/abs/cond-mat/9601030">https://arxiv.org/abs/cond-mat/9601030</a>
</div>
<div id="ref-Balestriero_2021_Learning_in_high_dimension_always_amounts" class="csl-entry" role="doc-biblioentry">
Balestriero, R., Pesenti, J., &amp; LeCun, Y. (2021). <span class="nocase">Learning in high dimension always amounts to extrapolation</span>. <a href="https://arxiv.org/abs/2110.09485">https://arxiv.org/abs/2110.09485</a>
</div>
<div id="ref-Batson_2021_Topological_obstructions_to_autoencoding" class="csl-entry" role="doc-biblioentry">
Batson, J., Haaf, C. G., Kahn, Y., &amp; Roberts, D. A. (2021). <span class="nocase">Topological obstructions to autoencoding</span>. <a href="https://arxiv.org/abs/2102.08380">https://arxiv.org/abs/2102.08380</a>
</div>
<div id="ref-Baydin_2019_Etalumis_Bringing_probabilistic_programming" class="csl-entry" role="doc-biblioentry">
Baydin, A.G. et al. (2019). <span class="nocase">Etalumis: Bringing probabilistic programming to scientific simulators at scale</span>. <a href="https://arxiv.org/abs/1907.03382">https://arxiv.org/abs/1907.03382</a>
</div>
<div id="ref-Behnke_2013_Data_Analysis_in_High_Energy_Physics_A_Practical" class="csl-entry" role="doc-biblioentry">
Behnke, O., Kr&#xF6;ninger, K., Schott, G., &amp; Sch&#xF6;rner-Sadenius, T. (2013). <em><span class="nocase">Data Analysis in High Energy Physics: A Practical Guide to Statistical Methods</span></em>. <span>Wiley</span>.
</div>
<div id="ref-Belinkov_2022_Probing_classifiers_Promises_shortcomings" class="csl-entry" role="doc-biblioentry">
Belinkov, Y. (2022). <span class="nocase">Probing classifiers: Promises, shortcomings, and advances.</span> <em>Computational Linguistics</em>, <em><span>48</span></em>, 207&#x2013;219.
</div>
<div id="ref-Belkin_2021_Fit_without_fear_remarkable_mathematical" class="csl-entry" role="doc-biblioentry">
Belkin, M. (2021). <span class="nocase">Fit without fear: remarkable mathematical phenomena of deep learning through the prism of interpolation</span>. <a href="https://arxiv.org/abs/2105.14368">https://arxiv.org/abs/2105.14368</a>
</div>
<div id="ref-Belkin_2019_Reconciling_modern_machine_learning_practice" class="csl-entry" role="doc-biblioentry">
Belkin, M., Hsu, D., Ma, S., &amp; Mandal, S. (2019). <span class="nocase">Reconciling modern machine-learning practice and the classical bias-variance trade-off.</span> <em>Proceedings of the National Academy of Sciences</em>, <em><span>116</span></em>, 15849&#x2013;15854. <a href="https://arxiv.org/abs/1812.11118">https://arxiv.org/abs/1812.11118</a>
</div>
<div id="ref-Bellman_1952_On_the_theory_of_dynamic_programming" class="csl-entry" role="doc-biblioentry">
Bellman, R. (1952). <span class="nocase">On the theory of dynamic programming.</span> <em>Proceedings of the National Academy of Sciences</em>, <em><span>38</span></em>, 716&#x2013;719.
</div>
<div id="ref-Bengio_2009_Learning_deep_architectures_for_AI" class="csl-entry" role="doc-biblioentry">
Bengio, Y. (2009). <span class="nocase">Learning deep architectures for AI.</span> <em>Foundations and Trends in Machine Learning</em>, <em><span>2</span></em>, 1&#x2013;127. <a href="https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf">https://www.iro.umontreal.ca/~lisa/pointeurs/TR1312.pdf</a>
</div>
<div id="ref-Benjamin_2017_Redefine_statistical_significance" class="csl-entry" role="doc-biblioentry">
Benjamin, D.J. et al. (2017). <span class="nocase">Redefine statistical significance.</span> <em>PsyArXiv</em>. July 22, 2017. <a href="https://psyarxiv.com/mky9j/">https://psyarxiv.com/mky9j/</a>
</div>
<div id="ref-Benjamini_2021_The_ASA_presidents_task_force_statement_on" class="csl-entry" role="doc-biblioentry">
Benjamini, Y. et al. (2021). <span class="nocase">The ASA president&#x2019;s task force statement on statistical significance and replicability.</span> <em>Annals of Applied Statistics</em>, <em><span>16</span></em>, 1&#x2013;2. <a href="https://magazine.amstat.org/blog/2021/08/01/task-force-statement-p-value/">https://magazine.amstat.org/blog/2021/08/01/task-force-statement-p-value/</a>
</div>
<div id="ref-Bensusan_2000_Is_machine_learning_experimental_philosophy" class="csl-entry" role="doc-biblioentry">
Bensusan, H. (2000). <span class="nocase">Is machine learning experimental philosophy of science?</span> In <em><span class="nocase">ECAI2000 Workshop notes on scientific Reasoning in Artificial Intelligence and the Philosophy of Science</span></em> (pp. 9&#x2013;14).
</div>
<div id="ref-Berger_2003_Could_Fisher_Jeffreys_and_Neyman_have_agreed_on" class="csl-entry" role="doc-biblioentry">
Berger, J. O. (2003). <span class="nocase">Could Fisher, Jeffreys and Neyman have agreed on testing?</span> <em>Statistical Science</em>, <em><span>18</span></em>, 1&#x2013;32.
</div>
<div id="ref-Berger_1988_The_Likelihood_Principle" class="csl-entry" role="doc-biblioentry">
Berger, J. O. &amp; Wolpert, R. L. (1988). <em><span>The Likelihood Principle</span></em> (2nd ed.). Haywood, CA: <span>The Institute of Mathematical Statistics</span>.
</div>
<div id="ref-Bertsch_2023_Unlimiformer_Long_range_transformers" class="csl-entry" role="doc-biblioentry">
Bertsch, A., Alon, U., Neubig, G., &amp; Gormley, M. R. (2023). <span class="nocase">Unlimiformer: Long-range transformers with unlimited length input</span>. <a href="https://arxiv.org/abs/2305.01625">https://arxiv.org/abs/2305.01625</a>
</div>
<div id="ref-Bhargava_2023_Whats_the_magic_word" class="csl-entry" role="doc-biblioentry">
Bhargava, A., Witkowski, C., Shah, M., &amp; Thomson, M. (2023). <span class="nocase">What&#x2019;s the magic word?</span> <span>A control theory of LLM prompting</span>. <a href="https://arxiv.org/abs/2310.04444">https://arxiv.org/abs/2310.04444</a>
</div>
<div id="ref-Bhattiprolu_2020_Criteria_for_projected_discovery_and_exclusion" class="csl-entry" role="doc-biblioentry">
Bhattiprolu, P. N., Martin, S. P., &amp; Wells, J. D. (2020). <span class="nocase">Criteria for projected discovery and exclusion sensitivities of counting experiments</span>. <a href="https://arxiv.org/abs/2009.07249">https://arxiv.org/abs/2009.07249</a>
</div>
<div id="ref-Billings_2002_The_challenge_of_poker" class="csl-entry" role="doc-biblioentry">
Billings, D., Davidson, A., Schaeffer, J., &amp; Szafron, D. (2002). <span class="nocase">The challenge of poker.</span> <em>Artificial Intelligence</em>, <em><span>134</span></em>, 201&#x2013;240. <a href="https://doi.org/10.1016/S0004-3702(01)00130-8">https://doi.org/10.1016/S0004-3702(01)00130-8</a>
</div>
<div id="ref-Billings_2003_Approximating_game_theoretic_optimal_strategies" class="csl-entry" role="doc-biblioentry">
Billings, D. et al. (2003). <span class="nocase">Approximating game-theoretic optimal strategies for full-scale poker.</span> <em>IJCAI</em>, <em><span>3</span></em>, 661. <a href="http://webdocs.cs.ualberta.ca/~duane/publications/pdf/2003ijcai.pdf">http://webdocs.cs.ualberta.ca/~duane/publications/pdf/2003ijcai.pdf</a>
</div>
<div id="ref-Birnbaum_1962_On_the_foundations_of_statistical_inference" class="csl-entry" role="doc-biblioentry">
Birnbaum, A. (1962). <span class="nocase">On the foundations of statistical inference.</span> <em>Journal of the American Statistical Association</em>, <em><span>57</span></em>, 269&#x2013;326.
</div>
<div id="ref-Bishop_2006_Pattern_Recognition_and_Machine_Learning" class="csl-entry" role="doc-biblioentry">
Bishop, C. M. (2006). <em><span class="nocase">Pattern Recognition and Machine Learning</span></em>. <span>Springer</span>.
</div>
<div id="ref-Blondel_2020_Learning_with_Fenchel_Young_losses" class="csl-entry" role="doc-biblioentry">
Blondel, M., Martins, A. F., &amp; Niculae, V. (2020). <span class="nocase">Learning with Fenchel-Young losses.</span> <em>Journal of Machine Learning Research</em>, <em><span>21</span></em>, 1&#x2013;69.
</div>
<div id="ref-Bogatskiy_2023_Explainable_equivariant_neural_networks" class="csl-entry" role="doc-biblioentry">
Bogatskiy, A. et al. (2023). <span class="nocase">Explainable equivariant neural networks for particle physics: PELICAN</span>. <a href="https://arxiv.org/abs/2307.16506">https://arxiv.org/abs/2307.16506</a>
</div>
<div id="ref-Bottou_1998_Stochastic_gradient_descent_tricks" class="csl-entry" role="doc-biblioentry">
Bottou, L. (1998). <span class="nocase">Stochastic gradient descent tricks</span>. In G. B. Orr &amp; K. R. Muller (Eds.), <em><span class="nocase">Neural Networks: Tricks of the trade</span></em>. <span>Springer</span>. <a href="https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/">https://www.microsoft.com/en-us/research/publication/stochastic-gradient-tricks/</a>
</div>
<div id="ref-Bousquet_2021_A_theory_of_universal_learning" class="csl-entry" role="doc-biblioentry">
Bousquet, O. et al. (2021). <span class="nocase">A theory of universal learning</span>. In <em><span class="nocase">Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing</span></em> (pp. 532&#x2013;541). <a href="https://dl.acm.org/doi/pdf/10.1145/3406325.3451087">https://dl.acm.org/doi/pdf/10.1145/3406325.3451087</a>
</div>
<div id="ref-Bowling_2015_Heads_up_limit_holdem_poker_is_solved" class="csl-entry" role="doc-biblioentry">
Bowling, M., Burch, N., Johanson, M., &amp; Tammelin, O. (2015). <span class="nocase">Heads-up limit hold&#x2019;em poker is solved.</span> <em>Science</em>, <em><span>347</span></em>, 145&#x2013;149. <a href="http://science.sciencemag.org/content/347/6218/145">http://science.sciencemag.org/content/347/6218/145</a>
</div>
<div id="ref-Bronstein_2021_Geometric_deep_learning_Grids_groups_graphs" class="csl-entry" role="doc-biblioentry">
Bronstein, M. M., Bruna, J., Cohen, T., &amp; Velickovic, P. (2021). <span class="nocase">Geometric deep learning: Grids, groups, graphs, geodesics, and gauges</span>. <a href="https://arxiv.org/abs/2104.13478">https://arxiv.org/abs/2104.13478</a>
</div>
<div id="ref-Brown_2001_Interval_estimation_for_a_binomial_proportion" class="csl-entry" role="doc-biblioentry">
Brown, L. D., Cai, T. T., &amp; DasGupta, A. (2001). <span class="nocase">Interval estimation for a binomial proportion.</span> <em>Statistical Science</em>, <em><span>16</span></em>, 101&#x2013;133. <a href="https://projecteuclid.org/euclid.ss/1009213286">https://projecteuclid.org/euclid.ss/1009213286</a>
</div>
<div id="ref-Brown_2020_Equilibrium_finding_for_large_adversarial" class="csl-entry" role="doc-biblioentry">
Brown, N. (2020). <em><span class="nocase">Equilibrium finding for large adversarial imperfect-information games</span></em>. (Ph.D. thesis). <a href="http://www.cs.cmu.edu/~noamb/thesis.pdf">http://www.cs.cmu.edu/~noamb/thesis.pdf</a>
</div>
<div id="ref-Brown_2020_Combining_deep_reinforcement_learning_and_search" class="csl-entry" role="doc-biblioentry">
Brown, N., Bakhtin, A., Lerer, A., &amp; Gong, Q. (2020). <span class="nocase">Combining deep reinforcement learning and search for imperfect-information games</span>. <a href="https://arxiv.org/abs/2007.13544">https://arxiv.org/abs/2007.13544</a>
</div>
<div id="ref-Brown_2019_Deep_counterfactual_regret_minimization" class="csl-entry" role="doc-biblioentry">
Brown, N., Lerer, A., Gross, S., &amp; Sandholm, T. (2019). <span class="nocase">Deep counterfactual regret minimization</span>. <a href="https://arxiv.org/abs/1811.00164">https://arxiv.org/abs/1811.00164</a>
</div>
<div id="ref-Brown_2018_Superhuman_AI_for_heads_up_no_limit_poker" class="csl-entry" role="doc-biblioentry">
Brown, N. &amp; Sandholm, T. (2018). <span class="nocase">Superhuman AI for heads-up no-limit poker: Libratus beats top professionals.</span> <em>Science</em>, <em><span>359</span></em>, 418&#x2013;424. <a href="https://science.sciencemag.org/content/359/6374/418">https://science.sciencemag.org/content/359/6374/418</a>
</div>
<div id="ref-Brown_2019_Solving_imperfect_information_games_via_discounted" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2019a). <span class="nocase">Solving imperfect-information games via discounted regret minimization.</span> <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, <em><span>33</span></em>, 1829&#x2013;1836. <a href="https://arxiv.org/abs/1809.04040">https://arxiv.org/abs/1809.04040</a>
</div>
<div id="ref-Brown_2019_Superhuman_AI_for_multiplayer_poker" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2019b). <span class="nocase">Superhuman AI for multiplayer poker.</span> <em>Science</em>, <em><span>365</span></em>, 885&#x2013;890. <a href="https://science.sciencemag.org/content/365/6456/885">https://science.sciencemag.org/content/365/6456/885</a>
</div>
<div id="ref-Brown_2020_Language_models_are_few_shot_learners" class="csl-entry" role="doc-biblioentry">
Brown, T.B. et al. (2020). <span class="nocase">Language models are few-shot learners</span>. (Paper on the GPT-3 model by OpenAI). <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>
</div>
<div id="ref-Bubeck_2023_A_universal_law_of_robustness_via_isoperimetry" class="csl-entry" role="doc-biblioentry">
Bubeck, S. &amp; Sellke, M. (2023). <span class="nocase">A universal law of robustness via isoperimetry.</span> <em>Journal of the ACM</em>, <em><span>70</span></em>, 1&#x2013;18. <a href="https://dl.acm.org/doi/full/10.1145/3578580">https://dl.acm.org/doi/full/10.1145/3578580</a>
</div>
<div id="ref-Bulatov_2022_Recurrent_memory_transformer" class="csl-entry" role="doc-biblioentry">
Bulatov, A., Kuratov, Y., &amp; Burtsev, M. S. (2022). <span class="nocase">Recurrent memory transformer</span>. <a href="https://arxiv.org/abs/2207.06881">https://arxiv.org/abs/2207.06881</a>
</div>
<div id="ref-Bulatov_2023_Scaling_transformer_to_1M_tokens_and_beyond" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2023). <span class="nocase">Scaling transformer to 1M tokens and beyond with RMT</span>. <a href="https://arxiv.org/abs/2304.11062">https://arxiv.org/abs/2304.11062</a>
</div>
<div id="ref-Burch_2018_Time_and_Space_Why_imperfect_information_games" class="csl-entry" role="doc-biblioentry">
Burch, N. (2018). <em><span class="nocase">Time and Space: Why imperfect information games are hard</span></em>. <span>University of Alberta</span>. (Ph.D. thesis). <a href="https://era.library.ualberta.ca/items/db44409f-b373-427d-be83-cace67d33c41/view/bcb00dca-39e6-4c43-9ec2-65026a50135e/Burch_Neil_E_201712_PhD.pdf">https://era.library.ualberta.ca/items/db44409f-b373-427d-be83-cace67d33c41/view/bcb00dca-39e6-4c43-9ec2-65026a50135e/Burch_Neil_E_201712_PhD.pdf</a>
</div>
<div id="ref-Burch_2012_Efficient_Monte_Carlo_counterfactual_regret" class="csl-entry" role="doc-biblioentry">
Burch, N., Lanctot, M., Szafron, D., &amp; Gibson, R. (2012). <span class="nocase">Efficient Monte Carlo counterfactual regret minimization in games with many player actions.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>25</span></em>. <a href="https://proceedings.neurips.cc/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf">https://proceedings.neurips.cc/paper/2012/file/3df1d4b96d8976ff5986393e8767f5b2-Paper.pdf</a>
</div>
<div id="ref-Burch_2019_Revisiting_CFR_and_alternating_updates" class="csl-entry" role="doc-biblioentry">
Burch, N., Moravcik, M., &amp; Schmid, M. (2019). <span class="nocase">Revisiting CFR+ and alternating updates.</span> <em>Journal of Artificial Intelligence Research</em>, <em><span>64</span></em>, 429&#x2013;443. <a href="https://www.jair.org/index.php/jair/article/view/11370">https://www.jair.org/index.php/jair/article/view/11370</a>
</div>
<div id="ref-Caldeira_2020_Deeply_uncertain_comparing_methods_of_uncertainty" class="csl-entry" role="doc-biblioentry">
Caldeira, J. &amp; Nord, B. (2020). <span class="nocase">Deeply uncertain: comparing methods of uncertainty quantification in deep learning algorithms.</span> <em>Machine Learning: Science and Technology</em>, <em><span>2</span></em>, 015002. <a href="https://iopscience.iop.org/article/10.1088/2632-2153/aba6f3">https://iopscience.iop.org/article/10.1088/2632-2153/aba6f3</a>
</div>
<div id="ref-Calin_2014_Geometric_Modeling_in_Probability_and_Statistics" class="csl-entry" role="doc-biblioentry">
Calin, O. &amp; Udriste, C. (2014). <em><span class="nocase">Geometric Modeling in Probability and Statistics</span></em>. <span>Springer Switzerland</span>.
</div>
<div id="ref-Canatar_2020_Spectral_bias_and_task_model_alignment_explain" class="csl-entry" role="doc-biblioentry">
Canatar, A., Bordelon, B., &amp; Pehlevan, C. (2020). <span class="nocase">Spectral bias and task-model alignment explain generalization in kernel regression and infinitely wide neural networks</span>. <a href="https://arxiv.org/abs/2006.13198">https://arxiv.org/abs/2006.13198</a>
</div>
<div id="ref-Cantwell_2022_Approximate_sampling_and_estimation_of_partition" class="csl-entry" role="doc-biblioentry">
Cantwell, G. T. (2022). <span class="nocase">Approximate sampling and estimation of partition functions using neural networks</span>. <a href="https://arxiv.org/abs/2209.10423">https://arxiv.org/abs/2209.10423</a>
</div>
<div id="ref-Carnap_1945_The_two_concepts_of_probability" class="csl-entry" role="doc-biblioentry">
Carnap, R. (1945). <span class="nocase">The two concepts of probability.</span> <em>Philosophy and Phenomenological Research</em>, <em><span>5</span></em>, 513&#x2013;32.
</div>
<div id="ref-Carnap_1947_Probability_as_a_guide_in_life" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1947). <span class="nocase">Probability as a guide in life.</span> <em>Journal of Philosophy</em>, <em><span>44</span></em>, 141&#x2013;48.
</div>
<div id="ref-Carnap_1952_The_Continuum_of_Inductive_Methods" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1952). <em><span class="nocase">The Continuum of Inductive Methods</span></em>. <span>University of Chicago Press</span>. <a href="https://www.phil.cmu.edu/projects/carnap/editorial/latex_pdf/1952-1.pdf">https://www.phil.cmu.edu/projects/carnap/editorial/latex_pdf/1952-1.pdf</a>
</div>
<div id="ref-Carnap_1953_What_is_probability" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1953). <span class="nocase">What is probability?</span> <em>Scientific American</em>, <em><span>189</span></em>, 128&#x2013;139. <a href="https://www.jstor.org/stable/24944342">https://www.jstor.org/stable/24944342</a>
</div>
<div id="ref-Casadei_2012_Estimating_the_selection_efficiency" class="csl-entry" role="doc-biblioentry">
Casadei, D. (2012). <span class="nocase">Estimating the selection efficiency.</span> <em>Journal of Instrumentation</em>, <em><span>7</span></em>, 08021. <a href="https://arxiv.org/abs/0908.0130">https://arxiv.org/abs/0908.0130</a>
</div>
<div id="ref-Cesa_Bianchi_2006_Prediction_Learning_and_Games" class="csl-entry" role="doc-biblioentry">
Cesa-Bianchi, N. &amp; Lugosi, G. (2006). <em><span class="nocase">Prediction, Learning, and Games</span></em>. <span>Cambridge University Press</span>. <a href="https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf">https://ii.uni.wroc.pl/~lukstafi/pmwiki/uploads/AGT/Prediction_Learning_and_Games.pdf</a>
</div>
<div id="ref-Chang_2020_Provable_benefits_of_overparameterization_in_model" class="csl-entry" role="doc-biblioentry">
Chang, X., Li, Y., Oymak, S., &amp; Thrampoulidis, C. (2020). <span class="nocase">Provable benefits of overparameterization in model compression: From double descent to pruning neural networks</span>. <a href="https://arxiv.org/abs/2012.08749">https://arxiv.org/abs/2012.08749</a>
</div>
<div id="ref-Chen_2018_Neural_ordinary_differential_equations" class="csl-entry" role="doc-biblioentry">
Chen, R. T. Q., Rubanova, Y., Bettencourt, J., &amp; Duvenaud, D. (2018). <span class="nocase">Neural ordinary differential equations</span>. <a href="https://arxiv.org/abs/1806.07366">https://arxiv.org/abs/1806.07366</a>
</div>
<div id="ref-Chen_2020_A_group_theoretic_framework_for_data_augmentation" class="csl-entry" role="doc-biblioentry">
Chen, S., Dobriban, E., &amp; Lee, J. H. (2020). <span class="nocase">A group-theoretic framework for data augmentation</span>. <a href="https://arxiv.org/abs/1907.10905">https://arxiv.org/abs/1907.10905</a>
</div>
<div id="ref-Chen_2016_Xgboost_A_scalable_tree_boosting_system" class="csl-entry" role="doc-biblioentry">
Chen, T. &amp; Guestrin, C. (2016). <span class="nocase">Xgboost: A scalable tree boosting system</span>. <a href="https://arxiv.org/abs/1603.02754">https://arxiv.org/abs/1603.02754</a>
</div>
<div id="ref-Chen_2018_Open_is_not_enough" class="csl-entry" role="doc-biblioentry">
Chen, X. et al. (2018). <span class="nocase">Open is not enough.</span> <em>Nature Physics</em>, <em><span>15</span></em>, 113&#x2013;119. <a href="https://www.nature.com/articles/s41567-018-0342-2">https://www.nature.com/articles/s41567-018-0342-2</a>
</div>
<div id="ref-Chiley_2019_Online_normalization_for_training_neural_networks" class="csl-entry" role="doc-biblioentry">
Chiley, V. et al. (2019). <span class="nocase">Online normalization for training neural networks.</span> <em>NeurIPS 2019</em>. <a href="https://arxiv.org/abs/1905.05894">https://arxiv.org/abs/1905.05894</a>
</div>
<div id="ref-Chowdhery_2022_PaLM_Scaling_language_modeling_with_pathways" class="csl-entry" role="doc-biblioentry">
Chowdhery, A. et al. (2022). <span class="nocase">PaLM: Scaling language modeling with pathways</span>. <a href="https://arxiv.org/abs/2204.02311">https://arxiv.org/abs/2204.02311</a>
</div>
<div id="ref-Church_2019_A_survey_of_25_years_of_evaluation" class="csl-entry" role="doc-biblioentry">
Church, K. W. &amp; Hestness, J. (2019). <span class="nocase">A survey of 25 years of evaluation.</span> <em>Natural Language Engineering</em>, <em><span>25</span></em>, 753&#x2013;767. <a href="https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-25-years-of-evaluation/E4330FAEB9202EC490218E3220DDA291">https://www.cambridge.org/core/journals/natural-language-engineering/article/survey-of-25-years-of-evaluation/E4330FAEB9202EC490218E3220DDA291</a>
</div>
<div id="ref-Cilibrasi_2005_Clustering_by_compression" class="csl-entry" role="doc-biblioentry">
Cilibrasi, R. &amp; Vitanyi, P. M. B. (2005). <span class="nocase">Clustering by compression.</span> <em>IEEE Transactions on Information Theory</em>, <em><span>51</span></em>, 1523&#x2013;1545.
</div>
<div id="ref-Ciresan_2012_Multi_column_deep_neural_network_for_traffic_sign" class="csl-entry" role="doc-biblioentry">
Ciresan, D., Meier, U., Masci, J., &amp; Schmidhuber, J. (2012). <span class="nocase">Multi-column deep neural network for traffic sign classification.</span> <em>Neural Networks</em>, <em><span>32</span></em>, 333&#x2013;338. <a href="https://arxiv.org/abs/1202.2745">https://arxiv.org/abs/1202.2745</a>
</div>
<div id="ref-Clopper_1934_The_use_of_confidence_or_fiducial_limits" class="csl-entry" role="doc-biblioentry">
Clopper, C. J. &amp; Pearson, E. S. (1934). <span class="nocase">The use of confidence or fiducial limits illustrated in the case of the binomial.</span> <em>Biometrika</em>, <em><span>26</span></em>, 404&#x2013;413.
</div>
<div id="ref-Coadou_2022_Boosted_decision_trees" class="csl-entry" role="doc-biblioentry">
Coadou, Y. (2022). <span class="nocase">Boosted decision trees</span>. <a href="https://arxiv.org/abs/2206.09645">https://arxiv.org/abs/2206.09645</a>
</div>
<div id="ref-Cohen_2019_Gauge_equivariant_convolutional_networks" class="csl-entry" role="doc-biblioentry">
Cohen, T. S., Weiler, M., Kicanaoglu, B., &amp; Welling, M. (2019). <span class="nocase">Gauge equivariant convolutional networks and the icosahedral CNN</span>. <a href="https://arxiv.org/abs/1902.04615">https://arxiv.org/abs/1902.04615</a>
</div>
<div id="ref-Cohen_2016_Group_equivariant_convolutional_networks" class="csl-entry" role="doc-biblioentry">
Cohen, T. S. &amp; Welling, M. (2016). <span class="nocase">Group equivariant convolutional networks.</span> <em>Proceedings of International Conference on Machine Learning</em>, <em><span>2016</span></em>, 2990&#x2013;9. <a href="http://proceedings.mlr.press/v48/cohenc16.pdf">http://proceedings.mlr.press/v48/cohenc16.pdf</a>
</div>
<div id="ref-Collobert_2019_A_fully_differentiable_beam_search_decoder" class="csl-entry" role="doc-biblioentry">
Collobert, R., Hannun, A., &amp; Synnaeve, G. (2019). <span class="nocase">A fully differentiable beam search decoder.</span> <em>International Conference on Machine Learning</em>, <em><span>2019</span></em>, 1341&#x2013;1350. <a href="http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf">http://proceedings.mlr.press/v97/collobert19a/collobert19a.pdf</a>
</div>
<div id="ref-Cousins_2018_Lectures_on_statistics_in_theory_Prelude" class="csl-entry" role="doc-biblioentry">
Cousins, R. D. (2018). <span class="nocase">Lectures on statistics in theory: Prelude to statistics in practice</span>. <a href="https://arxiv.org/abs/1807.05996">https://arxiv.org/abs/1807.05996</a>
</div>
<div id="ref-Cousins_1992_Incorporating_systematic_uncertainties_into" class="csl-entry" role="doc-biblioentry">
Cousins, R. D. &amp; Highland, V. L. (1992). <span class="nocase">Incorporating systematic uncertainties into an upper limit.</span> <em>Nuclear Instruments and Methods in Physics Research Section A</em>, <em><span>320</span></em>, 331&#x2013;335. <a href="http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.193.1581&amp;rep=rep1&amp;type=pdf">http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.193.1581&amp;rep=rep1&amp;type=pdf</a>
</div>
<div id="ref-Cowan_1998_Statistical_Data_Analysis" class="csl-entry" role="doc-biblioentry">
Cowan, G. (1998). <em><span>Statistical Data Analysis</span></em>. <span>Clarendon Press</span>.
</div>
<div id="ref-Cowan_2012_Discovery_sensitivity_for_a_counting_experiment" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2012). <span class="nocase">Discovery sensitivity for a counting experiment with background uncertainty</span>. <a href="https://www.pp.rhul.ac.uk/~cowan/stat/notes/medsigNote.pdf">https://www.pp.rhul.ac.uk/~cowan/stat/notes/medsigNote.pdf</a>
</div>
<div id="ref-Cowan_2016_StatisticsIn_CPatrignani_et_alParticle_Data" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2016). <span class="nocase">Statistics. In C. Patrignani et al. (Particle Data Group),</span>. <em>Chinese Physics C</em>, <em><span>40</span></em>, 100001. <a href="http://pdg.lbl.gov/2016/reviews/rpp2016-rev-statistics.pdf">http://pdg.lbl.gov/2016/reviews/rpp2016-rev-statistics.pdf</a>
</div>
<div id="ref-Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests" class="csl-entry" role="doc-biblioentry">
Cowan, G., Cranmer, K., Gross, E., &amp; Vitells, O. (2011). <span class="nocase">Asymptotic formulae for likelihood-based tests of new physics.</span> <em>European Physical Journal C</em>, <em><span>71</span></em>, 1544. <a href="https://arxiv.org/abs/1007.1727">https://arxiv.org/abs/1007.1727</a>
</div>
<div id="ref-Cowan_2012_Asymptotic_distribution_for_two_sided_tests" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2012). <span class="nocase">Asymptotic distribution for two-sided tests with lower and upper boundaries on the parameter of interest</span>. <a href="https://arxiv.org/abs/1210.6948">https://arxiv.org/abs/1210.6948</a>
</div>
<div id="ref-Cox_2006_Principles_of_Statistical_Inference" class="csl-entry" role="doc-biblioentry">
Cox, D. R. (2006). <em><span class="nocase">Principles of Statistical Inference</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Cramer_1946_A_contribution_to_the_theory_of_statistical" class="csl-entry" role="doc-biblioentry">
Cram&#xE9;r, H. (1946). <span class="nocase">A contribution to the theory of statistical estimation.</span> <em>Skandinavisk Aktuarietidskrift</em>, <em><span>29</span></em>, 85&#x2013;94.
</div>
<div id="ref-Cranmer_2015_Practical_statistics_for_the_LHC" class="csl-entry" role="doc-biblioentry">
Cranmer, K. (2015). <span class="nocase">Practical statistics for the LHC</span>. <a href="https://arxiv.org/abs/1503.07622">https://arxiv.org/abs/1503.07622</a>
</div>
<div id="ref-Cranmer_2019_The_frontier_of_simulation_based_inference" class="csl-entry" role="doc-biblioentry">
Cranmer, K., Brehmer, J., &amp; Louppe, G. (2019). <span class="nocase">The frontier of simulation-based inference</span>. <a href="https://arxiv.org/abs/1911.01429">https://arxiv.org/abs/1911.01429</a>
</div>
<div id="ref-Cranmer_2012_HistFactory_A_tool_for_creating_statistical" class="csl-entry" role="doc-biblioentry">
Cranmer, K. et al. (2012). <span class="nocase">HistFactory: A tool for creating statistical models for use with RooFit and RooStats</span>. Technical Report: CERN-OPEN-2012-016. <a href="http://inspirehep.net/record/1236448/">http://inspirehep.net/record/1236448/</a>
</div>
<div id="ref-Cranmer_2015_Approximating_likelihood_ratios_with_calibrated" class="csl-entry" role="doc-biblioentry">
Cranmer, K., Pavez, J., &amp; Louppe, G. (2015). <span class="nocase">Approximating likelihood ratios with calibrated discriminative classifiers</span>. <a href="https://arxiv.org/abs/1506.02169">https://arxiv.org/abs/1506.02169</a>
</div>
<div id="ref-Cranmer_2021_Machine_learning" class="csl-entry" role="doc-biblioentry">
Cranmer, K., Seljak, U., &amp; Terao, K. (2021). <span class="nocase">Machine learning</span>. In P. A. Z. et al. (Ed.), <em><span class="nocase">Progress of Theoretical and Experimental Physics</span></em>. <span>2020, 083C01</span>. (and 2021 update). <a href="https://pdg.lbl.gov/2021-rev/2021/reviews/contents_sports.html">https://pdg.lbl.gov/2021-rev/2021/reviews/contents_sports.html</a>
</div>
<div id="ref-Cranmer_2020_Discovering_symbolic_models_from_deep_learning" class="csl-entry" role="doc-biblioentry">
Cranmer, M. et al. (2020). <span class="nocase">Discovering symbolic models from deep learning with inductive biases</span>. <a href="https://arxiv.org/abs/2006.11287">https://arxiv.org/abs/2006.11287</a>
</div>
<div id="ref-DAgnolo_2019_Learning_New_Physics_from_a_Machine" class="csl-entry" role="doc-biblioentry">
D&#x2019;Agnolo, R. T. &amp; Wulzer, A. (2019). <span class="nocase">Learning New Physics from a Machine.</span> <em>Physical Review D</em>, <em><span>99</span></em>, 015014. <a href="https://arxiv.org/abs/1806.02350">https://arxiv.org/abs/1806.02350</a>
</div>
<div id="ref-Dao_2022_FlashAttention_Fast_and_memory_efficient_exact" class="csl-entry" role="doc-biblioentry">
Dao, T. et al. (2022). <span class="nocase">FlashAttention: Fast and memory-efficient exact attention with IO-awareness</span>. <a href="https://arxiv.org/abs/2205.14135">https://arxiv.org/abs/2205.14135</a>
</div>
<div id="ref-Dar_2021_A_farewell_to_the_bias_variance_tradeoff" class="csl-entry" role="doc-biblioentry">
Dar, Y., Muthukumar, V., &amp; Baraniuk, R. G. (2021). <span class="nocase">A farewell to the bias-variance tradeoff?</span> <span>An overview of the theory of overparameterized machine learning</span>. <a href="https://arxiv.org/abs/2109.02355">https://arxiv.org/abs/2109.02355</a>
</div>
<div id="ref-Dawid_2014_Discussion_of_On_the_Birnbaum_Argument" class="csl-entry" role="doc-biblioentry">
Dawid, A. P. (2014). <span class="nocase">Discussion of "On the Birnbaum Argument for the Strong Likelihood Principle".</span> <em>Statistical Science</em>, <em><span>29</span></em>, 240&#x2013;241. <a href="https://projecteuclid.org/journals/statistical-science/volume-29/issue-2/Discussion-of-On-the-Birnbaum-Argument-for-the-Strong-Likelihood/10.1214/14-STS470.full">https://projecteuclid.org/journals/statistical-science/volume-29/issue-2/Discussion-of-On-the-Birnbaum-Argument-for-the-Strong-Likelihood/10.1214/14-STS470.full</a>
</div>
<div id="ref-deCarvalho_2019_On_the_geometry_of_Bayesian_inference" class="csl-entry" role="doc-biblioentry">
de Carvalho, M., Page, G. L., &amp; Barney, B. J. (2019). <span class="nocase">On the geometry of Bayesian inference.</span> <em>Bayesian Analysis</em>, <em><span>14</span></em>, 1013&#x2013;1036. <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-4/On-the-Geometry-of-Bayesian-Inference/10.1214/18-BA1112.full">https://projecteuclid.org/journals/bayesian-analysis/volume-14/issue-4/On-the-Geometry-of-Bayesian-Inference/10.1214/18-BA1112.full</a>
</div>
<div id="ref-Dennett_1991_Real_patterns" class="csl-entry" role="doc-biblioentry">
Dennett, D. C. (1991). <span class="nocase">Real patterns.</span> <em>The Journal of Philosophy</em>, <em><span>88</span></em>, 27&#x2013;51. <a href="https://web.ics.purdue.edu/~drkelly/DCDRealPatterns1991.pdf">https://web.ics.purdue.edu/~drkelly/DCDRealPatterns1991.pdf</a>
</div>
<div id="ref-Devlin_2018_BERT_Pre_training_of_deep_bidirectional" class="csl-entry" role="doc-biblioentry">
Devlin, J., Chang, M., Lee, K., &amp; Toutanova, K. (2018). <span class="nocase">BERT: Pre-training of deep bidirectional transformers for language understanding</span>. <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a>
</div>
<div id="ref-Dhariwal_2021_Diffusion_models_beat_GANs_on_image_synthesis" class="csl-entry" role="doc-biblioentry">
Dhariwal, P. &amp; Nichol, A. (2021). <span class="nocase">Diffusion models beat GANs on image synthesis</span>. <a href="https://arxiv.org/abs/2105.05233">https://arxiv.org/abs/2105.05233</a>
</div>
<div id="ref-Dinan_2023_Effective_theory_of_transformers_at_initialization" class="csl-entry" role="doc-biblioentry">
Dinan, E., Yaida, S., &amp; Zhang, S. (2023). <span class="nocase">Effective theory of transformers at initialization</span>. <a href="https://arxiv.org/abs/2304.02034">https://arxiv.org/abs/2304.02034</a>
</div>
<div id="ref-Dosovitskiy_2020_An_image_is_worth_16x16_words_Transformers" class="csl-entry" role="doc-biblioentry">
Dosovitskiy, A. et al. (2020). <span class="nocase">An image is worth 16x16 words: Transformers for image recognition at scale</span>. <a href="https://arxiv.org/abs/2010.11929">https://arxiv.org/abs/2010.11929</a>
</div>
<div id="ref-Edelman_2021_Inductive_biases_and_variable_creation_in_self" class="csl-entry" role="doc-biblioentry">
Edelman, B. L., Goel, S., Kakade, S., &amp; Zhang, C. (2021). <span class="nocase">Inductive biases and variable creation in self-attention mechanisms</span>. <a href="https://arxiv.org/abs/2110.10090">https://arxiv.org/abs/2110.10090</a>
</div>
<div id="ref-Edwards_1974_The_history_of_likelihood" class="csl-entry" role="doc-biblioentry">
Edwards, A. W. F. (1974). <span class="nocase">The history of likelihood.</span> <em>International Statistical Review</em>, <em><span>42</span></em>, 9&#x2013;15.
</div>
<div id="ref-Efron_2016_Computer_Age_Statistical_Inference_Algorithms" class="csl-entry" role="doc-biblioentry">
Efron, B. &amp; Hastie, T. (2016). <em><span class="nocase">Computer Age Statistical Inference: Algorithms, evidence, and data science</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Evans_2013_What_does_the_proof_of_Birnbaums_theorem_prove" class="csl-entry" role="doc-biblioentry">
Evans, M. (2013). <span class="nocase">What does the proof of Birnbaum&#x2019;s theorem prove?</span> <a href="https://arxiv.org/abs/1302.5468">https://arxiv.org/abs/1302.5468</a>
</div>
<div id="ref-Fang_2022_Is_out_of_distribution_detection_learnable" class="csl-entry" role="doc-biblioentry">
Fang, Z. et al. (2022). <span class="nocase">Is out-of-distribution detection learnable?</span> <em>NeurIPS 2022</em>. <a href="https://arxiv.org/abs/2210.14707">https://arxiv.org/abs/2210.14707</a>
</div>
<div id="ref-Fefferman_2016_Testing_the_manifold_hypothesis" class="csl-entry" role="doc-biblioentry">
Fefferman, C., Mitter, S., &amp; Narayanan, H. (2016). <span class="nocase">Testing the manifold hypothesis.</span> <em>Journal of the American Mathematical Society</em>, <em><span>29</span></em>, 983&#x2013;1049. <a href="https://www.ams.org/journals/jams/2016-29-04/S0894-0347-2016-00852-4/S0894-0347-2016-00852-4.pdf">https://www.ams.org/journals/jams/2016-29-04/S0894-0347-2016-00852-4/S0894-0347-2016-00852-4.pdf</a>
</div>
<div id="ref-Feldman_1998_A_unified_approach_to_the_classical_statistical" class="csl-entry" role="doc-biblioentry">
Feldman, G. J. &amp; Cousins, R. D. (1998). <span class="nocase">A unified approach to the classical statistical analysis of small signals.</span> <em>Physical Review D</em>, <em><span>57</span></em>, 3873. <a href="https://arxiv.org/abs/physics/9711021">https://arxiv.org/abs/physics/9711021</a>
</div>
<div id="ref-Fienberg_2006_When_did_Bayesian_inference_become_Bayesian" class="csl-entry" role="doc-biblioentry">
Fienberg, S. E. (2006). <span class="nocase">When did Bayesian inference become "Bayesian"?</span> <em>Bayesian Analysis</em>, <em><span>1</span></em>, 1&#x2013;40. <a href="https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-1/When-did-Bayesian-inference-become-Bayesian/10.1214/06-BA101.full">https://projecteuclid.org/journals/bayesian-analysis/volume-1/issue-1/When-did-Bayesian-inference-become-Bayesian/10.1214/06-BA101.full</a>
</div>
<div id="ref-Firth_1957_A_synopsis_of_linguistic_theory_1930_1955" class="csl-entry" role="doc-biblioentry">
Firth, J. R. (1957). <span class="nocase">A synopsis of linguistic theory, 1930-1955</span>. In <em><span class="nocase">Studies in Linguistic Analysis</span></em> (pp. 1&#x2013;31). Oxford: <span>Blackwell</span>.
</div>
<div id="ref-Fisher_1912_On_an_absolute_criterion_for_fitting_frequency" class="csl-entry" role="doc-biblioentry">
Fisher, R. A. (1912). <span class="nocase">On an absolute criterion for fitting frequency curves.</span> <em>Statistical Science</em>, <em><span>12</span></em>, 39&#x2013;41.
</div>
<div id="ref-Fisher_1915_Frequency_distribution_of_the_values" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1915). <span class="nocase">Frequency distribution of the values of the correlation coefficient in samples of indefinitely large population.</span> <em>Biometrika</em>, <em><span>10</span></em>, 507&#x2013;521.
</div>
<div id="ref-Fisher_1921_On_the_probable_error_of_a_coefficient" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1921). <span class="nocase">On the "probable error" of a coefficient of correlation deduced from a small sample.</span> <em>Metron</em>, <em><span>1</span></em>, 1&#x2013;32.
</div>
<div id="ref-Fisher_1935_The_Design_of_Experiments" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1935). <em><span class="nocase">The Design of Experiments</span></em>. <span>Hafner</span>.
</div>
<div id="ref-Fisher_1955_Statistical_methods_and_scientific_induction" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1955). <span class="nocase">Statistical methods and scientific induction.</span> <em>Journal of the Royal Statistical Society, Series B</em>, <em><span>17</span></em>, 69&#x2013;78.
</div>
<div id="ref-Frankle_2018_The_lottery_ticket_hypothesis_Finding_sparse" class="csl-entry" role="doc-biblioentry">
Frankle, J. &amp; Carbin, M. (2018). <span class="nocase">The lottery ticket hypothesis: Finding sparse, trainable neural networks</span>. <a href="https://arxiv.org/abs/1803.03635">https://arxiv.org/abs/1803.03635</a>
</div>
<div id="ref-Freund_1997_A_decision_theoretic_generalization_of_on_line" class="csl-entry" role="doc-biblioentry">
Freund, Y. &amp; Schapire, R. E. (1997). <span class="nocase">A decision-theoretic generalization of on-line learning and an application to boosting.</span> <em>Journal of Computer and System Sciences</em>, <em><span>55</span></em>, 119&#x2013;139. <a href="https://doi.org/10.1006/jcss.1997.1504">https://doi.org/10.1006/jcss.1997.1504</a>
</div>
<div id="ref-Frechet_1943_Sur_lextension_de_certaines_evaluations" class="csl-entry" role="doc-biblioentry">
Fr&#xE9;chet, M. (1943). <span class="nocase">Sur l&#x2019;extension de certaines &#xE9;valuations statistiques au cas de petits &#xE9;chantillons.</span> <em>Revue de l&#x2019;Institut International de Statistique</em>, <em><span>11</span></em>, 182&#x2013;205.
</div>
<div id="ref-Fuchs_2020_SE3_Transformers_3D_roto_translation" class="csl-entry" role="doc-biblioentry">
Fuchs, F. B., Worrall, D. E., Fischer, V., &amp; Welling, M. (2020). <span class="nocase">SE(3)-Transformers: 3D roto-translation equivariant attention networks</span>. <a href="https://arxiv.org/abs/2006.10503">https://arxiv.org/abs/2006.10503</a>
</div>
<div id="ref-Fukushima_1982_Neocognitron_A_new_algorithm_for_pattern" class="csl-entry" role="doc-biblioentry">
Fukushima, K. &amp; Miyake, S. (1982). <span class="nocase">Neocognitron: A new algorithm for pattern recognition tolerant of deformations and shifts in position.</span> <em>Pattern Recognition</em>, <em><span>15</span></em>, 455&#x2013;469.
</div>
<div id="ref-Gamba_2022_Deep_double_descent_via_smooth_interpolation" class="csl-entry" role="doc-biblioentry">
Gamba, M., Englesson, E., Bj&#xF6;rkman, M., &amp; Azizpour, H. (2022). <span class="nocase">Deep double descent via smooth interpolation</span>. <a href="https://arxiv.org/abs/2209.10080">https://arxiv.org/abs/2209.10080</a>
</div>
<div id="ref-Gandenberger_2015_A_new_proof_of_the_likelihood_principle" class="csl-entry" role="doc-biblioentry">
Gandenberger, G. (2015). <span class="nocase">A new proof of the likelihood principle.</span> <em>British Journal for the Philosophy of Science</em>, <em><span>66</span></em>, 475&#x2013;503. <a href="https://www.journals.uchicago.edu/doi/abs/10.1093/bjps/axt039">https://www.journals.uchicago.edu/doi/abs/10.1093/bjps/axt039</a>
</div>
<div id="ref-Gandenberger_2016_Why_I_am_not_a_likelihoodist" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2016). <span class="nocase">Why I am not a likelihoodist.</span> <em>Philosopher&#x2019;s Imprint</em>, <em><span>16</span></em>, 1&#x2013;22. <a href="https://quod.lib.umich.edu/p/phimp/3521354.0016.007/--why-i-am-not-a-likelihoodist">https://quod.lib.umich.edu/p/phimp/3521354.0016.007/--why-i-am-not-a-likelihoodist</a>
</div>
<div id="ref-Gao_2020_An_information_geometric_distance_on_the_space" class="csl-entry" role="doc-biblioentry">
Gao, Y. &amp; Chaudhari, P. (2020). <span class="nocase">An information-geometric distance on the space of tasks</span>. <a href="https://arxiv.org/abs/2011.00613">https://arxiv.org/abs/2011.00613</a>
</div>
<div id="ref-Gelman_2017_Beyond_subjective_and_objective_in_statistics" class="csl-entry" role="doc-biblioentry">
Gelman, A. &amp; Hennig, C. (2017). <span class="nocase">Beyond subjective and objective in statistics.</span> <em>Journal of the Royal Statistical Society: Series A (Statistics in Society)</em>, <em><span>180</span></em>, 967&#x2013;1033.
</div>
<div id="ref-Gelman_2021_What_are_the_most_important_statistical_ideas" class="csl-entry" role="doc-biblioentry">
Gelman, A. &amp; Vehtari, A. (2021). <span class="nocase">What are the most important statistical ideas of the past 50 years?</span> <em>Journal of the American Statistical Association</em>, <em><span>116</span></em>, 2087&#x2013;2097. <a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1938081">https://www.tandfonline.com/doi/full/10.1080/01621459.2021.1938081</a>
</div>
<div id="ref-Geshkovski_2023_A_mathematical_perspective_on_Transformers" class="csl-entry" role="doc-biblioentry">
Geshkovski, B., Letrouit, C., Polyanskiy, Y., &amp; Rigollet, P. (2023). <span class="nocase">A mathematical perspective on Transformers</span>. <a href="https://arxiv.org/abs/2312.10794">https://arxiv.org/abs/2312.10794</a>
</div>
<div id="ref-Ghosh_2022_A_universal_trade_off_between_the_model_size_test" class="csl-entry" role="doc-biblioentry">
Ghosh, N. &amp; Belkin, M. (2022). <span class="nocase">A universal trade-off between the model size, test loss, and training loss of linear predictors</span>. <a href="https://arxiv.org/abs/2207.11621">https://arxiv.org/abs/2207.11621</a>
</div>
<div id="ref-Gibson_2014_Regret_minimization_in_games_and_the_development" class="csl-entry" role="doc-biblioentry">
Gibson, R. (2014). <em><span class="nocase">Regret minimization in games and the development of champion multiplayer computer poker-playing agents</span></em>. <span>University of Alberta</span>. (Ph.D. thesis). <a href="https://era.library.ualberta.ca/items/15d28cbf-49d4-42e5-a9c9-fc55b1d816af/view/5ee708c7-6b8b-4b96-b1f5-23cdd95b6a46/Gibson_Richard_Spring-202014.pdf">https://era.library.ualberta.ca/items/15d28cbf-49d4-42e5-a9c9-fc55b1d816af/view/5ee708c7-6b8b-4b96-b1f5-23cdd95b6a46/Gibson_Richard_Spring-202014.pdf</a>
</div>
<div id="ref-Goldreich_1997_On_universal_learning_algorithms" class="csl-entry" role="doc-biblioentry">
Goldreich, O. &amp; Ron, D. (1997). <span class="nocase">On universal learning algorithms.</span> <em>Information Processing Letters</em>, <em><span>63</span></em>, 131&#x2013;136. <a href="https://www.wisdom.weizmann.ac.il/~oded/p_ul.html">https://www.wisdom.weizmann.ac.il/~oded/p_ul.html</a>
</div>
<div id="ref-Goodfellow_2016_Deep_Learning" class="csl-entry" role="doc-biblioentry">
Goodfellow, I., Bengio, Y., &amp; Courville, A. (2016). <em><span>Deep Learning</span></em>. <span>MIT Press</span>. <a href="http://www.deeplearningbook.org">http://www.deeplearningbook.org</a>
</div>
<div id="ref-Goodman_1999_Toward_evidence_based_medical_statistics_1_The_P" class="csl-entry" role="doc-biblioentry">
Goodman, S. N. (1999a). <span class="nocase">Toward evidence-based medical statistics 1: The P value fallacy.</span> <em>Annals of Internal Medicine</em>, <em><span>130</span></em>, 995&#x2013;1004. <a href="https://courses.botany.wisc.edu/botany_940/06EvidEvol/papers/goodman1.pdf">https://courses.botany.wisc.edu/botany_940/06EvidEvol/papers/goodman1.pdf</a>
</div>
<div id="ref-Goodman_1999_Toward_evidence_based_medical_statistics_2" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1999b). <span class="nocase">Toward evidence-based medical statistics 2: The Bayes factor.</span> <em>Annals of Internal Medicine</em>, <em><span>130</span></em>, 1005&#x2013;1013. <a href="https://courses.botany.wisc.edu/botany_940/06EvidEvol/papers/goodman2.pdf">https://courses.botany.wisc.edu/botany_940/06EvidEvol/papers/goodman2.pdf</a>
</div>
<div id="ref-Gorard_2016_What_to_do_instead_of_significance_testing" class="csl-entry" role="doc-biblioentry">
Gorard, S. &amp; Gorard, J. (2016). <span class="nocase">What to do instead of significance testing? Calculating the &#x2019;number of counterfactual cases needed to disturb a finding&#x2019;.</span> <em>International Journal of Social Research Methodology</em>, <em><span>19</span></em>, 481&#x2013;490.
</div>
<div id="ref-Graves_2013_Generating_sequences_with_recurrent_neural" class="csl-entry" role="doc-biblioentry">
Graves, A. (2013). <span class="nocase">Generating sequences with recurrent neural networks</span>. <a href="https://arxiv.org/abs/1308.0850">https://arxiv.org/abs/1308.0850</a>
</div>
<div id="ref-Grinsztajn_2022_Why_do_tree_based_models_still_outperform_deep" class="csl-entry" role="doc-biblioentry">
Grinsztajn, L., Oyallon, E., &amp; Varoquaux, G. (2022). <span class="nocase">Why do tree-based models still outperform deep learning on tabular data?</span> <a href="https://arxiv.org/abs/2207.08815">https://arxiv.org/abs/2207.08815</a>
</div>
<div id="ref-Gu_2023_Mamba_Linear_time_sequence_modeling" class="csl-entry" role="doc-biblioentry">
Gu, A. &amp; Dao, T. (2023). <span class="nocase">Mamba: Linear-time sequence modeling with selective state spaces</span>. <a href="https://arxiv.org/abs/2312.00752">https://arxiv.org/abs/2312.00752</a>
</div>
<div id="ref-Gu_2021_Efficiently_modeling_long_sequences" class="csl-entry" role="doc-biblioentry">
Gu, A., Goel, K., &amp; R&#xE9;, C. (2021). <span class="nocase">Efficiently modeling long sequences with structured state spaces</span>. <a href="https://arxiv.org/abs/2111.00396">https://arxiv.org/abs/2111.00396</a>
</div>
<div id="ref-Gurnee_2023_Finding_neurons_in_a_haystack_Case_studies" class="csl-entry" role="doc-biblioentry">
Gurnee, W. et al. (2023). <span class="nocase">Finding neurons in a haystack: Case studies with sparse probing</span>. <a href="https://arxiv.org/abs/2305.01610">https://arxiv.org/abs/2305.01610</a>
</div>
<div id="ref-Gurnee_2023_Language_models_represent_space_and_time" class="csl-entry" role="doc-biblioentry">
Gurnee, W. &amp; Tegmark, M. (2023). <span class="nocase">Language models represent space and time</span>. <a href="https://arxiv.org/abs/2310.02207">https://arxiv.org/abs/2310.02207</a>
</div>
<div id="ref-Habara_2023_Convergence_analysis_and_acceleration" class="csl-entry" role="doc-biblioentry">
Habara, K., Fukuda, E. H., &amp; Yamashita, N. (2023). <span class="nocase">Convergence analysis and acceleration of the smoothing methods for solving extensive-form games</span>. <a href="https://arxiv.org/abs/2303.11046">https://arxiv.org/abs/2303.11046</a>
</div>
<div id="ref-Haber_2017_Stable_architectures_for_deep_neural_networks" class="csl-entry" role="doc-biblioentry">
Haber, E. &amp; Ruthotto, L. (2017). <span class="nocase">Stable architectures for deep neural networks</span>. <a href="https://arxiv.org/abs/1705.03341">https://arxiv.org/abs/1705.03341</a>
</div>
<div id="ref-Hacking_1965_Logic_of_Statistical_Inference" class="csl-entry" role="doc-biblioentry">
Hacking, I. (1965). <em><span class="nocase">Logic of Statistical Inference</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Hacking_1971_Jacques_Bernoullis_Art_of_conjecturing" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1971). <span class="nocase">Jacques Bernoulli&#x2019;s Art of conjecturing.</span> <em>The British Journal for the Philosophy of Science</em>, <em><span>22</span></em>, 209&#x2013;229.
</div>
<div id="ref-Halverson_2020_Neural_networks_and_quantum_field_theory" class="csl-entry" role="doc-biblioentry">
Halverson, J., Maiti, A., &amp; Stoner, K. (2020). <span class="nocase">Neural networks and quantum field theory</span>. <a href="https://arxiv.org/abs/2008.08601">https://arxiv.org/abs/2008.08601</a>
</div>
<div id="ref-Hanley_1983_If_nothing_goes_wrong_is_everything_all_right" class="csl-entry" role="doc-biblioentry">
Hanley, J. A. &amp; Lippman-Hand, A. (1983). <span class="nocase">If nothing goes wrong, is everything all right?: Interpreting zero numerators.</span> <em>JAMA</em>, <em><span>249</span></em>, 1743&#x2013;1745.
</div>
<div id="ref-Hart_2000_A_simple_adaptive_procedure_leading_to_correlated" class="csl-entry" role="doc-biblioentry">
Hart, S. &amp; Mas&#x2010;Colell, A. (2000). <span class="nocase">A simple adaptive procedure leading to correlated equilibrium.</span> <em>Econometrica</em>, <em><span>68</span></em>, 1127&#x2013;1150. <a href="https://www.ma.imperial.ac.uk/~dturaev/Hart0.pdf">https://www.ma.imperial.ac.uk/~dturaev/Hart0.pdf</a>
</div>
<div id="ref-Hastie_2022_Surprises_in_high_dimensional_ridgeless_least" class="csl-entry" role="doc-biblioentry">
Hastie, T., Montanari, A., Rosset, S., &amp; Tibshirani, R. J. (2022). <span class="nocase">Surprises in high-dimensional ridgeless least squares interpolation.</span> <em>Annals of Statistics</em>, <em><span>50</span></em>, 949. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9481183/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9481183/</a>
</div>
<div id="ref-Hastie_2009_The_Elements_of_Statistical_Learning_Data_Mining" class="csl-entry" role="doc-biblioentry">
Hastie, T., Tibshirani, R., &amp; Friedman, J. (2009). <em><span class="nocase">The Elements of Statistical Learning: Data Mining, Inference, and Prediction</span></em> (2nd ed.). <span>Springer</span>.
</div>
<div id="ref-He_2015_Deep_residual_learning_for_image_recognition" class="csl-entry" role="doc-biblioentry">
He, K., Zhang, X., Ren, S., &amp; Sun, J. (2015). <span class="nocase">Deep residual learning for image recognition</span>. <a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>
</div>
<div id="ref-Heinrich_2007_Systematic_errors" class="csl-entry" role="doc-biblioentry">
Heinrich, J. &amp; Lyons, L. (2007). <span class="nocase">Systematic errors.</span> <em>Annual Reviews of Nuclear and Particle Science</em>, <em><span>57</span></em>, 145&#x2013;169. <a href="https://www.annualreviews.org/doi/abs/10.1146/annurev.nucl.57.090506.123052">https://www.annualreviews.org/doi/abs/10.1146/annurev.nucl.57.090506.123052</a>
</div>
<div id="ref-Heinrich_2016_Deep_reinforcement_learning_from_self_play" class="csl-entry" role="doc-biblioentry">
Heinrich, J. &amp; Silver, D. (2016). <span class="nocase">Deep reinforcement learning from self-play in imperfect-information games</span>. <a href="https://arxiv.org/abs/1603.01121">https://arxiv.org/abs/1603.01121</a>
</div>
<div id="ref-Henighan_2023_Superposition_memorization_and_double_descent" class="csl-entry" role="doc-biblioentry">
Henighan, T. et al. (2023). <span class="nocase">Superposition, memorization, and double descent</span>. <a href="https://transformer-circuits.pub/2023/toy-double-descent/index.html">https://transformer-circuits.pub/2023/toy-double-descent/index.html</a>
</div>
<div id="ref-Hennig_2015_What_are_the_true_clusters" class="csl-entry" role="doc-biblioentry">
Hennig, C. (2015). <span class="nocase">What are the true clusters?</span> <em>Pattern Recognition Letters</em>, <em><span>64</span></em>, 53&#x2013;62. <a href="https://arxiv.org/abs/1502.02555">https://arxiv.org/abs/1502.02555</a>
</div>
<div id="ref-Hestness_2017_Deep_learning_scaling_is_predictable_empirically" class="csl-entry" role="doc-biblioentry">
Hestness, J. et al. (2017). <span class="nocase">Deep learning scaling is predictable, empirically</span>. <a href="https://arxiv.org/abs/1712.00409">https://arxiv.org/abs/1712.00409</a>
</div>
<div id="ref-Hochreiter_1997_Long_short_term_memory" class="csl-entry" role="doc-biblioentry">
Hochreiter, S. &amp; Schmidhuber, J. (1997). <span class="nocase">Long short-term memory.</span> <em>Neural Computation</em>, <em><span>9</span></em>, 1735&#x2013;1780.
</div>
<div id="ref-Hoffmann_2022_Training_compute_optimal_large_language_models" class="csl-entry" role="doc-biblioentry">
Hoffmann, J. et al. (2022). <span class="nocase">Training compute-optimal large language models</span>. <a href="https://arxiv.org/abs/2203.15556">https://arxiv.org/abs/2203.15556</a>
</div>
<div id="ref-Holzmuller_2020_On_the_universality_of_the_double_descent_peak" class="csl-entry" role="doc-biblioentry">
Holzm&#xFC;ller, D. (2020). <span class="nocase">On the universality of the double descent peak in ridgeless regression</span>. <a href="https://arxiv.org/abs/2010.01851">https://arxiv.org/abs/2010.01851</a>
</div>
<div id="ref-Hornik_1989_Multilayer_feedforward_networks_are_universal" class="csl-entry" role="doc-biblioentry">
Hornik, K., Stinchcombe, M., &amp; White, H. (1989). <span class="nocase">Multilayer feedforward networks are universal approximators.</span> <em>Neural Networks</em>, <em><span>2</span></em>, 359&#x2013;366. <a href="https://cognitivemedium.com/magic_paper/assets/Hornik.pdf">https://cognitivemedium.com/magic_paper/assets/Hornik.pdf</a>
</div>
<div id="ref-Howard_2017_MobileNets_Efficient_convolutional_neural" class="csl-entry" role="doc-biblioentry">
Howard, A.G. et al. (2017). <span class="nocase">MobileNets: Efficient convolutional neural networks for mobile vision applications</span>. <a href="https://arxiv.org/abs/1704.04861">https://arxiv.org/abs/1704.04861</a>
</div>
<div id="ref-Howard_2021_Foundations_of_a_fast_data_driven_machine" class="csl-entry" role="doc-biblioentry">
Howard, J. N., Mandt, S., Whiteson, D., &amp; Yang, Y. (2021). <span class="nocase">Foundations of a fast, data-driven, machine-learned simulator</span>. <a href="https://arxiv.org/abs/2101.08944">https://arxiv.org/abs/2101.08944</a>
</div>
<div id="ref-Hutchins_2000_Yehoshua_Bar_Hillel_A_philosophers_contribution" class="csl-entry" role="doc-biblioentry">
Hutchins, J. (2000). <span class="nocase">Yehoshua Bar-Hillel: A philosophers&#x2019; contribution to machine translation</span>.
</div>
<div id="ref-Hutter_2007_Universal_Algorithmic_Intelligence_A_mathematical" class="csl-entry" role="doc-biblioentry">
Hutter, M. (2007). <span class="nocase">Universal Algorithmic Intelligence: A mathematical top-down approach</span>. In <em><span>Artificial General Intelligence</span></em> (pp. 227&#x2013;290). <span>Springer</span>. <a href="http://www.hutter1.net/ai/aixigentle.htm">http://www.hutter1.net/ai/aixigentle.htm</a>
</div>
<div id="ref-Ingrosso_2022_Data_driven_emergence_of_convolutional_structure" class="csl-entry" role="doc-biblioentry">
Ingrosso, A. &amp; Goldt, S. (2022). <span class="nocase">Data-driven emergence of convolutional structure in neural networks</span>. <a href="https://arxiv.org/abs/2202.00565">https://arxiv.org/abs/2202.00565</a>
</div>
<div id="ref-Ioannidis_2005_Why_most_published_research_findings_are_false" class="csl-entry" role="doc-biblioentry">
Ioannidis, J. P. (2005). <span class="nocase">Why most published research findings are false.</span> <em>PLOS Medicine</em>, <em><span>2</span></em>, 696&#x2013;701.
</div>
<div id="ref-Ismael_2023_Reflections_on_the_asymmetry_of_causation" class="csl-entry" role="doc-biblioentry">
Ismael, J. (2023). <span class="nocase">Reflections on the asymmetry of causation.</span> <em>Interface Focus</em>, <em><span>13</span></em>, 20220081. <a href="https://royalsocietypublishing.org/doi/pdf/10.1098/rsfs.2022.0081">https://royalsocietypublishing.org/doi/pdf/10.1098/rsfs.2022.0081</a>
</div>
<div id="ref-Ismailov_2020_A_three_layer_neural_network_can_represent_any" class="csl-entry" role="doc-biblioentry">
Ismailov, V. (2020). <span class="nocase">A three layer neural network can represent any multivariate function</span>. <a href="https://arxiv.org/abs/2012.03016">https://arxiv.org/abs/2012.03016</a>
</div>
<div id="ref-James_2006_Statistical_Methods_in_Experimental_Particle" class="csl-entry" role="doc-biblioentry">
James, F. (2006). <em><span class="nocase">Statistical Methods in Experimental Particle Physics</span></em> (2nd ed.). <span>World Scientific</span>.
</div>
<div id="ref-James_1975_MINUIT_A_system_for_function_minimization" class="csl-entry" role="doc-biblioentry">
James, F. &amp; Roos, M. (1975). <span class="nocase">MINUIT: A system for function minimization and analysis of the parameter errors and corrections.</span> <em>Computational Physics Communications</em>, <em><span>10</span></em>, 343&#x2013;367. <a href="https://cds.cern.ch/record/310399">https://cds.cern.ch/record/310399</a>
</div>
<div id="ref-James_1961_Estimation_with_quadratic_loss" class="csl-entry" role="doc-biblioentry">
James, W. &amp; Stein, C. (1961). <span class="nocase">Estimation with quadratic loss</span>. In <em><span class="nocase">Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Vol. 1</span></em> (pp. 361&#x2013;379). <span>University of California Press</span>. <a href="https://projecteuclid.org/accountAjax/Download?urlId=bsmsp%2F1200512173&amp;downloadType=presschapter&amp;isResultClick=True">https://projecteuclid.org/accountAjax/Download?urlId=bsmsp%2F1200512173&amp;downloadType=presschapter&amp;isResultClick=True</a>
</div>
<div id="ref-Johanson_2013_Measuring_the_size_of_large_no_limit_poker_games" class="csl-entry" role="doc-biblioentry">
Johanson, M. (2013). <span class="nocase">Measuring the size of large no-limit poker games</span>. <a href="https://arxiv.org/abs/1302.7008">https://arxiv.org/abs/1302.7008</a>
</div>
<div id="ref-Johanson_2016_Robust_Strategies_and_Counter_Strategies_From" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2016). <em><span class="nocase">Robust Strategies and Counter-Strategies: From Superhuman to Optimal Play</span></em>. <span>University of Alberta</span>. (Ph.D. thesis). <a href="http://johanson.ca/publications/theses/2016-johanson-phd-thesis/2016-johanson-phd-thesis.pdf">http://johanson.ca/publications/theses/2016-johanson-phd-thesis/2016-johanson-phd-thesis.pdf</a>
</div>
<div id="ref-Johanson_2012_Efficient_Nash_equilibrium_approximation_through" class="csl-entry" role="doc-biblioentry">
Johanson, M. et al. (2012). <span class="nocase">Efficient Nash equilibrium approximation through Monte Carlo counterfactual regret minimization.</span> <em>Proceedings of the 11th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2012)</em>, <em><span>2</span></em>, 837&#x2013;846. <a href="https://www.idi.ntnu.no/emner/it3105/materials/poker/monte-carlo-cfm-2012.pdf">https://www.idi.ntnu.no/emner/it3105/materials/poker/monte-carlo-cfm-2012.pdf</a>
</div>
<div id="ref-Johanson_2011_Accelerating_best_response_calculation_in_large" class="csl-entry" role="doc-biblioentry">
Johanson, M., Waugh, K., Bowling, M., &amp; Zinkevich, M. (2011). <span class="nocase">Accelerating best response calculation in large extensive games.</span> <em>IJCAI 2011, Proceedings of the 22nd International Joint Conference on Artificial Intelligence</em>, <em><span>11</span></em>, 258&#x2013;265. <a href="http://www.cs.cmu.edu/~kwaugh/publications/johanson11.pdf">http://www.cs.cmu.edu/~kwaugh/publications/johanson11.pdf</a>
</div>
<div id="ref-Joyce_2017_A_review_of_no_free_lunch_theorems_and_their" class="csl-entry" role="doc-biblioentry">
Joyce, T. &amp; Herrmann, J. M. (2017). <span class="nocase">A review of no free lunch theorems, and their implications for metaheuristic optimisation</span>. In X. S. Yang (Ed.), <em><span class="nocase">Nature-Inspired Algorithms and Applied Optimization</span></em> (pp. 27&#x2013;52).
</div>
<div id="ref-Junk_1999_Confidence_level_computation_for_combining" class="csl-entry" role="doc-biblioentry">
Junk, T. (1999). <span class="nocase">Confidence level computation for combining searches with small statistics.</span> <em>Nuclear Instruments and Methods in Physics Research Section A</em>, <em><span>434</span></em>, 435&#x2013;443. <a href="https://arxiv.org/abs/hep-ex/9902006">https://arxiv.org/abs/hep-ex/9902006</a>
</div>
<div id="ref-Jurafsky_2022_Speech_and_Language_Processing_An_introduction" class="csl-entry" role="doc-biblioentry">
Jurafsky, D. &amp; Martin, J. H. (2022). <em><span class="nocase">Speech and Language Processing: An introduction to natural language processing, computational linguistics, and speech recognition</span></em> (3rd ed.). <a href="https://web.stanford.edu/~jurafsky/slp3/ed3book_jan122022.pdf">https://web.stanford.edu/~jurafsky/slp3/ed3book_jan122022.pdf</a>
</div>
<div id="ref-Kaplan_2019_Notes_on_contemporary_machine_learning" class="csl-entry" role="doc-biblioentry">
Kaplan, J. et al. (2019). <span class="nocase">Notes on contemporary machine learning for physicists</span>. <a href="https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf">https://sites.krieger.jhu.edu/jared-kaplan/files/2019/04/ContemporaryMLforPhysicists.pdf</a>
</div>
<div id="ref-Kaplan_2020_Scaling_laws_for_neural_language_models" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2020). <span class="nocase">Scaling laws for neural language models</span>. <a href="https://arxiv.org/abs/2001.08361">https://arxiv.org/abs/2001.08361</a>
</div>
<div id="ref-Kardum_2020_Rudolf_Carnap_The_grandfather_of_artificial" class="csl-entry" role="doc-biblioentry">
Kardum, M. (2020). <span class="nocase">Rudolf Carnap&#x2013;The grandfather of artificial neural networks: The influence of Carnap&#x2019;s philosophy on Walter Pitts</span>. In S. Skansi (Ed.), <em><span>Guide To Deep Learning Basics: Logical, Historical And Philosophical Perspectives</span></em> (pp. 55&#x2013;66). <span>Springer</span>.
</div>
<div id="ref-Karniadakis_2021_Physics_informed_machine_learning" class="csl-entry" role="doc-biblioentry">
Karniadakis, G.E. et al. (2021). <span class="nocase">Physics-informed machine learning.</span> <em>Nature Reviews Physics</em>, <em><span>3</span></em>, 422&#x2013;440. <a href="https://doi.org/10.1038/s42254-021-00314-5">https://doi.org/10.1038/s42254-021-00314-5</a>
</div>
<div id="ref-Keynes_1921_A_Treatise_on_Probability" class="csl-entry" role="doc-biblioentry">
Keynes, J. M. (1921). <em><span class="nocase">A Treatise on Probability</span></em>. London: <span>Macmillan and Co</span>.
</div>
<div id="ref-Kiani_2022_projUNN_efficient_method_for_training_deep" class="csl-entry" role="doc-biblioentry">
Kiani, B., Balestriero, R., Lecun, Y., &amp; Lloyd, S. (2022). <span class="nocase">projUNN: efficient method for training deep networks with unitary matrices</span>. <a href="https://arxiv.org/abs/2203.05483">https://arxiv.org/abs/2203.05483</a>
</div>
<div id="ref-Korb_2001_Machine_learning_as_philosophy_of_science" class="csl-entry" role="doc-biblioentry">
Korb, K. B. (2001). <span class="nocase">Machine learning as philosophy of science</span>. In <em><span class="nocase">Proceedings of the ECML-PKDD-01 Workshop on Machine Learning as Experimental Philosophy of Science</span></em>. <span>Freiburg</span>.
</div>
<div id="ref-Kosinski_2023_Theory_of_mind_may_have_spontaneously_emerged" class="csl-entry" role="doc-biblioentry">
Kosinski, M. (2023). <span class="nocase">Theory of mind may have spontaneously emerged in large language models</span>. <a href="https://arxiv.org/abs/2302.02083">https://arxiv.org/abs/2302.02083</a>
</div>
<div id="ref-Kovarik_2022_Rethinking_formal_models_of_partially_observable" class="csl-entry" role="doc-biblioentry">
Kovarik, V. et al. (2022). <span class="nocase">Rethinking formal models of partially observable multiagent decision making.</span> <em>Artificial Intelligence</em>, <em><span>303</span></em>, 103645. <a href="https://arxiv.org/abs/1906.11110">https://arxiv.org/abs/1906.11110</a>
</div>
<div id="ref-Krenn_2022_On_scientific_understanding_with_artificial" class="csl-entry" role="doc-biblioentry">
Krenn, M. et al. (2022). <span class="nocase">On scientific understanding with artificial intelligence.</span> <em>Nature Reviews Physics</em>. <a href="https://www.nature.com/articles/s42254-022-00518-3">https://www.nature.com/articles/s42254-022-00518-3</a>
</div>
<div id="ref-Krizhevsky_2012_ImageNet_classification_with_deep_convolutional" class="csl-entry" role="doc-biblioentry">
Krizhevsky, A., Sutskever, I., &amp; Hinton, G. E. (2012). <span class="nocase">ImageNet classification with deep convolutional neural networks.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>2012</span></em>, 1097&#x2013;1105. <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>
</div>
<div id="ref-Kruschke_2018_The_Bayesian_New_Statistics_Hypothesis_testing" class="csl-entry" role="doc-biblioentry">
Kruschke, J. K. &amp; Liddell, T. M. (2018). <span class="nocase">The Bayesian New Statistics: Hypothesis testing, estimation, meta-analysis, and power analysis from a Bayesian perspective.</span> <em>Psychonomic Bulletin &amp; Review</em>, <em><span>25</span></em>, 178&#x2013;206. <a href="https://link.springer.com/article/10.3758/s13423-016-1221-4">https://link.springer.com/article/10.3758/s13423-016-1221-4</a>
</div>
<div id="ref-Kuhn_1950_A_simplified_two_person_poker" class="csl-entry" role="doc-biblioentry">
Kuhn, H. W. (1950). <span class="nocase">A simplified two-person poker.</span> <em>Contributions to the Theory of Games</em>, <em><span>1</span></em>, 97&#x2013;103.
</div>
<div id="ref-Kun_2018_A_Programmers_Introduction_to_Mathematics" class="csl-entry" role="doc-biblioentry">
Kun, J. (2018). <em><span class="nocase">A Programmer&#x2019;s Introduction to Mathematics</span></em>. <span>CreateSpace Independent Publishing Platform</span>.
</div>
<div id="ref-Lan_2019_ALBERT_A_lite_BERT_for_self_supervised_learning" class="csl-entry" role="doc-biblioentry">
Lan, Z. et al. (2019). <span class="nocase">ALBERT: A lite BERT for self-supervised learning of language representations</span>. <a href="https://arxiv.org/abs/1909.11942">https://arxiv.org/abs/1909.11942</a>
</div>
<div id="ref-Lanctot_2013_Monte_Carlo_Sample_and_Regret_Minimization" class="csl-entry" role="doc-biblioentry">
Lanctot, M. (2013). <em><span class="nocase">Monte Carlo Sample and Regret Minimization for Equilibrium Computation and Decision-Making in Large Extensive Form Games</span></em>. <span>University of Alberta</span>. (PhD thesis). <a href="http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf">http://mlanctot.info/files/papers/PhD_Thesis_MarcLanctot.pdf</a>
</div>
<div id="ref-Lanctot_2017_A_unified_game_theoretic_approach_to_multiagent" class="csl-entry" role="doc-biblioentry">
Lanctot, M. et al. (2017). <span class="nocase">A unified game-theoretic approach to multiagent reinforcement learning.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>30</span></em>. <a href="https://arxiv.org/abs/1711.00832">https://arxiv.org/abs/1711.00832</a>
</div>
<div id="ref-Lanctot_2009_Monte_Carlo_sampling_for_regret_minimization" class="csl-entry" role="doc-biblioentry">
Lanctot, M., Waugh, K., Zinkevich, M., &amp; Bowling, M. (2009). <span class="nocase">Monte Carlo sampling for regret minimization in extensive games.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>22</span></em>, 1078&#x2013;1086. <a href="https://proceedings.neurips.cc/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf">https://proceedings.neurips.cc/paper/2009/file/00411460f7c92d2124a67ea0f4cb5f85-Paper.pdf</a>
</div>
<div id="ref-Lauc_2020_Machine_learning_and_the_philosophical_problems" class="csl-entry" role="doc-biblioentry">
Lauc, D. (2020). <span class="nocase">Machine learning and the philosophical problems of induction</span>. In S. Skansi (Ed.), <em><span>Guide To Deep Learning Basics: Logical, Historical And Philosophical Perspectives</span></em> (pp. 93&#x2013;106). <span>Springer</span>.
</div>
<div id="ref-LeCun_2015_Deep_learning" class="csl-entry" role="doc-biblioentry">
LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). <span class="nocase">Deep learning.</span> <em>Nature</em>, <em><span>521</span></em>, 436&#x2013;44.
</div>
<div id="ref-LeCun_1998_Efficient_BackProp" class="csl-entry" role="doc-biblioentry">
LeCun, Y. &amp; Bottou, L. (1998). <span>Efficient BackProp</span>. In G. B. Orr &amp; K. R. Muller (Eds.), <em><span class="nocase">Neural Networks: Tricks of the trade</span></em>. <span>Springer</span>. <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf">http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf</a>
</div>
<div id="ref-LeCun_1998_Gradient_based_learning_applied_to_document" class="csl-entry" role="doc-biblioentry">
LeCun, Y., Bottou, L., Bengio, Y., &amp; Haffner, P. (1998). <span class="nocase">Gradient-based learning applied to document recognition.</span> <em>Proceedings of the IEEE</em>, <em><span>86</span></em>, 2278&#x2013;2324. <a href="http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf">http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf</a>
</div>
<div id="ref-LeCun_1989_Backpropagation_applied_to_handwritten_zip_code" class="csl-entry" role="doc-biblioentry">
LeCun, Y. et al. (1989). <span class="nocase">Backpropagation applied to handwritten zip code recognition.</span> <em>Neural Computation</em>, <em><span>1</span></em>, 541&#x2013;551. <a href="https://web.archive.org/web/20150611222615/http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf">https://web.archive.org/web/20150611222615/http://yann.lecun.com/exdb/publis/pdf/lecun-89e.pdf</a>
</div>
<div id="ref-Leemis_2008_Univariate_distribution_relationships" class="csl-entry" role="doc-biblioentry">
Leemis, L. M. &amp; McQueston, J. T. (2008). <span class="nocase">Univariate distribution relationships.</span> <em>The American Statistician</em>, <em><span>62</span></em>, 45&#x2013;53. <a href="http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf">http://www.stat.rice.edu/~dobelman/courses/texts/leemis.distributions.2008amstat.pdf</a>
</div>
<div id="ref-Lei_2018_Geometric_understanding_of_deep_learning" class="csl-entry" role="doc-biblioentry">
Lei, N., Luo, Z., Yau, S., &amp; Gu, D. X. (2018). <span class="nocase">Geometric understanding of deep learning</span>. <a href="https://arxiv.org/abs/1805.10451">https://arxiv.org/abs/1805.10451</a>
</div>
<div id="ref-Lewis_1981_Causal_decision_theory" class="csl-entry" role="doc-biblioentry">
Lewis, D. (1981). <span class="nocase">Causal decision theory.</span> <em>Australasian Journal of Philosophy</em>, <em><span>59</span></em>, 5&#x2013;30. <a href="https://www.andrewmbailey.com/dkl/Causal_Decision_Theory.pdf">https://www.andrewmbailey.com/dkl/Causal_Decision_Theory.pdf</a>
</div>
<div id="ref-Lewis_2019_BART_Denoising_sequence_to_sequence_pre_training" class="csl-entry" role="doc-biblioentry">
Lewis, M. et al. (2019). <span class="nocase">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension</span>. <a href="https://arxiv.org/abs/1910.13461">https://arxiv.org/abs/1910.13461</a>
</div>
<div id="ref-Li_2020_Regret_minimization_via_novel_vectorized_sampling" class="csl-entry" role="doc-biblioentry">
Li, H. et al. (2020). <span class="nocase">Regret minimization via novel vectorized sampling policies and exploration</span>. <a href="http://aaai-rlg.mlanctot.info/2020/papers/AAAI20-RLG_paper_14.pdf">http://aaai-rlg.mlanctot.info/2020/papers/AAAI20-RLG_paper_14.pdf</a>
</div>
<div id="ref-Lin_2018_ResNet_with_one_neuron_hidden_layers_is" class="csl-entry" role="doc-biblioentry">
Lin, H. &amp; Jegelka, S. (2018). <span class="nocase">ResNet with one-neuron hidden layers is a universal approximator</span>. <a href="https://arxiv.org/abs/1806.10909">https://arxiv.org/abs/1806.10909</a>
</div>
<div id="ref-Lista_2016_Practical_statistics_for_particle_physicists" class="csl-entry" role="doc-biblioentry">
Lista, L. (2016a). <span class="nocase">Practical statistics for particle physicists</span>. <a href="https://arxiv.org/abs/1609.04150">https://arxiv.org/abs/1609.04150</a>
</div>
<div id="ref-Lista_2016_Statistical_Methods_for_Data_Analysis_in_Particle" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2016b). <em><span class="nocase">Statistical Methods for Data Analysis in Particle Physics</span></em>. <span>Springer</span>. <a href="http://foswiki.oris.mephi.ru/pub/Main/Literature/st_methods_for_data_analysis_in_particle_ph.pdf">http://foswiki.oris.mephi.ru/pub/Main/Literature/st_methods_for_data_analysis_in_particle_ph.pdf</a>
</div>
<div id="ref-Lisy_2016_Equilibrium_approximation_quality_of_current_no" class="csl-entry" role="doc-biblioentry">
Lisy, V. &amp; Bowling, M. (2016). <span class="nocase">Equilibrium approximation quality of current no-limit poker bots</span>. <a href="https://arxiv.org/abs/1612.07547">https://arxiv.org/abs/1612.07547</a>
</div>
<div id="ref-Liu_2021_Pay_attention_to_MLPs" class="csl-entry" role="doc-biblioentry">
Liu, H., Dai, Z., So, D. R., &amp; Le, Q. V. (2021). <span class="nocase">Pay attention to MLPs</span>. <a href="https://arxiv.org/abs/2105.08050">https://arxiv.org/abs/2105.08050</a>
</div>
<div id="ref-Liu_2019_RoBERTa_A_robustly_optimized_BERT_pretraining" class="csl-entry" role="doc-biblioentry">
Liu, Y. et al. (2019). <span class="nocase">RoBERTa: A robustly optimized BERT pretraining approach</span>. <a href="https://arxiv.org/abs/1907.11692">https://arxiv.org/abs/1907.11692</a>
</div>
<div id="ref-Liu_2021_A_survey_of_visual_transformers" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2021). <span class="nocase">A survey of visual transformers</span>. <a href="https://arxiv.org/abs/2111.06091">https://arxiv.org/abs/2111.06091</a>
</div>
<div id="ref-Liu_2023_Representation_Learning_for_Natural_Language" class="csl-entry" role="doc-biblioentry">
Liu, Z., Lin, Y., &amp; Sun, M. (2023). <em><span class="nocase">Representation Learning for Natural Language Processing</span></em>. <span>Springer</span>. <a href="https://link.springer.com/book/10.1007/978-981-99-1600-9">https://link.springer.com/book/10.1007/978-981-99-1600-9</a>
</div>
<div id="ref-Liu_2022_AI_Poincare_2_Machine_learning_conservation_laws" class="csl-entry" role="doc-biblioentry">
Liu, Z., Madhavan, V., &amp; Tegmark, M. (2022). <span class="nocase">AI Poincare 2: Machine learning conservation laws from differential equations</span>. <a href="https://arxiv.org/abs/2203.12610">https://arxiv.org/abs/2203.12610</a>
</div>
<div id="ref-Lovering_2022_Unit_testing_for_concepts_in_neural_networks" class="csl-entry" role="doc-biblioentry">
Lovering, C. &amp; Pavlick, E. (2022). <span class="nocase">Unit testing for concepts in neural networks.</span> <em>Transactions of the Association for Computational Linguistics</em>, <em><span>10</span></em>, 1193&#x2013;1208. <a href="https://aclanthology.org/2022.tacl-1.69/">https://aclanthology.org/2022.tacl-1.69/</a>
</div>
<div id="ref-Lu_2017_The_expressive_power_of_neural_networks_A_view" class="csl-entry" role="doc-biblioentry">
Lu, Z. et al. (2017). <span class="nocase">The expressive power of neural networks: A view from the width.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>30</span></em>. <a href="https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf">https://proceedings.neurips.cc/paper/2017/file/32cbf687880eb1674a07bf717761dd3a-Paper.pdf</a>
</div>
<div id="ref-Lundberg_2021_What_is_your_estimand_Defining_the_target" class="csl-entry" role="doc-biblioentry">
Lundberg, I., Johnson, R., &amp; Stewart, B. M. (2021). <span class="nocase">What is your estimand? Defining the target quantity connects statistical evidence to theory.</span> <em>American Sociological Review</em>, <em><span>86</span></em>, 532&#x2013;565. <a href="https://journals.sagepub.com/doi/abs/10.1177/00031224211004187">https://journals.sagepub.com/doi/abs/10.1177/00031224211004187</a>
</div>
<div id="ref-Lyons_2008_Open_statistical_issues_in_particle_physics" class="csl-entry" role="doc-biblioentry">
Lyons, L. (2008). <span class="nocase">Open statistical issues in particle physics.</span> <em>The Annals of Applied Statistics</em>, <em><span>2</span></em>, 887&#x2013;915. <a href="https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Open-statistical-issues-in-Particle-Physics/10.1214/08-AOAS163.full">https://projecteuclid.org/journals/annals-of-applied-statistics/volume-2/issue-3/Open-statistical-issues-in-Particle-Physics/10.1214/08-AOAS163.full</a>
</div>
<div id="ref-Ma_2024_The_era_of_1_bit_LLMs_All_large_language_models" class="csl-entry" role="doc-biblioentry">
Ma, S. et al. (2024). <span class="nocase">The era of 1-bit LLMs: All large language models are in 1</span>.
</div>
<div id="ref-Ma_2024_Megalodon_Efficient_LLM_pretraining_and_inference" class="csl-entry" role="doc-biblioentry">
Ma, X. et al. (2024). <span class="nocase">Megalodon: Efficient LLM pretraining and inference with unlimited context length</span>. <a href="https://arxiv.org/abs/2404.08801">https://arxiv.org/abs/2404.08801</a>
</div>
<div id="ref-MacKay_2003_Information_Theory_Inference_and_Learning" class="csl-entry" role="doc-biblioentry">
MacKay, D. J. C. (2003). <em><span class="nocase">Information Theory, Inference, and Learning Algorithms</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Maddox_2023_Rethinking_parameter_counting_in_deep_models" class="csl-entry" role="doc-biblioentry">
Maddox, W. J., Benton, G., &amp; Wilson, A. G. (2023). <span class="nocase">Rethinking parameter counting in deep models: Effective dimensionality revisited</span>. <a href="https://arxiv.org/abs/2003.02139">https://arxiv.org/abs/2003.02139</a>
</div>
<div id="ref-Mahowald_2023_Dissociating_language_and_thought_in_large" class="csl-entry" role="doc-biblioentry">
Mahowald, K. et al. (2023). <span class="nocase">Dissociating language and thought in large language models: a cognitive perspective</span>. <a href="https://arxiv.org/abs/2301.06627">https://arxiv.org/abs/2301.06627</a>
</div>
<div id="ref-Marchetti_2023_Harmonics_of_learning_Universal_fourier_features" class="csl-entry" role="doc-biblioentry">
Marchetti, G. L., Hillar, C., Kragic, D., &amp; Sanborn, S. (2023). <span class="nocase">Harmonics of learning: Universal fourier features emerge in invariant networks</span>. <a href="https://arxiv.org/abs/2312.08550">https://arxiv.org/abs/2312.08550</a>
</div>
<div id="ref-Mayo_1981_In_defense_of_the_Neyman_Pearson_theory" class="csl-entry" role="doc-biblioentry">
Mayo, D. G. (1981). <span class="nocase">In defense of the Neyman-Pearson theory of confidence intervals.</span> <em>Philosophy of Science</em>, <em><span>48</span></em>, 269&#x2013;280.
</div>
<div id="ref-Mayo_1996_Error_and_the_Growth_of_Experimental_Knowledge" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1996). <em><span class="nocase">Error and the Growth of Experimental Knowledge</span></em>. <span>Chicago University Press</span>.
</div>
<div id="ref-Mayo_2014_On_the_Birnbaum_Argument_for_the_Strong_Likelihood" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2014). <span class="nocase">On the Birnbaum Argument for the Strong Likelihood Principle,</span>. <em>Statistical Science</em>, <em><span>29</span></em>, 227&#x2013;266.
</div>
<div id="ref-Mayo_2018_Statistical_Inference_as_Severe_Testing_How" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2018). <em><span class="nocase">Statistical Inference as Severe Testing: How to Get Beyond the Statistics Wars</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Mayo_2019_The_law_of_likelihood_and_error_statistics" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2019). <span class="nocase">The law of likelihood and error statistics</span>. <a href="https://errorstatistics.com/2019/04/04/excursion-1-tour-ii-error-probing-tools-versus-logics-of-evidence-excerpt/">https://errorstatistics.com/2019/04/04/excursion-1-tour-ii-error-probing-tools-versus-logics-of-evidence-excerpt/</a>
</div>
<div id="ref-Mayo_2021_Significance_tests_Vitiated_or_vindicated" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2021). <span class="nocase">Significance tests: Vitiated or vindicated by the replication crisis in psychology?</span> <em>Review of Philosophy and Psychology</em>, <em><span>12</span></em>, 101&#x2013;121. <a href="https://link.springer.com/article/10.1007/s13164-020-00501-w">https://link.springer.com/article/10.1007/s13164-020-00501-w</a>
</div>
<div id="ref-Mayo_2006_Severe_testing_as_a_basic_concept_in_a_Neyman" class="csl-entry" role="doc-biblioentry">
Mayo, D. G. &amp; Spanos, A. (2006). <span class="nocase">Severe testing as a basic concept in a Neyman-Pearson philosophy of induction.</span> <em>British Journal for the Philosophy of Science</em>, <em><span>57</span></em>, 323&#x2013;357.
</div>
<div id="ref-Mayo_2011_Error_statistics" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2011). <span class="nocase">Error statistics</span>. In <em><span class="nocase">Philosophy of Statistics</span></em> (pp. 153&#x2013;198). <span>North-Holland</span>.
</div>
<div id="ref-McCarthy_1955_A_proposal_for_the_Dartmouth_Summer_Research" class="csl-entry" role="doc-biblioentry">
McCarthy, J., Minsky, M. L., Rochester, N., &amp; Shannon, C. E. (1955). <span class="nocase">A proposal for the Dartmouth Summer Research Project on Artificial Intelligence</span>. <a href="http://www-formal.stanford.edu/jmc/history/dartmouth.pdf">http://www-formal.stanford.edu/jmc/history/dartmouth.pdf</a>
</div>
<div id="ref-McDermott_2019_When_and_why_metaheuristics_researchers_can_ignore" class="csl-entry" role="doc-biblioentry">
McDermott, J. (2019). <span class="nocase">When and why metaheuristics researchers can ignore "no free lunch" theorems</span>. <a href="https://arxiv.org/abs/1906.03280">https://arxiv.org/abs/1906.03280</a>
</div>
<div id="ref-McDougall_2023_Copy_suppression_Comprehensively_understanding" class="csl-entry" role="doc-biblioentry">
McDougall, C. et al. (2023). <span class="nocase">Copy suppression: Comprehensively understanding an attention head</span>. <a href="https://arxiv.org/abs/2310.04625">https://arxiv.org/abs/2310.04625</a>
</div>
<div id="ref-McFadden_1973_Conditional_logit_analysis_of_qualitative_choice" class="csl-entry" role="doc-biblioentry">
McFadden, D. &amp; Zarembka, P. (1973). <span class="nocase">Conditional logit analysis of qualitative choice behavior</span>. In <em><span class="nocase">Frontiers in Econometrics</span></em> (pp. 105&#x2013;142). New York: <span>Academic Press</span>.
</div>
<div id="ref-Meehl_1978_Theoretical_risks_and_tabular_asterisks_Sir_Karl" class="csl-entry" role="doc-biblioentry">
Meehl, P. E. (1978). <span class="nocase">Theoretical risks and tabular asterisks: Sir Karl, Sir Ronald, and the slow progress of soft psychology.</span> <em>Journal of Consulting and Clinical Psychology</em>, <em><span>46</span></em>, 806&#x2013;834.
</div>
<div id="ref-Meng_2023_Locating_and_editing_factual_associations_in_GPT" class="csl-entry" role="doc-biblioentry">
Meng, K., Bau, D., Andonian, A., &amp; Belinkov, Y. (2023). <span class="nocase">Locating and editing factual associations in GPT https://arxiv</span>.
</div>
<div id="ref-Merrill_2022_The_parallelism_tradeoff_Limitations_of_log" class="csl-entry" role="doc-biblioentry">
Merrill, W. &amp; Sabharwal, A. (2022). <span class="nocase">The parallelism tradeoff: Limitations of log-precision transformers</span>. <a href="https://arxiv.org/abs/2207.00729">https://arxiv.org/abs/2207.00729</a>
</div>
<div id="ref-Mialon_2023_Augmented_Language_Models_a_Survey" class="csl-entry" role="doc-biblioentry">
Mialon, G. et al. (2023). <span class="nocase">Augmented Language Models: a Survey</span>. <a href="https://arxiv.org/abs/2302.07842">https://arxiv.org/abs/2302.07842</a>
</div>
<div id="ref-Mikolov_2013_Efficient_estimation_of_word_representations" class="csl-entry" role="doc-biblioentry">
Mikolov, T., Chen, K., Corrado, G., &amp; Dean, J. (2013). <span class="nocase">Efficient estimation of word representations in vector space</span>. <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>
</div>
<div id="ref-Mikolov_2013_Distributed_representations_of_words_and_phrases" class="csl-entry" role="doc-biblioentry">
Mikolov, T. et al. (2013). <span class="nocase">Distributed representations of words and phrases and their compositionality</span>. <a href="https://arxiv.org/abs/1310.4546">https://arxiv.org/abs/1310.4546</a>
</div>
<div id="ref-Mikolov_2013_Linguistic_regularities_in_continuous_space_word" class="csl-entry" role="doc-biblioentry">
Mikolov, T., Yih, W. T., &amp; Zweig, G. (2013). <span class="nocase">Linguistic regularities in continuous space word representations</span>. NAACL HLT 2013. <a href="https://www.aclweb.org/anthology/N13-1090.pdf">https://www.aclweb.org/anthology/N13-1090.pdf</a>
</div>
<div id="ref-Minsky_1969_Perceptrons_An_Introduction_to_Computational" class="csl-entry" role="doc-biblioentry">
Minsky, M. &amp; Papert, S. (1969). <em><span class="nocase">Perceptrons: An Introduction to Computational Geometry</span></em>. <span>MIT Press</span>.
</div>
<div id="ref-Mitchell_1980_The_need_for_biases_in_learning_generalizations" class="csl-entry" role="doc-biblioentry">
Mitchell, T. M. (1980). <span class="nocase">The need for biases in learning generalizations</span>. In <em><span class="nocase">Readings in Machine Learning</span></em> (pp. 184&#x2013;192). <span>San Mateo, CA, USA</span>. <a href="http://www.cs.cmu.edu/afs/cs/usr/mitchell/ftp/pubs/NeedForBias_1980.pdf">http://www.cs.cmu.edu/afs/cs/usr/mitchell/ftp/pubs/NeedForBias_1980.pdf</a>
</div>
<div id="ref-Mnih_2013_Playing_Atari_with_deep_reinforcement_learning" class="csl-entry" role="doc-biblioentry">
Mnih, V. et al. (2013). <span class="nocase">Playing Atari with deep reinforcement learning</span>. <a href="https://arxiv.org/abs/1312.5602">https://arxiv.org/abs/1312.5602</a>
</div>
<div id="ref-Mnih_2015_Human_level_control_through_deep_reinforcement" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2015). <span class="nocase">Human-level control through deep reinforcement learning.</span> <em>Nature</em>, <em><span>518</span></em>, 529&#x2013;533. <a href="http://files.davidqiu.com//research/nature14236.pdf">http://files.davidqiu.com//research/nature14236.pdf</a>
</div>
<div id="ref-Mohamadi_2023_ChatGPT_in_the_age_of_generative_AI_and_large" class="csl-entry" role="doc-biblioentry">
Mohamadi, S. et al. (2023). <span class="nocase">ChatGPT in the age of generative AI and large language models: A concise survey</span>. <a href="https://arxiv.org/abs/2307.04251v1">https://arxiv.org/abs/2307.04251v1</a>
</div>
<div id="ref-Moravcik_2017_DeepStack_Expert_level_artificial_intelligence" class="csl-entry" role="doc-biblioentry">
Moravcik, M. et al. (2017). <span class="nocase">DeepStack: Expert-level artificial intelligence in heads-up no-limit poker.</span> <em>Science</em>, <em><span>356</span></em>, 508&#x2013;513. <a href="https://arxiv.org/abs/1701.01724">https://arxiv.org/abs/1701.01724</a>
</div>
<div id="ref-Muennighoff_2023_Scaling_data_constrained_language_models" class="csl-entry" role="doc-biblioentry">
Muennighoff, N. et al. (2023). <span class="nocase">Scaling data-constrained language models</span>. <a href="https://arxiv.org/abs/2305.16264">https://arxiv.org/abs/2305.16264</a>
</div>
<div id="ref-Murphy_2012_Machine_Learning_A_probabilistic_perspective" class="csl-entry" role="doc-biblioentry">
Murphy, K. P. (2012). <em><span class="nocase">Machine Learning: A probabilistic perspective</span></em>. <span>MIT Press</span>.
</div>
<div id="ref-Murphy_2022_Probabilistic_Machine_Learning_An_introduction" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2022). <em><span class="nocase">Probabilistic Machine Learning: An introduction</span></em>. <span>MIT Press</span>.
</div>
<div id="ref-Muthukumar_2019_Harmless_interpolation_of_noisy_data_in_regression" class="csl-entry" role="doc-biblioentry">
Muthukumar, V., Vodrahalli, K., Subramanian, V., &amp; Sahai, A. (2019). <span class="nocase">Harmless interpolation of noisy data in regression</span>. <a href="https://arxiv.org/abs/1903.09139">https://arxiv.org/abs/1903.09139</a>
</div>
<div id="ref-Nagarajan_2021_Explaining_generalization_in_deep_learning" class="csl-entry" role="doc-biblioentry">
Nagarajan, V. (2021). <em><span class="nocase">Explaining generalization in deep learning: progress and fundamental limits</span></em>. (Ph.D. thesis). <a href="https://arxiv.org/abs/2110.08922">https://arxiv.org/abs/2110.08922</a>
</div>
<div id="ref-Nakkiran_2021_Turing_universal_learners_with_optimal_scaling" class="csl-entry" role="doc-biblioentry">
Nakkiran, P. (2021). <span class="nocase">Turing-universal learners with optimal scaling laws</span>. <a href="https://arxiv.org/abs/2111.05321">https://arxiv.org/abs/2111.05321</a>
</div>
<div id="ref-Nakkiran_2019_Deep_double_descent_Where_bigger_models_and_more" class="csl-entry" role="doc-biblioentry">
Nakkiran, P. et al. (2019). <span class="nocase">Deep double descent: Where bigger models and more data hurt</span>. <a href="https://arxiv.org/abs/1912.02292">https://arxiv.org/abs/1912.02292</a>
</div>
<div id="ref-Neller_2013_An_introduction_to_counterfactual_regret" class="csl-entry" role="doc-biblioentry">
Neller, T. W. &amp; Lanctot, M. (2013). <span class="nocase">An introduction to counterfactual regret minimization.</span> <em>Proceedings of Model AI Assignments</em>, <em><span>11</span></em>. <a href="http://cs.gettysburg.edu/~tneller/modelai/2013/cfr/cfr.pdf">http://cs.gettysburg.edu/~tneller/modelai/2013/cfr/cfr.pdf</a>
</div>
<div id="ref-Neyman_1955_The_problem_of_inductive_inference" class="csl-entry" role="doc-biblioentry">
Neyman, J. (1955). <span class="nocase">The problem of inductive inference.</span> <em>Communications on Pure and Applied Mathematics</em>, <em><span>8</span></em>, 13&#x2013;45. <a href="https://errorstatistics.files.wordpress.com/2017/04/neyman-1955-the-problem-of-inductive-inference-searchable.pdf">https://errorstatistics.files.wordpress.com/2017/04/neyman-1955-the-problem-of-inductive-inference-searchable.pdf</a>
</div>
<div id="ref-Neyman_1977_Frequentist_probability_and_frequentist" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1977). <span class="nocase">Frequentist probability and frequentist statistics.</span> <em>Synthese</em>, <em><span>36</span></em>, 97&#x2013;131.
</div>
<div id="ref-Neyman_1933_On_the_problem_of_the_most_efficient_tests" class="csl-entry" role="doc-biblioentry">
Neyman, J. &amp; Pearson, E. S. (1933). <span class="nocase">On the problem of the most efficient tests of statistical hypotheses.</span> <em>Philosophical Transactions of the Royal Society A</em>, <em><span>231</span></em>, 289&#x2013;337.
</div>
<div id="ref-Nielsen_2013_Cramer_Rao_lower_bound_and_information_geometry" class="csl-entry" role="doc-biblioentry">
Nielsen, F. (2013). <span class="nocase">Cramer-Rao lower bound and information geometry</span>. <a href="https://arxiv.org/abs/1301.3578">https://arxiv.org/abs/1301.3578</a>
</div>
<div id="ref-Nielsen_2020_An_elementary_introduction_to_information" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2020). <span class="nocase">An elementary introduction to information geometry.</span> <em>Entropy</em>, <em><span>22</span></em>, 1100. <a href="https://www.mdpi.com/1099-4300/22/10/1100">https://www.mdpi.com/1099-4300/22/10/1100</a>
</div>
<div id="ref-Nirenburg_1996_Bar_Hillel_and_Machine_Translation_Then_and_Now" class="csl-entry" role="doc-biblioentry">
Nirenburg, S. (1996). <span class="nocase">Bar Hillel and Machine Translation: Then and Now</span>.
</div>
<div id="ref-Nissim_2019_Fair_is_better_than_sensational_Man_is_to_doctor" class="csl-entry" role="doc-biblioentry">
Nissim, M., Noord, R. van, &amp; Goot, R. van der. (2019). <span class="nocase">Fair is better than sensational: Man is to doctor as woman is to doctor.</span> <em>Computational Linguistics</em>, <em><span>46</span></em>, 487&#x2013;497.
</div>
<div id="ref-Norvig_2011_On_Chomsky_and_the_Two_Cultures_of_Statistical" class="csl-entry" role="doc-biblioentry">
Norvig, P. (2011). <span class="nocase">On Chomsky and the Two Cultures of Statistical Learning</span>. <a href="https://norvig.com/chomsky.html">https://norvig.com/chomsky.html</a>
</div>
<div id="ref-OHagan_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2B" class="csl-entry" role="doc-biblioentry">
O&#x2019;Hagan, A. (2010). <em><span class="nocase">Kendall&#x2019;s Advanced Theory of Statistics, Vol 2B: Bayesian Inference</span></em>. <span>Wiley</span>.
</div>
<div id="ref-OpenAI_2023_GPT_4_Technical_Report" class="csl-entry" role="doc-biblioentry">
OpenAI. (2023). <span>GPT-4 Technical Report</span>. <a href="https://cdn.openai.com/papers/gpt-4.pdf">https://cdn.openai.com/papers/gpt-4.pdf</a>
</div>
<div id="ref-Opper_2001_Learning_to_generalize" class="csl-entry" role="doc-biblioentry">
Opper, M. (2001). <span class="nocase">Learning to generalize.</span> <em>Frontiers of Life</em>, <em><span>3</span></em>, 763&#x2013;775.
</div>
<div id="ref-Opper_1996_Statistical_mechanics_of_generalization" class="csl-entry" role="doc-biblioentry">
Opper, M. &amp; Kinzel, W. (1996). <span class="nocase">Statistical mechanics of generalization</span>. In <em><span class="nocase">Models of Neural Networks III: Association, Generalization, and Representation</span></em> (pp. 151&#x2013;209). <span>Springer New York</span>. <a href="https://gwern.net/doc/ai/nn/1996-opper.pdf">https://gwern.net/doc/ai/nn/1996-opper.pdf</a>
</div>
<div id="ref-Otsuka_2023_Thinking_About_Statistics_The_Philosophical" class="csl-entry" role="doc-biblioentry">
Otsuka, J. (2023). <em><span>Thinking About Statistics: The Philosophical Foundations</span></em>. <span>Routledge</span>.
</div>
<div id="ref-Ouyang_2022_Training_language_models_to_follow_instructions" class="csl-entry" role="doc-biblioentry">
Ouyang, L. et al. (2022). <span class="nocase">Training language models to follow instructions with human feedback</span>. <a href="https://arxiv.org/abs/2203.02155">https://arxiv.org/abs/2203.02155</a>
</div>
<div id="ref-Park_2022_How_do_vision_transformers_work" class="csl-entry" role="doc-biblioentry">
Park, N. &amp; Kim, S. (2022). <span class="nocase">How do vision transformers work?</span> <a href="https://arxiv.org/abs/2202.06709">https://arxiv.org/abs/2202.06709</a>
</div>
<div id="ref-Patel_2022_Mapping_language_models_to_grounded_conceptual" class="csl-entry" role="doc-biblioentry">
Patel, R. &amp; Pavlick, E. (2022). <span class="nocase">Mapping language models to grounded conceptual spaces.</span> <em>International Conference on Learning Representations</em>, <em><span>2022</span></em>. <a href="https://openreview.net/pdf?id=gJcEM8sxHK">https://openreview.net/pdf?id=gJcEM8sxHK</a>
</div>
<div id="ref-Pearl_2009_Causal_inference_in_statistics_An_overview" class="csl-entry" role="doc-biblioentry">
Pearl, J. (2009). <span class="nocase">Causal inference in statistics: An overview.</span> <em>Statistics Surveys</em>, <em><span>3</span></em>, 96&#x2013;146. <a href="https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.pdf">https://projecteuclid.org/journals/statistics-surveys/volume-3/issue-none/Causal-inference-in-statistics-An-overview/10.1214/09-SS057.pdf</a>
</div>
<div id="ref-Pearl_2018_The_Book_of_Why_The_new_science_of_cause" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2018). <em><span class="nocase">The Book of Why: The new science of cause and effect</span></em>. <span>Basic Books</span>.
</div>
<div id="ref-Pearson_1900_On_the_criterion_that_a_given_system_of_deviations" class="csl-entry" role="doc-biblioentry">
Pearson, K. (1900). <span class="nocase">On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling.</span> <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em>, <em><span>50</span></em>, 157&#x2013;175.
</div>
<div id="ref-Peirce_1883_Studies_in_Logic" class="csl-entry" role="doc-biblioentry">
Peirce, C. S. (1883). <em><span class="nocase">Studies in Logic</span></em>. Boston: <span>Little, Brown, and Co</span>.
</div>
<div id="ref-Peng_2023_RWKV_Reinventing_RNNs_for_the_Transformer_Era" class="csl-entry" role="doc-biblioentry">
Peng, B. et al. (2023). <span class="nocase">RWKV: Reinventing RNNs for the Transformer Era</span>. <a href="https://arxiv.org/abs/2305.13048">https://arxiv.org/abs/2305.13048</a>
</div>
<div id="ref-Perone_2018_NLP_word_representations_and_the_Wittgenstein" class="csl-entry" role="doc-biblioentry">
Perone, C. S. (2018). <span class="nocase">NLP word representations and the Wittgenstein philosophy of language</span>. <a href="http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/">http://blog.christianperone.com/2018/05/nlp-word-representations-and-the-wittgenstein-philosophy-of-language/</a>
</div>
<div id="ref-Peters_2017_Elements_of_Causal_Inference" class="csl-entry" role="doc-biblioentry">
Peters, J., Janzing, D., &amp; Scholkopf, B. (2017). <em><span class="nocase">Elements of Causal Inference</span></em>. <span>MIT Press</span>.
</div>
<div id="ref-Phuong_2022_Formal_algorithms_for_transformers" class="csl-entry" role="doc-biblioentry">
Phuong, M. &amp; Hutter, M. (2022). <span class="nocase">Formal algorithms for transformers</span>. <a href="https://arxiv.org/abs/2207.09238">https://arxiv.org/abs/2207.09238</a>
</div>
<div id="ref-Piantadosi_2023_Modern_language_models_refute_Chomskys_approach" class="csl-entry" role="doc-biblioentry">
Piantadosi, S. T. (2023). <span class="nocase">Modern language models refute Chomsky&#x2019;s approach to language</span>. <a href="https://lingbuzz.net/lingbuzz/007180">https://lingbuzz.net/lingbuzz/007180</a>
</div>
<div id="ref-Ponsen_2011_Computing_approximate_Nash_equilibria_and_robust" class="csl-entry" role="doc-biblioentry">
Ponsen, M., De Jong, S., &amp; Lanctot, M. (2011). <span class="nocase">Computing approximate Nash equilibria and robust best-responses using sampling.</span> <em>Journal of Artificial Intelligence Research</em>, <em><span>42</span></em>, 575&#x2013;605. <a href="https://arxiv.org/abs/1401.4591">https://arxiv.org/abs/1401.4591</a>
</div>
<div id="ref-Radford_2019_Language_models_are_unsupervised_multitask" class="csl-entry" role="doc-biblioentry">
Radford, A. et al. (2019). <span class="nocase">Language models are unsupervised multitask learners</span>. (Paper on the GPT-2 model by OpenAI). <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf">https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf</a>
</div>
<div id="ref-Radford_2018_Improving_language_understanding_by_generative" class="csl-entry" role="doc-biblioentry">
Radford, A., Narasimhan, K., Salimans, T., &amp; Sutskever, I. (2018). <span class="nocase">Improving language understanding by generative pre-training</span>. (Paper on the GPT model by OpenAI). <a href="https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf">https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a>
</div>
<div id="ref-Rae_2022_Scaling_language_models_Methods_analysis" class="csl-entry" role="doc-biblioentry">
Rae, J.W. et al. (2022). <span class="nocase">Scaling language models: Methods, analysis &amp; insights from training Gopher</span>. <a href="https://arxiv.org/abs/2112.11446">https://arxiv.org/abs/2112.11446</a>
</div>
<div id="ref-Raffel_2019_Exploring_the_limits_of_transfer_learning" class="csl-entry" role="doc-biblioentry">
Raffel, C. et al. (2019). <span class="nocase">Exploring the limits of transfer learning with a unified text-to-text transformer</span>. <a href="https://arxiv.org/abs/1910.10683">https://arxiv.org/abs/1910.10683</a>
</div>
<div id="ref-Raissi_2017_Physics_informed_deep_learning_Part_I_Data" class="csl-entry" role="doc-biblioentry">
Raissi, M., Perdikaris, P., &amp; Karniadakis, G. E. (2017a). <span class="nocase">Physics informed deep learning (Part I): Data-driven solutions of nonlinear partial differential equations</span>. <a href="https://arxiv.org/abs/1711.10561">https://arxiv.org/abs/1711.10561</a>
</div>
<div id="ref-Raissi_2017_Physics_informed_deep_learning_Part_II_Data" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2017b). <span class="nocase">Physics informed deep learning (Part II): Data-driven discovery of nonlinear partial differential equations</span>. <a href="https://arxiv.org/abs/1711.10566">https://arxiv.org/abs/1711.10566</a>
</div>
<div id="ref-Rao_1945_Information_and_the_accuracy_attainable" class="csl-entry" role="doc-biblioentry">
Rao, C. R. (1945). <span class="nocase">Information and the accuracy attainable in the estimation of statistical parameters.</span> <em>Bulletin of the Calcutta Mathematical Society</em>, <em><span>37</span></em>, 81&#x2013;91.
</div>
<div id="ref-Rao_1947_Minimum_variance_and_the_estimation_of_several" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1947). <span class="nocase">Minimum variance and the estimation of several parameters</span>. In <em><span class="nocase">Mathematical Proceedings of the Cambridge Philosophical Society</span></em>. <span>43, 280&#x2013;283</span>. Cambridge University Press.
</div>
<div id="ref-Rao_2016_Testing_point_null_hypothesis_of_a_normal_mean" class="csl-entry" role="doc-biblioentry">
Rao, C. R. &amp; Lovric, M. M. (2016). <span class="nocase">Testing point null hypothesis of a normal mean and the truth: 21st century perspective.</span> <em>Journal of Modern Applied Statistical Methods</em>, <em><span>15</span></em>, 2&#x2013;21. <a href="http://digitalcommons.wayne.edu/jmasm/vol15/iss2/3">http://digitalcommons.wayne.edu/jmasm/vol15/iss2/3</a>
</div>
<div id="ref-Rathmanner_2011_A_philosophical_treatise_of_universal_induction" class="csl-entry" role="doc-biblioentry">
Rathmanner, S. &amp; Hutter, M. (2011). <span class="nocase">A philosophical treatise of universal induction.</span> <em>Entropy</em>, <em><span>13</span></em>, 1076&#x2013;1136. <a href="https://www.mdpi.com/1099-4300/13/6/1076">https://www.mdpi.com/1099-4300/13/6/1076</a>
</div>
<div id="ref-Read_2002_Presentation_of_search_results_the_CLs_technique" class="csl-entry" role="doc-biblioentry">
Read, A. L. (2002). <span class="nocase">Presentation of search results: the CLs technique.</span> <em>Journal of Physics G: Nuclear and Particle Physics</em>, <em><span>28</span></em>, 2693. <a href="https://indico.cern.ch/event/398949/attachments/799330/1095613/The_CLs_Technique.pdf">https://indico.cern.ch/event/398949/attachments/799330/1095613/The_CLs_Technique.pdf</a>
</div>
<div id="ref-Reid_1998_Neyman" class="csl-entry" role="doc-biblioentry">
Reid, C. (1998). <em><span>Neyman</span></em>. <span>Springer-Verlag</span>.
</div>
<div id="ref-Rice_2007_Mathematical_Statistics_and_Data_Analysis" class="csl-entry" role="doc-biblioentry">
Rice, J. A. (2007). <em><span class="nocase">Mathematical Statistics and Data Analysis</span></em> (3rd ed.). <span>Thomson</span>.
</div>
<div id="ref-Roberts_2021_Why_is_AI_hard_and_physics_simple" class="csl-entry" role="doc-biblioentry">
Roberts, D. A. (2021). <span class="nocase">Why is AI hard and physics simple?</span> <a href="https://arxiv.org/abs/2104.00008">https://arxiv.org/abs/2104.00008</a>
</div>
<div id="ref-Roberts_2021_The_Principles_of_Deep_Learning_Theory" class="csl-entry" role="doc-biblioentry">
Roberts, D. A., Yaida, S., &amp; Hanin, B. (2021). <em><span class="nocase">The Principles of Deep Learning Theory: An Effective Theory Approach to Understanding Neural Networks</span></em>. <span>Cambridge University Press</span>. <a href="https://deeplearningtheory.com/PDLT.pdf">https://deeplearningtheory.com/PDLT.pdf</a>
</div>
<div id="ref-Robins_1999_On_the_impossibility_of_inferring_causation_from" class="csl-entry" role="doc-biblioentry">
Robins, J. M. &amp; Wasserman, L. (1999). <span class="nocase">On the impossibility of inferring causation from association without background knowledge</span>. In C. Glymour &amp; G. Cooper (Eds.), <em><span class="nocase">Computation, Causation, and Discovery</span></em> (pp. 305&#x2013;321). <span>AAAI &amp; MIT Press</span>.
</div>
<div id="ref-Ronen_2022_DeepDPM_Deep_clustering_with_an_unknown_number" class="csl-entry" role="doc-biblioentry">
Ronen, M., Finder, S. E., &amp; Freifeld, O. (2022). <span class="nocase">DeepDPM: Deep clustering with an unknown number of clusters</span>. <a href="https://arxiv.org/abs/2203.14309">https://arxiv.org/abs/2203.14309</a>
</div>
<div id="ref-Roughgarden_2016_Twenty_Lectures_on_Algorithmic_Game_Theory" class="csl-entry" role="doc-biblioentry">
Roughgarden, T. (2016). <em><span class="nocase">Twenty Lectures on Algorithmic Game Theory</span></em>. <span>Cambridge University Press</span>.
</div>
<div id="ref-Royall_1997_Statistical_Evidence_A_likelihood_paradigm" class="csl-entry" role="doc-biblioentry">
Royall, R. (1997). <em><span class="nocase">Statistical Evidence: A likelihood paradigm</span></em>. <span>CRC Press</span>.
</div>
<div id="ref-Rozeboom_1960_The_fallacy_of_the_null_hypothesis_significance" class="csl-entry" role="doc-biblioentry">
Rozeboom, W. W. (1960). <span class="nocase">The fallacy of the null-hypothesis significance test.</span> <em>Psychological Bulletin</em>, <em><span>57</span></em>, 416.
</div>
<div id="ref-Rubin_1974_Estimating_causal_effects_of_treatments" class="csl-entry" role="doc-biblioentry">
Rubin, D. B. (1974). <span class="nocase">Estimating causal effects of treatments in randomized and nonrandomized studies.</span> <em>Journal of Educational Psychology</em>, <em><span>66</span></em>, 688. <a href="https://psycnet.apa.org/fulltext/1975-06502-001.pdf">https://psycnet.apa.org/fulltext/1975-06502-001.pdf</a>
</div>
<div id="ref-Rumelhart_1986_Learning_representations_by_back_propagating" class="csl-entry" role="doc-biblioentry">
Rumelhart, D. E., Hinton, G. E., &amp; Williams, R. J. (1986). <span class="nocase">Learning representations by back-propagating errors.</span> <em>Nature</em>, <em><span>323</span></em>, 533&#x2013;536. <a href="https://www.nature.com/articles/323533a0.pdf">https://www.nature.com/articles/323533a0.pdf</a>
</div>
<div id="ref-Salsburg_2001_The_Lady_Tasting_Tea" class="csl-entry" role="doc-biblioentry">
Salsburg, D. (2001). <em><span>The Lady Tasting Tea</span></em>. <span>Holt</span>.
</div>
<div id="ref-Savage_1954_The_Foundations_of_Statistics" class="csl-entry" role="doc-biblioentry">
Savage, L. J. (1954). <em><span class="nocase">The Foundations of Statistics</span></em>. <span>John Wiley &amp; Sons</span>.
</div>
<div id="ref-Schaeffer_2023_Double_descent_demystified_Identifying" class="csl-entry" role="doc-biblioentry">
Schaeffer, R. et al. (2023). <span class="nocase">Double descent demystified: Identifying, interpreting &amp; ablating the sources of a deep learning puzzle</span>. <a href="https://arxiv.org/abs/2303.14151">https://arxiv.org/abs/2303.14151</a>
</div>
<div id="ref-Schmid_2019_Variance_reduction_in_Monte_Carlo_counterfactual" class="csl-entry" role="doc-biblioentry">
Schmid, M. et al. (2019). <span class="nocase">Variance reduction in Monte Carlo counterfactual regret minimization (VR-MCCFR) for extensive form games using baselines</span>. <a href="https://ojs.aaai.org/index.php/AAAI/article/view/4048/3926">https://ojs.aaai.org/index.php/AAAI/article/view/4048/3926</a>
</div>
<div id="ref-Schmid_2021_Player_of_games" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2021). <span class="nocase">Player of games</span>. <a href="https://arxiv.org/abs/2112.03178">https://arxiv.org/abs/2112.03178</a>
</div>
<div id="ref-Shalev_Shwarz_2014_Understanding_Machine_Learning_From_Theory" class="csl-entry" role="doc-biblioentry">
Shalev-Shwarz, S. &amp; Ben-David, S. (2014). <em><span class="nocase">Understanding Machine Learning: From Theory to Algorithms</span></em>. <span>Cambridge University Press</span>. <a href="https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf">https://www.cs.huji.ac.il/w~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf</a>
</div>
<div id="ref-Silver_2016_Mastering_the_game_of_Go_with_deep_neural_networks" class="csl-entry" role="doc-biblioentry">
Silver, D. et al. (2016). <span class="nocase">Mastering the game of Go with deep neural networks and tree search.</span> <em>Nature</em>, <em><span>529</span></em>, 484&#x2013;489.
</div>
<div id="ref-Silver_2017_Mastering_chess_and_shogi_by_self_play" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2017a). <span class="nocase">Mastering chess and shogi by self-play with a general reinforcement learning algorithm</span>. <a href="https://arxiv.org/abs/1712.01815">https://arxiv.org/abs/1712.01815</a>
</div>
<div id="ref-Silver_2017_Mastering_the_game_of_Go_without_human_knowledge" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2017b). <span class="nocase">Mastering the game of Go without human knowledge.</span> <em>Nature</em>, <em><span>550</span></em>, 354&#x2013;359.
</div>
<div id="ref-Simonyan_2014_Very_deep_convolutional_networks_for_large_scale" class="csl-entry" role="doc-biblioentry">
Simonyan, K. &amp; Zisserman, A. (2014). <span class="nocase">Very deep convolutional networks for large-scale image recognition</span>. <a href="https://arxiv.org/abs/1409.1556">https://arxiv.org/abs/1409.1556</a>
</div>
<div id="ref-Sinervo_2002_Signal_significance_in_particle_physics" class="csl-entry" role="doc-biblioentry">
Sinervo, P. (2002). <span class="nocase">Signal significance in particle physics</span>. In M. R. Whalley &amp; L. Lyons (Eds.), <em><span class="nocase">Proceedings of the Conference on Advanced Statistical Techniques in Particle Physics</span></em>. Durham, UK: <span>Institute of Particle Physics Phenomenology</span>. <a href="https://arxiv.org/abs/hep-ex/0208005v1">https://arxiv.org/abs/hep-ex/0208005v1</a>
</div>
<div id="ref-Sinervo_2003_Definition_and_treatment_of_systematic" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2003). <span class="nocase">Definition and treatment of systematic uncertainties in high energy physics and astrophysics</span>. In Lyons L., Mount R., &amp; R. Reitmeyer (Eds.), <em><span class="nocase">Proceedings of the Conference on Statistical Problems in Particle Physics, Astrophysics, and Cosmology (PhyStat2003)</span></em> (pp. 122&#x2013;129). <span>Stanford Linear Accelerator Center</span>. <a href="https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf">https://www.slac.stanford.edu/econf/C030908/papers/TUAT004.pdf</a>
</div>
<div id="ref-Singh_2022_Phenomenology_of_double_descent_in_finite_width" class="csl-entry" role="doc-biblioentry">
Singh, S. P., Lucchi, A., Hofmann, T., &amp; Sch&#xF6;lkopf, B. (2022). <span class="nocase">Phenomenology of double descent in finite-width neural networks</span>. <a href="https://arxiv.org/abs/2203.07337">https://arxiv.org/abs/2203.07337</a>
</div>
<div id="ref-Skelac_2020_Meaning_as_use_From_Wittgenstein_to_Googles" class="csl-entry" role="doc-biblioentry">
Skelac, I. &amp; Jandric, A. (2020). <span class="nocase">Meaning as use: From Wittgenstein to Google&#x2019;s Word2vec</span>. In S. Skansi (Ed.), <em><span>Guide To Deep Learning Basics: Logical, Historical And Philosophical Perspectives</span></em> (pp. 41&#x2013;53). <span>Springer</span>.
</div>
<div id="ref-Slonim_2005_Information_based_clustering" class="csl-entry" role="doc-biblioentry">
Slonim, N., Atwal, G. S., Tkacik, G., &amp; Bialek, W. (2005). <span class="nocase">Information-based clustering.</span> <em>Proceedings of the National Academy of Sciences</em>, <em><span>102</span></em>, 18297&#x2013;18302. <a href="https://arxiv.org/abs/q-bio/0511043">https://arxiv.org/abs/q-bio/0511043</a>
</div>
<div id="ref-Smith_2019_A_gentle_introduction_to_information_geometry" class="csl-entry" role="doc-biblioentry">
Smith, L. (2019). <span class="nocase">A gentle introduction to information geometry</span>. September 27, 2019. <a href="http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html">http://www.robots.ox.ac.uk/~lsgs/posts/2019-09-27-info-geom.html</a>
</div>
<div id="ref-Sohl_Dickstein_2020_Two_equalities_expressing_the_determinant" class="csl-entry" role="doc-biblioentry">
Sohl-Dickstein, J. (2020). <span class="nocase">Two equalities expressing the determinant of a matrix in terms of expectations over matrix-vector products</span>. <a href="https://arxiv.org/abs/2005.06553">https://arxiv.org/abs/2005.06553</a>
</div>
<div id="ref-Solomonoff_2016_Ray_Solomonoff_and_the_Dartmouth_Summer_Research" class="csl-entry" role="doc-biblioentry">
Solomonoff, G. (2016). <span class="nocase">Ray Solomonoff and the Dartmouth Summer Research Project in Artificial Intelligence, 1956</span>. <a href="http://raysolomonoff.com/dartmouth/dartray.pdf">http://raysolomonoff.com/dartmouth/dartray.pdf</a>
</div>
<div id="ref-Southey_2012_Bayes_bluff_Opponent_modelling_in_poker" class="csl-entry" role="doc-biblioentry">
Southey, F. et al. (2012). <span class="nocase">Bayes&#x2019; bluff: Opponent modelling in poker</span>. <a href="https://arxiv.org/abs/1207.1411">https://arxiv.org/abs/1207.1411</a>
</div>
<div id="ref-Spears_2018_Deep_learning_A_guide_for_practitioners" class="csl-entry" role="doc-biblioentry">
Spears, B.K. et al. (2018). <span class="nocase">Deep learning: A guide for practitioners in the physical sciences.</span> <em>Physics of Plasmas</em>, <em><span>25</span></em>, 080901.
</div>
<div id="ref-Stahlberg_2019_Neural_machine_translation_A_review" class="csl-entry" role="doc-biblioentry">
Stahlberg, F. (2019). <span class="nocase">Neural machine translation: A review</span>. <a href="https://arxiv.org/abs/1912.02047">https://arxiv.org/abs/1912.02047</a>
</div>
<div id="ref-Stein_1956_Inadmissibility_of_the_usual_estimator" class="csl-entry" role="doc-biblioentry">
Stein, C. (1956). <span class="nocase">Inadmissibility of the usual estimator for the mean of a multivariate normal distribution.</span> <em>Proceedings of the Third Berkeley Symposium on Mathematical Statistics and Probability</em>, <em><span>1</span></em>, 197&#x2013;206.
</div>
<div id="ref-Steinhardt_2012_Beyond_Bayesians_and_frequentists" class="csl-entry" role="doc-biblioentry">
Steinhardt, J. (2012). <span class="nocase">Beyond Bayesians and frequentists</span>. <a href="https://jsteinhardt.stat.berkeley.edu/files/stats-essay.pdf">https://jsteinhardt.stat.berkeley.edu/files/stats-essay.pdf</a>
</div>
<div id="ref-Steinhardt_2022_More_is_different_for_AI" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2022). <span class="nocase">More is different for AI</span>. <a href="https://bounded-regret.ghost.io/more-is-different-for-ai/">https://bounded-regret.ghost.io/more-is-different-for-ai/</a>
</div>
<div id="ref-Stuart_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2A" class="csl-entry" role="doc-biblioentry">
Stuart, A., Ord, K., &amp; Arnold, S. (2010). <em><span class="nocase">Kendall&#x2019;s Advanced Theory of Statistics, Vol 2A: Classical Inference and the Linear Model</span></em>. <span>Wiley</span>.
</div>
<div id="ref-Sun_2023_Retentive_network_A_successor_to_transformer" class="csl-entry" role="doc-biblioentry">
Sun, Y. et al. (2023). <span class="nocase">Retentive network: A successor to transformer for large language models</span>. <a href="https://arxiv.org/abs/2307.08621">https://arxiv.org/abs/2307.08621</a>
</div>
<div id="ref-Sutskever_2015_A_brief_overview_of_deep_learning" class="csl-entry" role="doc-biblioentry">
Sutskever, I. (2015). <span class="nocase">A brief overview of deep learning</span>. <a href="https://web.archive.org/web/20220728224752/http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html">https://web.archive.org/web/20220728224752/http://yyue.blogspot.com/2015/01/a-brief-overview-of-deep-learning.html</a>
</div>
<div id="ref-Sutskever_2014_Sequence_to_sequence_learning_with_neural" class="csl-entry" role="doc-biblioentry">
Sutskever, I., Vinyals, O., &amp; Le, Q. V. (2014). <span class="nocase">Sequence to sequence learning with neural networks.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>2014</span></em>, 3104&#x2013;3112. <a href="https://arxiv.org/abs/1409.3215">https://arxiv.org/abs/1409.3215</a>
</div>
<div id="ref-Sutton_2019_The_bitter_lesson" class="csl-entry" role="doc-biblioentry">
Sutton, R. S. (2019). <span class="nocase">The bitter lesson</span>. <a href="http://www.incompleteideas.net/IncIdeas/BitterLesson.html">http://www.incompleteideas.net/IncIdeas/BitterLesson.html</a>
</div>
<div id="ref-Sutton_2018_Reinforcement_Learning" class="csl-entry" role="doc-biblioentry">
Sutton, R. S. &amp; Barto, A. G. (2018). <em><span>Reinforcement Learning</span></em> (2nd ed.). <span>MIT Press</span>.
</div>
<div id="ref-Sznajder_2018_Inductive_logic_as_explication_The_evolution" class="csl-entry" role="doc-biblioentry">
Sznajder, M. (2018). <span class="nocase">Inductive logic as explication: The evolution of Carnap&#x2019;s notion of logical probability.</span> <em>The Monist</em>, <em><span>101</span></em>, 417&#x2013;440.
</div>
<div id="ref-Tammelin_2014_Solving_large_imperfect_information_games_using" class="csl-entry" role="doc-biblioentry">
Tammelin, O. (2014). <span class="nocase">Solving large imperfect information games using CFR+</span>. <a href="https://arxiv.org/abs/1407.5042">https://arxiv.org/abs/1407.5042</a>
</div>
<div id="ref-Tammelin_2015_Solving_heads_up_limit_texas_holdem" class="csl-entry" role="doc-biblioentry">
Tammelin, O., Burch, N., Johanson, M., &amp; Bowling, M. (2015). <span class="nocase">Solving heads-up limit texas hold&#x2019;em.</span> <em>International Joint Conference on Artificial Intelligence</em>, <em><span>24</span></em>. <a href="http://johanson.ca/publications/poker/2015-ijcai-cfrplus/2015-ijcai-cfrplus.pdf">http://johanson.ca/publications/poker/2015-ijcai-cfrplus/2015-ijcai-cfrplus.pdf</a>
</div>
<div id="ref-Tan_2019_EfficientNet_Rethinking_model_scaling" class="csl-entry" role="doc-biblioentry">
Tan, M. &amp; Le, Q. V. (2019). <span class="nocase">EfficientNet: Rethinking model scaling for convolutional neural networks</span>. <a href="https://arxiv.org/abs/1905.11946">https://arxiv.org/abs/1905.11946</a>
</div>
<div id="ref-Tan_2021_EfficientNetV2_Smaller_models_and_faster_training" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2021). <span class="nocase">EfficientNetV2: Smaller models and faster training</span>. <a href="https://arxiv.org/abs/2104.00298">https://arxiv.org/abs/2104.00298</a>
</div>
<div id="ref-Tay_2022_Efficient_transformers_A_survey" class="csl-entry" role="doc-biblioentry">
Tay, Y., Dehghani, M., Bahri, D., &amp; Metzler, D. (2022). <span class="nocase">Efficient transformers: A survey</span>. <a href="https://arxiv.org/abs/2009.06732">https://arxiv.org/abs/2009.06732</a>
</div>
<div id="ref-Tegmark_1997_Karhunen_Loeve_eigenvalue_problems_in_cosmology" class="csl-entry" role="doc-biblioentry">
Tegmark, M., Taylor, A. N., &amp; Heavens, A. F. (1997). <span class="nocase">Karhunen-Loeve eigenvalue problems in cosmology: How should we tackle large data sets?</span> <em>The Astrophysical Journal</em>, <em><span>480</span></em>, 22&#x2013;35. <a href="https://arxiv.org/abs/astro-ph/9603021">https://arxiv.org/abs/astro-ph/9603021</a>
</div>
<div id="ref-Tenney_2019_What_do_you_learn_from_context" class="csl-entry" role="doc-biblioentry">
Tenney, I. et al. (2019). <span class="nocase">What do you learn from context?</span> <span>Probing for sentence structure in contextualized word representations</span>. <a href="https://arxiv.org/abs/1905.06316">https://arxiv.org/abs/1905.06316</a>
</div>
<div id="ref-Thuerey_2021_Physics_based_deep_learning" class="csl-entry" role="doc-biblioentry">
Thuerey, N. et al. (2021). <span class="nocase">Physics-based deep learning</span>. <a href="https://arxiv.org/abs/2109.05237">https://arxiv.org/abs/2109.05237</a>
</div>
<div id="ref-Timbers_2020_Approximate_exploitability_Learning_a_best" class="csl-entry" role="doc-biblioentry">
Timbers, F. (2020). <span class="nocase">Approximate exploitability: Learning a best response in large games</span>. <a href="https://arxiv.org/abs/2004.09677">https://arxiv.org/abs/2004.09677</a>
</div>
<div id="ref-Tukey_1977_Exploratory_Data_Analysis" class="csl-entry" role="doc-biblioentry">
Tukey, J. W. (1977). <em><span>Exploratory Data Analysis</span></em>. <span>Pearson</span>.
</div>
<div id="ref-Udrescu_2020_Symbolic_pregression_Discovering_physical_laws" class="csl-entry" role="doc-biblioentry">
Udrescu, S. &amp; Tegmark, M. (2020). <span class="nocase">Symbolic pregression: Discovering physical laws from raw distorted video</span>. <a href="https://arxiv.org/abs/2005.11212">https://arxiv.org/abs/2005.11212</a>
</div>
<div id="ref-vanHandel_2016_Probability_in_high_dimensions" class="csl-entry" role="doc-biblioentry">
van Handel, R. (2016). <span class="nocase">Probability in high dimensions</span>. Lecture notes at Princeton. <a href="https://web.math.princeton.edu/~rvan/APC550.pdf">https://web.math.princeton.edu/~rvan/APC550.pdf</a>
</div>
<div id="ref-Vapnik_1994_Measuring_the_VC_dimension_of_a_learning_machine" class="csl-entry" role="doc-biblioentry">
Vapnik, V., Levin, E., &amp; LeCun, Y. (1994). <span class="nocase">Measuring the VC-dimension of a learning machine.</span> <em>Neural Computation</em>, <em><span>6</span></em>, 851&#x2013;876.
</div>
<div id="ref-Vaswani_2017_Attention_is_all_you_need" class="csl-entry" role="doc-biblioentry">
Vaswani, A. et al. (2017). <span class="nocase">Attention is all you need.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>2017</span></em>, 5998&#x2013;6008. <a href="https://arxiv.org/abs/1706.03762">https://arxiv.org/abs/1706.03762</a>
</div>
<div id="ref-Venn_1888_The_Logic_of_Chance" class="csl-entry" role="doc-biblioentry">
Venn, J. (1888). <em><span class="nocase">The Logic of Chance</span></em>. London: <span>MacMillan and Co</span>. (Originally published in 1866).
</div>
<div id="ref-Vershynin_2018_High_Dimensional_Probability_An_Introduction" class="csl-entry" role="doc-biblioentry">
Vershynin, R. (2018). <em><span class="nocase">High-Dimensional Probability: An Introduction with Applications in Data Science</span></em>. <span>Cambridge University Press</span>. <a href="https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf">https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf</a>
</div>
<div id="ref-Wainer_2007_The_most_dangerous_equation" class="csl-entry" role="doc-biblioentry">
Wainer, H. (2007). <span class="nocase">The most dangerous equation.</span> <em>American Scientist</em>, <em><span>95</span></em>, 249&#x2013;256. <a href="https://sites.stat.washington.edu/people/peter/498.Sp16/Equation.pdf">https://sites.stat.washington.edu/people/peter/498.Sp16/Equation.pdf</a>
</div>
<div id="ref-Wakefield_2013_Bayesian_and_Frequentist_Regression_Methods" class="csl-entry" role="doc-biblioentry">
Wakefield, J. (2013). <em><span class="nocase">Bayesian and Frequentist Regression Methods</span></em>. <span>Springer</span>.
</div>
<div id="ref-Wald_1943_Tests_of_statistical_hypotheses_concerning_several" class="csl-entry" role="doc-biblioentry">
Wald, A. (1943). <span class="nocase">Tests of statistical hypotheses concerning several parameters when the number of observations is large.</span> <em>Transactions of the American Mathematical Society</em>, <em><span>54</span></em>, 426&#x2013;482. <a href="https://www.ams.org/journals/tran/1943-054-03/S0002-9947-1943-0012401-3/S0002-9947-1943-0012401-3.pdf">https://www.ams.org/journals/tran/1943-054-03/S0002-9947-1943-0012401-3/S0002-9947-1943-0012401-3.pdf</a>
</div>
<div id="ref-Wang_2023_BitNet_Scaling_1_bit_transformers_for_large" class="csl-entry" role="doc-biblioentry">
Wang, H. et al. (2023). <span class="nocase">BitNet: Scaling 1-bit transformers for large language models</span>. <a href="https://arxiv.org/abs/2310.11453">https://arxiv.org/abs/2310.11453</a>
</div>
<div id="ref-Wasserman_2003_All_of_Statistics_A_Concise_Course_in_Statistical" class="csl-entry" role="doc-biblioentry">
Wasserman, L. (2003). <em><span class="nocase">All of Statistics: A Concise Course in Statistical Inference</span></em>. <span>Springer</span>.
</div>
<div id="ref-Wasserstein_2019_Moving_to_a_World_Beyond_p005" class="csl-entry" role="doc-biblioentry">
Wasserstein, R. L., Allen, L. S., &amp; Lazar, N. A. (2019). <span class="nocase">Moving to a World Beyond "p&lt;0.05".</span> <em>American Statistician</em>, <em><span>73</span></em>, 1&#x2013;19.
</div>
<div id="ref-Wasserstein_2016_The_ASAs_statement_on_p_values_Context_process" class="csl-entry" role="doc-biblioentry">
Wasserstein, R. L. &amp; Lazar, N. A. (2016). <span class="nocase">The ASA&#x2019;s statement on p-values: Context, process, and purpose.</span> <em>American Statistician</em>, <em><span>70</span></em>, 129&#x2013;133.
</div>
<div id="ref-Watson_2019_The_explanation_game_A_formal_framework" class="csl-entry" role="doc-biblioentry">
Watson, D. &amp; Floridi, L. (2019). <span class="nocase">The explanation game: A formal framework for interpretable machine learning.</span> <em>SSRN</em>, <em><span>3509737</span></em>. <a href="https://ssrn.com/abstract=3509737">https://ssrn.com/abstract=3509737</a>
</div>
<div id="ref-Weisberg_2019_Odds__Ends_Introducing_Probability__Decision" class="csl-entry" role="doc-biblioentry">
Weisberg, J. (2019). <em><span class="nocase">Odds &amp; Ends: Introducing Probability &amp; Decision with a Visual Emphasis</span></em>. <a href="https://jonathanweisberg.org/vip/">https://jonathanweisberg.org/vip/</a>
</div>
<div id="ref-Werbos_1990_Backpropagation_through_time_what_it_does_and_how" class="csl-entry" role="doc-biblioentry">
Werbos, P. J. (1990). <span class="nocase">Backpropagation through time: what it does and how to do it.</span> <em>Proceedings of the IEEE</em>, <em><span>78</span></em>, 1550&#x2013;1560. <a href="http://www.werbos.com/Neural/BTT.pdf">http://www.werbos.com/Neural/BTT.pdf</a>
</div>
<div id="ref-Wilks_1938_The_large_sample_distribution_of_the_likelihood" class="csl-entry" role="doc-biblioentry">
Wilks, S. S. (1938). <span class="nocase">The large-sample distribution of the likelihood ratio for testing composite hypotheses.</span> <em>The Annals of Mathematical Statistics</em>, <em><span>9</span></em>, 60&#x2013;62. <a href="https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-9/issue-1/The-Large-Sample-Distribution-of-the-Likelihood-Ratio-for-Testing/10.1214/aoms/1177732360.full">https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-9/issue-1/The-Large-Sample-Distribution-of-the-Likelihood-Ratio-for-Testing/10.1214/aoms/1177732360.full</a>
</div>
<div id="ref-Williamson_2009_The_philosophy_of_science_and_its_relation" class="csl-entry" role="doc-biblioentry">
Williamson, J. (2009). <span class="nocase">The philosophy of science and its relation to machine learning</span>. In <em><span class="nocase">Scientific Data Mining and Knowledge Discovery</span></em> (pp. 77&#x2013;89). <span>Springer, Berlin, Heidelberg</span>.
</div>
<div id="ref-Wittgenstein_2009_Philosophical_Investigations" class="csl-entry" role="doc-biblioentry">
Wittgenstein, L. (2009). <em><span>Philosophical Investigations</span></em>. (E. Anscombe &amp; P. Hacker, Trans., P. Hacker &amp; J. Schulte, Eds.) (4th ed.). <span>Wiley-Blackwell</span>. (Originally published in 1953).
</div>
<div id="ref-Wolfram_2023_What_is_ChatGPT_doing_and_why_does_it_work" class="csl-entry" role="doc-biblioentry">
Wolfram, S. (2023). <span class="nocase">What is ChatGPT doing&#x2014;and why does it work?</span> <a href="https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/">https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</a>
</div>
<div id="ref-Wolpert_1996_The_lack_of_a_priori_distinctions_between_learning" class="csl-entry" role="doc-biblioentry">
Wolpert, D. H. (1996). <span class="nocase">The lack of a priori distinctions between learning algorithms.</span> <em>Neural Computation</em>, <em><span>8</span></em>, 1341&#x2013;1390.
</div>
<div id="ref-Wolpert_2007_Physical_limits_of_inference" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (2007). <span class="nocase">Physical limits of inference</span>. <a href="https://arxiv.org/abs/0708.1362">https://arxiv.org/abs/0708.1362</a>
</div>
<div id="ref-Wolpert_2020_Noisy_deductive_reasoning_How_humans_construct" class="csl-entry" role="doc-biblioentry">
Wolpert, D. H. &amp; Kinney, D. (2020). <span class="nocase">Noisy deductive reasoning: How humans construct math, and how math constructs universes</span>. <a href="https://arxiv.org/abs/2012.08298">https://arxiv.org/abs/2012.08298</a>
</div>
<div id="ref-Wolpert_1995_No_free_lunch_theorems_for_search" class="csl-entry" role="doc-biblioentry">
Wolpert, D. H. &amp; Macready, W. G. (1995). <span class="nocase">No free lunch theorems for search</span>. Technical Report SFI-TR-95-02-010, Santa Fe Institute.
</div>
<div id="ref-Wolpert_1997_No_free_lunch_theorems_for_optimization" class="csl-entry" role="doc-biblioentry">
&#x2014;&#x2014;&#x2014;. (1997). <span class="nocase">No free lunch theorems for optimization.</span> <em>IEEE Transactions on Evolutionary Computation</em>, <em><span>1</span></em>, 67&#x2013;82.
</div>
<div id="ref-Wu_2016_Googles_neural_machine_translation_system" class="csl-entry" role="doc-biblioentry">
Wu, Y. et al. (2016). <span class="nocase">Google&#x2019;s neural machine translation system: Bridging the gap between human and machine translation</span>. <a href="https://arxiv.org/abs/1409.0473">https://arxiv.org/abs/1409.0473</a>
</div>
<div id="ref-Yang_2023_Dropout_drops_double_descent" class="csl-entry" role="doc-biblioentry">
Yang, T. &amp; Suzuki, J. (2023). <span class="nocase">Dropout drops double descent</span>. <a href="https://arxiv.org/abs/2305.16179">https://arxiv.org/abs/2305.16179</a>
</div>
<div id="ref-Yang_2019_XLNet_Generalized_autoregressive_pretraining" class="csl-entry" role="doc-biblioentry">
Yang, Z. et al. (2019). <span class="nocase">XLNet: Generalized autoregressive pretraining for language understanding</span>. <a href="https://arxiv.org/abs/1906.08237">https://arxiv.org/abs/1906.08237</a>
</div>
<div id="ref-Zaheer_2020_Big_Bird_Transformers_for_longer_sequences" class="csl-entry" role="doc-biblioentry">
Zaheer, M. et al. (2020). <span class="nocase">Big Bird: Transformers for longer sequences</span>. <a href="https://arxiv.org/abs/2007.14062">https://arxiv.org/abs/2007.14062</a>
</div>
<div id="ref-Zech_1995_Comparing_statistical_data_to_Monte_Carlo" class="csl-entry" role="doc-biblioentry">
Zech, G. (1995). <span class="nocase">Comparing statistical data to Monte Carlo simulation: Parameter fitting and unfolding</span>. (DESY-95-113). Deutsches Elektronen-Synchrotron (DESY). <a href="https://cds.cern.ch/record/284321">https://cds.cern.ch/record/284321</a>
</div>
<div id="ref-Zhang_1998_Complete_anytime_beam_search" class="csl-entry" role="doc-biblioentry">
Zhang, W. (1998). <span class="nocase">Complete anytime beam search.</span> <em>AAAI Proceedings</em>, <em><span>98</span></em>, 425&#x2013;430. <a href="https://cdn.aaai.org/AAAI/1998/AAAI98-060.pdf">https://cdn.aaai.org/AAAI/1998/AAAI98-060.pdf</a>
</div>
<div id="ref-Zhou_2005_Beam_stack_search_Integrating_backtracking" class="csl-entry" role="doc-biblioentry">
Zhou, R. &amp; Hansen, E. A. (2005). <span class="nocase">Beam-stack search: Integrating backtracking with beam search.</span> <em>ICAPS</em>, <em><span>15</span></em>, 90&#x2013;98. <a href="https://cdn.aaai.org/ICAPS/2005/ICAPS05-010.pdf">https://cdn.aaai.org/ICAPS/2005/ICAPS05-010.pdf</a>
</div>
<div id="ref-Zinkevich_2007_Regret_minimization_in_games_with_incomplete" class="csl-entry" role="doc-biblioentry">
Zinkevich, M., Johanson, M., Bowling, M., &amp; Piccione, C. (2007). <span class="nocase">Regret minimization in games with incomplete information.</span> <em>Advances in Neural Information Processing Systems</em>, <em><span>20</span></em>, 1729&#x2013;1736. <a href="https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf">https://proceedings.neurips.cc/paper/2007/file/08d98638c6fcd194a4b1e6992063e944-Paper.pdf</a>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr />
<ol>
<li id="fn1" role="doc-endnote"><p><span class="citation" data-cites="Edwards_1974_The_history_of_likelihood">Edwards (1974)</span>, p.&#xA0;9.<a href="#fnref1" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn2" role="doc-endnote"><p><span class="citation" data-cites="Hacking_1971_Jacques_Bernoullis_Art_of_conjecturing">Hacking (1971)</span>.<a href="#fnref2" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn3" role="doc-endnote"><p>Bernoulli, J. (1713). <em>Ars Conjectandi</em>, Chapter II, Part IV, defining the art of conjecture [<a href="https://en.wikiquote.org/wiki/Jacob_Bernoulli">wikiquote</a>].<a href="#fnref3" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn4" role="doc-endnote"><p><span class="citation" data-cites="Venn_1888_The_Logic_of_Chance">Venn (1888)</span>.<a href="#fnref4" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn5" role="doc-endnote"><p><span class="citation" data-cites="Peirce_1883_Studies_in_Logic">Peirce (1883)</span>, p.&#xA0;126&#x2013;181.<a href="#fnref5" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn6" role="doc-endnote"><p><span class="citation" data-cites="Pearson_1900_On_the_criterion_that_a_given_system_of_deviations">Pearson (1900)</span>.<a href="#fnref6" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn7" role="doc-endnote"><p><span class="citation" data-cites="Keynes_1921_A_Treatise_on_Probability">Keynes (1921)</span>.<a href="#fnref7" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn8" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1912_On_an_absolute_criterion_for_fitting_frequency">Fisher (1912)</span>.<a href="#fnref8" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn9" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1915_Frequency_distribution_of_the_values">Fisher (1915)</span>.<a href="#fnref9" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn10" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1921_On_the_probable_error_of_a_coefficient">Fisher (1921)</span>.<a href="#fnref10" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn11" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1955_Statistical_methods_and_scientific_induction">Fisher (1955)</span>.<a href="#fnref11" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn12" role="doc-endnote"><p><span class="citation" data-cites="Salsburg_2001_The_Lady_Tasting_Tea">Salsburg (2001)</span>.<a href="#fnref12" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn13" role="doc-endnote"><p><span class="citation" data-cites="Reid_1998_Neyman">Reid (1998)</span>.<a href="#fnref13" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn14" role="doc-endnote"><p><span class="citation" data-cites="Neyman_1955_The_problem_of_inductive_inference">Neyman (1955)</span>.<a href="#fnref14" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn15" role="doc-endnote"><p><span class="citation" data-cites="Stuart_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2A">Stuart, Ord, &amp; Arnold (2010)</span>.<a href="#fnref15" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn16" role="doc-endnote"><p><span class="citation" data-cites="James_2006_Statistical_Methods_in_Experimental_Particle">F. James (2006)</span>.<a href="#fnref16" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn17" role="doc-endnote"><p><span class="citation" data-cites="Cowan_1998_Statistical_Data_Analysis">Cowan (1998)</span> and <span class="citation" data-cites="Cowan_2016_StatisticsIn_CPatrignani_et_alParticle_Data">Cowan (2016)</span>.<a href="#fnref17" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn18" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2015_Practical_statistics_for_the_LHC">Cranmer (2015)</span>.<a href="#fnref18" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn19" role="doc-endnote"><p><span class="citation" data-cites="Lista_2016_Statistical_Methods_for_Data_Analysis_in_Particle">Lista (2016b)</span>.<a href="#fnref19" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn20" role="doc-endnote"><p><span class="citation" data-cites="Lista_2016_Practical_statistics_for_particle_physicists">Lista (2016a)</span>.<a href="#fnref20" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn21" role="doc-endnote"><p><span class="citation" data-cites="Cox_2006_Principles_of_Statistical_Inference">Cox (2006)</span>.<a href="#fnref21" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn22" role="doc-endnote"><p><span class="citation" data-cites="Behnke_2013_Data_Analysis_in_High_Energy_Physics_A_Practical">Behnke, Kr&#xF6;ninger, Schott, &amp; Sch&#xF6;rner-Sadenius (2013)</span>.<a href="#fnref22" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn23" role="doc-endnote"><p><span class="citation" data-cites="Cousins_2018_Lectures_on_statistics_in_theory_Prelude">Cousins (2018)</span>.<a href="#fnref23" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn24" role="doc-endnote"><p><span class="citation" data-cites="Weisberg_2019_Odds__Ends_Introducing_Probability__Decision">Weisberg (2019)</span>.<a href="#fnref24" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn25" role="doc-endnote"><p><span class="citation" data-cites="Gelman_2021_What_are_the_most_important_statistical_ideas">Gelman &amp; Vehtari (2021)</span>.<a href="#fnref25" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn26" role="doc-endnote"><p><span class="citation" data-cites="Otsuka_2023_Thinking_About_Statistics_The_Philosophical">Otsuka (2023)</span>.<a href="#fnref26" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn27" role="doc-endnote"><p><span class="citation" data-cites="Carnap_1947_Probability_as_a_guide_in_life">Carnap (1947)</span>.<a href="#fnref27" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn28" role="doc-endnote"><p><span class="citation" data-cites="Carnap_1953_What_is_probability">Carnap (1953)</span>.<a href="#fnref28" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn29" role="doc-endnote"><p><span class="citation" data-cites="Goodfellow_2016_Deep_Learning">Goodfellow, Bengio, &amp; Courville (2016)</span>, p.&#xA0;72-73.<a href="#fnref29" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn30" role="doc-endnote"><p><span class="citation" data-cites="Cowan_1998_Statistical_Data_Analysis">Cowan (1998)</span>, p.&#xA0;20-22.<a href="#fnref30" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn31" role="doc-endnote"><p><span class="citation" data-cites="Arras_1998_An_introduction_to_error_propagation_Derivation">Arras (1998)</span>.<a href="#fnref31" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn32" role="doc-endnote"><p><span class="citation" data-cites="Fienberg_2006_When_did_Bayesian_inference_become_Bayesian">Fienberg (2006)</span>.<a href="#fnref32" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn33" role="doc-endnote"><p><span class="citation" data-cites="Weisberg_2019_Odds__Ends_Introducing_Probability__Decision">Weisberg (2019)</span>, ch.&#xA0;15.<a href="#fnref33" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn34" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1921_On_the_probable_error_of_a_coefficient">Fisher (1921)</span>, p.&#xA0;15.<a href="#fnref34" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn35" role="doc-endnote"><p><span class="citation" data-cites="Stein_1956_Inadmissibility_of_the_usual_estimator">Stein (1956)</span>.<a href="#fnref35" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn36" role="doc-endnote"><p><span class="citation" data-cites="James_1961_Estimation_with_quadratic_loss">W. James &amp; Stein (1961)</span>.<a href="#fnref36" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn37" role="doc-endnote"><p><span class="citation" data-cites="vanHandel_2016_Probability_in_high_dimensions">van Handel (2016)</span>.<a href="#fnref37" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn38" role="doc-endnote"><p><span class="citation" data-cites="Vershynin_2018_High_Dimensional_Probability_An_Introduction">Vershynin (2018)</span>.<a href="#fnref38" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn39" role="doc-endnote"><p><span class="citation" data-cites="Leemis_2008_Univariate_distribution_relationships">Leemis &amp; McQueston (2008)</span>.<a href="#fnref39" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn40" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2012_HistFactory_A_tool_for_creating_statistical">Cranmer, K. et al. (2012)</span>.<a href="#fnref40" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn41" role="doc-endnote"><p>This assumption that the model models the data &#x201C;reasonably&#x201D; well reflects that to the degree required by your analysis, the important features of the data match well within the systematic uncertainties parametrized within the model. If the model is incomplete because it is missing an important feature of the data, then this is the &#x201C;<em>ugly</em>&#x201D; (class-3) error in the <a href="#sinervo-classification-of-systematic-uncertainties">Sinervo classification of systematic uncertainties</a>.<a href="#fnref41" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn42" role="doc-endnote"><p><span class="citation" data-cites="Cowan_1998_Statistical_Data_Analysis">Cowan (1998)</span> and <span class="citation" data-cites="Cowan_2016_StatisticsIn_CPatrignani_et_alParticle_Data">Cowan (2016)</span>, p.&#xA0;TODO.<a href="#fnref42" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn43" role="doc-endnote"><p><span class="citation" data-cites="Aldrich_1997_RAFisher_and_the_making_of_maximum_likelihood">Aldrich (1997)</span>.<a href="#fnref43" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn44" role="doc-endnote"><p><span class="citation" data-cites="James_2006_Statistical_Methods_in_Experimental_Particle">F. James (2006)</span>, p.&#xA0;234.<a href="#fnref44" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn45" role="doc-endnote"><p><span class="citation" data-cites="Cox_2006_Principles_of_Statistical_Inference">Cox (2006)</span>, p.&#xA0;11.<a href="#fnref45" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn46" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2012_Machine_Learning_A_probabilistic_perspective">Murphy (2012)</span>, p.&#xA0;222.<a href="#fnref46" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn47" role="doc-endnote"><p><span class="citation" data-cites="Frechet_1943_Sur_lextension_de_certaines_evaluations">Fr&#xE9;chet (1943)</span>, <span class="citation" data-cites="Cramer_1946_A_contribution_to_the_theory_of_statistical">Cram&#xE9;r (1946)</span>, <span class="citation" data-cites="Rao_1945_Information_and_the_accuracy_attainable">Rao (1945)</span>, and <span class="citation" data-cites="Rao_1947_Minimum_variance_and_the_estimation_of_several">Rao (1947)</span>.<a href="#fnref47" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn48" role="doc-endnote"><p><span class="citation" data-cites="Rice_2007_Mathematical_Statistics_and_Data_Analysis">Rice (2007)</span>, p.&#xA0;300&#x2013;2.<a href="#fnref48" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn49" role="doc-endnote"><p><span class="citation" data-cites="Nielsen_2013_Cramer_Rao_lower_bound_and_information_geometry">Nielsen (2013)</span>.<a href="#fnref49" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn50" role="doc-endnote"><p><span class="citation" data-cites="Cowan_1998_Statistical_Data_Analysis">Cowan (1998)</span>, p.&#xA0;130-5.<a href="#fnref50" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn51" role="doc-endnote"><p><span class="citation" data-cites="James_2006_Statistical_Methods_in_Experimental_Particle">F. James (2006)</span>, p.&#xA0;234.<a href="#fnref51" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn52" role="doc-endnote"><p><span class="citation" data-cites="James_1975_MINUIT_A_system_for_function_minimization">F. James &amp; Roos (1975)</span>.<a href="#fnref52" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn53" role="doc-endnote"><p><span class="citation" data-cites="Cowan_2012_Asymptotic_distribution_for_two_sided_tests">Cowan, Cranmer, Gross, &amp; Vitells (2012)</span>.<a href="#fnref53" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn54" role="doc-endnote"><p><span class="citation" data-cites="Wainer_2007_The_most_dangerous_equation">Wainer (2007)</span>.<a href="#fnref54" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn55" role="doc-endnote"><p><span class="citation" data-cites="Tegmark_1997_Karhunen_Loeve_eigenvalue_problems_in_cosmology">Tegmark, Taylor, &amp; Heavens (1997)</span>.<a href="#fnref55" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn56" role="doc-endnote"><p><span class="citation" data-cites="Clopper_1934_The_use_of_confidence_or_fiducial_limits">Clopper &amp; Pearson (1934)</span>.<a href="#fnref56" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn57" role="doc-endnote"><p><span class="citation" data-cites="Agresti_1998_Approximate_is_better_than_exact_for_interval">Agresti &amp; Coull (1998)</span>.<a href="#fnref57" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn58" role="doc-endnote"><p><span class="citation" data-cites="Hanley_1983_If_nothing_goes_wrong_is_everything_all_right">Hanley &amp; Lippman-Hand (1983)</span>.<a href="#fnref58" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn59" role="doc-endnote"><p><span class="citation" data-cites="Brown_2001_Interval_estimation_for_a_binomial_proportion">L. D. Brown, Cai, &amp; DasGupta (2001)</span>.<a href="#fnref59" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn60" role="doc-endnote"><p><span class="citation" data-cites="Casadei_2012_Estimating_the_selection_efficiency">Casadei (2012)</span>.<a href="#fnref60" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn61" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1935_The_Design_of_Experiments">Fisher (1935)</span>, p.&#xA0;16.<a href="#fnref61" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn62" role="doc-endnote"><p><span class="citation" data-cites="Goodman_1999_Toward_evidence_based_medical_statistics_1_The_P">Goodman (1999a)</span>. p.&#xA0;998.<a href="#fnref62" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn63" role="doc-endnote"><p><span class="citation" data-cites="ATLAS_2011_Procedure_for_the_LHC_Higgs_boson_search">ATLAS and CMS Collaborations (2011)</span>.<a href="#fnref63" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn64" role="doc-endnote"><p><span class="citation" data-cites="Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">Cowan, Cranmer, Gross, &amp; Vitells (2011)</span>.<a href="#fnref64" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn65" role="doc-endnote"><p><span class="citation" data-cites="Neyman_1933_On_the_problem_of_the_most_efficient_tests">Neyman &amp; Pearson (1933)</span>.<a href="#fnref65" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn66" role="doc-endnote"><p><span class="citation" data-cites="Feldman_1998_A_unified_approach_to_the_classical_statistical">Feldman &amp; Cousins (1998)</span>.<a href="#fnref66" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn67" role="doc-endnote"><p><span class="citation" data-cites="Sinervo_2002_Signal_significance_in_particle_physics">Sinervo (2002)</span> and <span class="citation" data-cites="Cowan_2012_Discovery_sensitivity_for_a_counting_experiment">Cowan (2012)</span>.<a href="#fnref67" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn68" role="doc-endnote"><p><span class="citation" data-cites="Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">Cowan et al. (2011)</span>, p.&#xA0;2&#x2013;3.<a href="#fnref68" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn69" role="doc-endnote"><p><span class="citation" data-cites="Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">Cowan et al. (2011)</span>, p.&#xA0;3.<a href="#fnref69" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn70" role="doc-endnote"><p><span class="citation" data-cites="Cousins_1992_Incorporating_systematic_uncertainties_into">Cousins &amp; Highland (1992)</span>.<a href="#fnref70" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn71" role="doc-endnote"><p><span class="citation" data-cites="Junk_1999_Confidence_level_computation_for_combining">Junk (1999)</span>.<a href="#fnref71" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn72" role="doc-endnote"><p><span class="citation" data-cites="Read_2002_Presentation_of_search_results_the_CLs_technique">Read (2002)</span>.<a href="#fnref72" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn73" role="doc-endnote"><p><span class="citation" data-cites="ATLAS_2011_The_CLs_method_Information_for_conference">ATLAS Statistics Forum (2011)</span>.<a href="#fnref73" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn74" role="doc-endnote"><p><span class="citation" data-cites="Wilks_1938_The_large_sample_distribution_of_the_likelihood">Wilks (1938)</span>.<a href="#fnref74" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn75" role="doc-endnote"><p><span class="citation" data-cites="Wald_1943_Tests_of_statistical_hypotheses_concerning_several">Wald (1943)</span>.<a href="#fnref75" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn76" role="doc-endnote"><p><span class="citation" data-cites="Cowan_2011_Asymptotic_formulae_for_likelihood_based_tests">Cowan et al. (2011)</span>.<a href="#fnref76" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn77" role="doc-endnote"><p><span class="citation" data-cites="Bhattiprolu_2020_Criteria_for_projected_discovery_and_exclusion">Bhattiprolu, Martin, &amp; Wells (2020)</span>.<a href="#fnref77" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn78" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2012_Machine_Learning_A_probabilistic_perspective">Murphy (2012)</span>, p.&#xA0;197.<a href="#fnref78" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn79" role="doc-endnote"><p><span class="citation" data-cites="Goodman_1999_Toward_evidence_based_medical_statistics_2">Goodman (1999b)</span>.<a href="#fnref79" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn80" role="doc-endnote"><p><span class="citation" data-cites="Goodman_1999_Toward_evidence_based_medical_statistics_1_The_P">Goodman (1999a)</span>. p.&#xA0;995.<a href="#fnref80" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn81" role="doc-endnote"><p><span class="citation" data-cites="Sinervo_2003_Definition_and_treatment_of_systematic">Sinervo (2003)</span>.<a href="#fnref81" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn82" role="doc-endnote"><p><span class="citation" data-cites="Heinrich_2007_Systematic_errors">Heinrich &amp; Lyons (2007)</span>.<a href="#fnref82" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn83" role="doc-endnote"><p><span class="citation" data-cites="Caldeira_2020_Deeply_uncertain_comparing_methods_of_uncertainty">Caldeira &amp; Nord (2020)</span>.<a href="#fnref83" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn84" role="doc-endnote"><p><span class="citation" data-cites="Lyons_2008_Open_statistical_issues_in_particle_physics">Lyons (2008)</span>, p.&#xA0;890.<a href="#fnref84" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn85" role="doc-endnote"><p><span class="citation" data-cites="Rubin_1974_Estimating_causal_effects_of_treatments">Rubin (1974)</span>.<a href="#fnref85" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn86" role="doc-endnote"><p><span class="citation" data-cites="Lewis_1981_Causal_decision_theory">Lewis (1981)</span>.<a href="#fnref86" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn87" role="doc-endnote"><p><span class="citation" data-cites="Pearl_2018_The_Book_of_Why_The_new_science_of_cause">Pearl (2018)</span>.<a href="#fnref87" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn88" role="doc-endnote"><p><span class="citation" data-cites="Pearl_2009_Causal_inference_in_statistics_An_overview">Pearl (2009)</span>.<a href="#fnref88" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn89" role="doc-endnote"><p><span class="citation" data-cites="Robins_1999_On_the_impossibility_of_inferring_causation_from">Robins &amp; Wasserman (1999)</span>.<a href="#fnref89" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn90" role="doc-endnote"><p><span class="citation" data-cites="Peters_2017_Elements_of_Causal_Inference">Peters, Janzing, &amp; Scholkopf (2017)</span>.<a href="#fnref90" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn91" role="doc-endnote"><p><span class="citation" data-cites="Lundberg_2021_What_is_your_estimand_Defining_the_target">Lundberg, Johnson, &amp; Stewart (2021)</span>.<a href="#fnref91" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn92" role="doc-endnote"><p><span class="citation" data-cites="Ismael_2023_Reflections_on_the_asymmetry_of_causation">Ismael (2023)</span>.<a href="#fnref92" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn93" role="doc-endnote"><p><span class="citation" data-cites="Tukey_1977_Exploratory_Data_Analysis">Tukey (1977)</span>.<a href="#fnref93" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn94" role="doc-endnote"><p><span class="citation" data-cites="Chen_2018_Open_is_not_enough">Chen, X. et al. (2018)</span>.<a href="#fnref94" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn95" role="doc-endnote"><p><span class="citation" data-cites="Carnap_1945_The_two_concepts_of_probability">Carnap (1945)</span>.<a href="#fnref95" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn96" role="doc-endnote"><p><span class="citation" data-cites="Royall_1997_Statistical_Evidence_A_likelihood_paradigm">Royall (1997)</span>, p.&#xA0;171&#x2013;2.<a href="#fnref96" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn97" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2015_Practical_statistics_for_the_LHC">Cranmer (2015)</span>, p.&#xA0;6.<a href="#fnref97" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn98" role="doc-endnote"><p><span class="citation" data-cites="Neyman_1933_On_the_problem_of_the_most_efficient_tests">Neyman &amp; Pearson (1933)</span>.<a href="#fnref98" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn99" role="doc-endnote"><p><span class="citation" data-cites="Kruschke_2018_The_Bayesian_New_Statistics_Hypothesis_testing">Kruschke &amp; Liddell (2018)</span>.<a href="#fnref99" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn100" role="doc-endnote"><p><span class="citation" data-cites="Edwards_1974_The_history_of_likelihood">Edwards (1974)</span>.<a href="#fnref100" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn101" role="doc-endnote"><p><span class="citation" data-cites="Birnbaum_1962_On_the_foundations_of_statistical_inference">Birnbaum (1962)</span>.<a href="#fnref101" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn102" role="doc-endnote"><p><span class="citation" data-cites="Hacking_1965_Logic_of_Statistical_Inference">Hacking (1965)</span>.<a href="#fnref102" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn103" role="doc-endnote"><p><span class="citation" data-cites="Berger_1988_The_Likelihood_Principle">Berger &amp; Wolpert (1988)</span>.<a href="#fnref103" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn104" role="doc-endnote"><p><span class="citation" data-cites="OHagan_2010_Kendalls_Advanced_Theory_of_Statistics_Vol_2B">O&#x2019;Hagan (2010)</span>, p.&#xA0;17&#x2013;18.<a href="#fnref104" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn105" role="doc-endnote"><p><span class="citation" data-cites="Gandenberger_2015_A_new_proof_of_the_likelihood_principle">Gandenberger (2015)</span>.<a href="#fnref105" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn106" role="doc-endnote"><p><span class="citation" data-cites="Evans_2013_What_does_the_proof_of_Birnbaums_theorem_prove">Evans (2013)</span>.<a href="#fnref106" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn107" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2014_On_the_Birnbaum_Argument_for_the_Strong_Likelihood">Mayo (2014)</span>.<a href="#fnref107" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn108" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2019_The_law_of_likelihood_and_error_statistics">Mayo (2019)</span>.<a href="#fnref108" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn109" role="doc-endnote"><p><span class="citation" data-cites="Dawid_2014_Discussion_of_On_the_Birnbaum_Argument">Dawid (2014)</span>.<a href="#fnref109" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn110" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2019_The_law_of_likelihood_and_error_statistics">Mayo (2019)</span>.<a href="#fnref110" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn111" role="doc-endnote"><p><span class="citation" data-cites="Lyons_2008_Open_statistical_issues_in_particle_physics">Lyons (2008)</span>, p.&#xA0;891.<a href="#fnref111" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn112" role="doc-endnote"><p><span class="citation" data-cites="Sznajder_2018_Inductive_logic_as_explication_The_evolution">Sznajder (2018)</span>.<a href="#fnref112" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn113" role="doc-endnote"><p><span class="citation" data-cites="Carnap_1952_The_Continuum_of_Inductive_Methods">Carnap (1952)</span>.<a href="#fnref113" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn114" role="doc-endnote"><p><span class="citation" data-cites="Hacking_1965_Logic_of_Statistical_Inference">Hacking (1965)</span>.<a href="#fnref114" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn115" role="doc-endnote"><p><span class="citation" data-cites="Neyman_1977_Frequentist_probability_and_frequentist">Neyman (1977)</span>.<a href="#fnref115" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn116" role="doc-endnote"><p><span class="citation" data-cites="Rozeboom_1960_The_fallacy_of_the_null_hypothesis_significance">Rozeboom (1960)</span>.<a href="#fnref116" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn117" role="doc-endnote"><p><span class="citation" data-cites="Meehl_1978_Theoretical_risks_and_tabular_asterisks_Sir_Karl">Meehl (1978)</span>.<a href="#fnref117" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn118" role="doc-endnote"><p><span class="citation" data-cites="Zech_1995_Comparing_statistical_data_to_Monte_Carlo">Zech (1995)</span>.<a href="#fnref118" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn119" role="doc-endnote"><p><span class="citation" data-cites="Royall_1997_Statistical_Evidence_A_likelihood_paradigm">Royall (1997)</span>.<a href="#fnref119" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn120" role="doc-endnote"><p><span class="citation" data-cites="Berger_2003_Could_Fisher_Jeffreys_and_Neyman_have_agreed_on">Berger (2003)</span>.<a href="#fnref120" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn121" role="doc-endnote"><p><span class="citation" data-cites="Mayo_1981_In_defense_of_the_Neyman_Pearson_theory">Mayo (1981)</span>.<a href="#fnref121" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn122" role="doc-endnote"><p><span class="citation" data-cites="Mayo_1996_Error_and_the_Growth_of_Experimental_Knowledge">Mayo (1996)</span>.<a href="#fnref122" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn123" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2006_Severe_testing_as_a_basic_concept_in_a_Neyman">Mayo &amp; Spanos (2006)</span>.<a href="#fnref123" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn124" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2011_Error_statistics">Mayo &amp; Spanos (2011)</span>.<a href="#fnref124" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn125" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2018_Statistical_Inference_as_Severe_Testing_How">Mayo (2018)</span>.<a href="#fnref125" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn126" role="doc-endnote"><p><span class="citation" data-cites="Gelman_2017_Beyond_subjective_and_objective_in_statistics">Gelman &amp; Hennig (2017)</span>.<a href="#fnref126" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn127" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2012_Machine_Learning_A_probabilistic_perspective">Murphy (2012)</span>, ch.&#xA0;6.6.<a href="#fnref127" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn128" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2022_Probabilistic_Machine_Learning_An_introduction">Murphy (2022)</span>, p.&#xA0;195&#x2013;198.<a href="#fnref128" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn129" role="doc-endnote"><p><span class="citation" data-cites="Gandenberger_2016_Why_I_am_not_a_likelihoodist">Gandenberger (2016)</span>.<a href="#fnref129" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn130" role="doc-endnote"><p><span class="citation" data-cites="Wakefield_2013_Bayesian_and_Frequentist_Regression_Methods">Wakefield (2013)</span>, ch.&#xA0;4.<a href="#fnref130" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn131" role="doc-endnote"><p><span class="citation" data-cites="Efron_2016_Computer_Age_Statistical_Inference_Algorithms">Efron &amp; Hastie (2016)</span>, p.&#xA0;30&#x2013;36.<a href="#fnref131" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn132" role="doc-endnote"><p><span class="citation" data-cites="Kruschke_2018_The_Bayesian_New_Statistics_Hypothesis_testing">Kruschke &amp; Liddell (2018)</span>.<a href="#fnref132" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn133" role="doc-endnote"><p><span class="citation" data-cites="Steinhardt_2012_Beyond_Bayesians_and_frequentists">Steinhardt (2012)</span>.<a href="#fnref133" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn134" role="doc-endnote"><p><span class="citation" data-cites="Goodman_1999_Toward_evidence_based_medical_statistics_1_The_P">Goodman (1999a)</span>. p.&#xA0;999.<a href="#fnref134" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn135" role="doc-endnote"><p><span class="citation" data-cites="Ioannidis_2005_Why_most_published_research_findings_are_false">Ioannidis (2005)</span>.<a href="#fnref135" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn136" role="doc-endnote"><p><span class="citation" data-cites="Wasserstein_2016_The_ASAs_statement_on_p_values_Context_process">Wasserstein &amp; Lazar (2016)</span>.<a href="#fnref136" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn137" role="doc-endnote"><p><span class="citation" data-cites="Wasserstein_2019_Moving_to_a_World_Beyond_p005">Wasserstein, Allen, &amp; Lazar (2019)</span>.<a href="#fnref137" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn138" role="doc-endnote"><p><span class="citation" data-cites="Benjamin_2017_Redefine_statistical_significance">Benjamin, D.J. et al. (2017)</span>.<a href="#fnref138" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn139" role="doc-endnote"><p><span class="citation" data-cites="Fisher_1935_The_Design_of_Experiments">Fisher (1935)</span>, p.&#xA0;13&#x2013;14.<a href="#fnref139" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn140" role="doc-endnote"><p><span class="citation" data-cites="Rao_2016_Testing_point_null_hypothesis_of_a_normal_mean">Rao &amp; Lovric (2016)</span>.<a href="#fnref140" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn141" role="doc-endnote"><p><span class="citation" data-cites="Mayo_2021_Significance_tests_Vitiated_or_vindicated">Mayo (2021)</span>.<a href="#fnref141" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn142" role="doc-endnote"><p><span class="citation" data-cites="Gorard_2016_What_to_do_instead_of_significance_testing">Gorard &amp; Gorard (2016)</span>.<a href="#fnref142" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn143" role="doc-endnote"><p><span class="citation" data-cites="Benjamini_2021_The_ASA_presidents_task_force_statement_on">Benjamini, Y. et al. (2021)</span>, p.&#xA0;1.<a href="#fnref143" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn144" role="doc-endnote"><p><span class="citation" data-cites="Hastie_2009_The_Elements_of_Statistical_Learning_Data_Mining">Hastie, Tibshirani, &amp; Friedman (2009)</span>.<a href="#fnref144" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn145" role="doc-endnote"><p><span class="citation" data-cites="MacKay_2003_Information_Theory_Inference_and_Learning">MacKay (2003)</span>.<a href="#fnref145" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn146" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2012_Machine_Learning_A_probabilistic_perspective">Murphy (2012)</span>.<a href="#fnref146" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn147" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2022_Probabilistic_Machine_Learning_An_introduction">Murphy (2022)</span>, p.&#xA0;195&#x2013;198.<a href="#fnref147" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn148" role="doc-endnote"><p><span class="citation" data-cites="Shalev_Shwarz_2014_Understanding_Machine_Learning_From_Theory">Shalev-Shwarz &amp; Ben-David (2014)</span>.<a href="#fnref148" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn149" role="doc-endnote"><p><span class="citation" data-cites="Vapnik_1994_Measuring_the_VC_dimension_of_a_learning_machine">Vapnik, Levin, &amp; LeCun (1994)</span>.<a href="#fnref149" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn150" role="doc-endnote"><p><span class="citation" data-cites="Shalev_Shwarz_2014_Understanding_Machine_Learning_From_Theory">Shalev-Shwarz &amp; Ben-David (2014)</span>, p.&#xA0;67&#x2013;82.<a href="#fnref150" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn151" role="doc-endnote"><p><span class="citation" data-cites="McCarthy_1955_A_proposal_for_the_Dartmouth_Summer_Research">McCarthy, Minsky, Rochester, &amp; Shannon (1955)</span>.<a href="#fnref151" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn152" role="doc-endnote"><p><span class="citation" data-cites="Solomonoff_2016_Ray_Solomonoff_and_the_Dartmouth_Summer_Research">Solomonoff (2016)</span>.<a href="#fnref152" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn153" role="doc-endnote"><p><span class="citation" data-cites="Kardum_2020_Rudolf_Carnap_The_grandfather_of_artificial">Kardum (2020)</span>.<a href="#fnref153" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn154" role="doc-endnote"><p><span class="citation" data-cites="Murphy_2012_Machine_Learning_A_probabilistic_perspective">Murphy (2012)</span>, p.&#xA0;21.<a href="#fnref154" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn155" role="doc-endnote"><p>Note: <em>Label smoothing</em> is a regularization technique that smears the activation over other labels, but we don&#x2019;t do that here.<a href="#fnref155" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn156" role="doc-endnote"><p>&#x201C;Logit&#x201D; was coined by <a href="https://en.wikipedia.org/wiki/Joseph_Berkson">Joseph Berkson</a> (1899-1982).<a href="#fnref156" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn157" role="doc-endnote"><p><span class="citation" data-cites="McFadden_1973_Conditional_logit_analysis_of_qualitative_choice">McFadden &amp; Zarembka (1973)</span>.<a href="#fnref157" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn158" role="doc-endnote"><p><span class="citation" data-cites="Blondel_2020_Learning_with_Fenchel_Young_losses">Blondel, Martins, &amp; Niculae (2020)</span>.<a href="#fnref158" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn159" role="doc-endnote"><p><span class="citation" data-cites="Goodfellow_2016_Deep_Learning">Goodfellow et al. (2016)</span>, p.&#xA0;129.<a href="#fnref159" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn160" role="doc-endnote"><p><span class="citation" data-cites="Freund_1997_A_decision_theoretic_generalization_of_on_line">Freund &amp; Schapire (1997)</span>.<a href="#fnref160" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn161" role="doc-endnote"><p><span class="citation" data-cites="Chen_2016_Xgboost_A_scalable_tree_boosting_system">T. Chen &amp; Guestrin (2016)</span>.<a href="#fnref161" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn162" role="doc-endnote"><p><span class="citation" data-cites="Aytekin_2022_Neural_networks_are_decision_trees">Aytekin (2022)</span>.<a href="#fnref162" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn163" role="doc-endnote"><p><span class="citation" data-cites="Grinsztajn_2022_Why_do_tree_based_models_still_outperform_deep">Grinsztajn, Oyallon, &amp; Varoquaux (2022)</span>.<a href="#fnref163" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn164" role="doc-endnote"><p><span class="citation" data-cites="Coadou_2022_Boosted_decision_trees">Coadou (2022)</span>.<a href="#fnref164" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn165" role="doc-endnote"><p><span class="citation" data-cites="Slonim_2005_Information_based_clustering">Slonim, Atwal, Tkacik, &amp; Bialek (2005)</span>.<a href="#fnref165" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn166" role="doc-endnote"><p><span class="citation" data-cites="Batson_2021_Topological_obstructions_to_autoencoding">Batson, Haaf, Kahn, &amp; Roberts (2021)</span>.<a href="#fnref166" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn167" role="doc-endnote"><p><span class="citation" data-cites="Hennig_2015_What_are_the_true_clusters">Hennig (2015)</span>.<a href="#fnref167" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn168" role="doc-endnote"><p><span class="citation" data-cites="Lauc_2020_Machine_learning_and_the_philosophical_problems">Lauc (2020)</span>, p.&#xA0;103&#x2013;4.<a href="#fnref168" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn169" role="doc-endnote"><p><span class="citation" data-cites="Ronen_2022_DeepDPM_Deep_clustering_with_an_unknown_number">Ronen, Finder, &amp; Freifeld (2022)</span>.<a href="#fnref169" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn170" role="doc-endnote"><p><span class="citation" data-cites="Fang_2022_Is_out_of_distribution_detection_learnable">Fang, Z. et al. (2022)</span>.<a href="#fnref170" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn171" role="doc-endnote"><p><span class="citation" data-cites="Bengio_2009_Learning_deep_architectures_for_AI">Bengio (2009)</span>.<a href="#fnref171" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn172" role="doc-endnote"><p><span class="citation" data-cites="LeCun_2015_Deep_learning">LeCun, Bengio, &amp; Hinton (2015)</span>.<a href="#fnref172" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn173" role="doc-endnote"><p><span class="citation" data-cites="Sutskever_2015_A_brief_overview_of_deep_learning">Sutskever (2015)</span>.<a href="#fnref173" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn174" role="doc-endnote"><p><span class="citation" data-cites="Goodfellow_2016_Deep_Learning">Goodfellow et al. (2016)</span>.<a href="#fnref174" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn175" role="doc-endnote"><p><span class="citation" data-cites="Kaplan_2019_Notes_on_contemporary_machine_learning">Kaplan, J. et al. (2019)</span>.<a href="#fnref175" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn176" role="doc-endnote"><p><span class="citation" data-cites="Rumelhart_1986_Learning_representations_by_back_propagating">Rumelhart, Hinton, &amp; Williams (1986)</span>.<a href="#fnref176" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn177" role="doc-endnote"><p><span class="citation" data-cites="LeCun_1998_Efficient_BackProp">LeCun &amp; Bottou (1998)</span>.<a href="#fnref177" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn178" role="doc-endnote"><p><span class="citation" data-cites="Bottou_1998_Stochastic_gradient_descent_tricks">Bottou (1998)</span>.<a href="#fnref178" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn179" role="doc-endnote"><p><span class="citation" data-cites="Norvig_2011_On_Chomsky_and_the_Two_Cultures_of_Statistical">Norvig (2011)</span>.<a href="#fnref179" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn180" role="doc-endnote"><p><span class="citation" data-cites="Sutton_2019_The_bitter_lesson">Sutton (2019)</span>.<a href="#fnref180" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn181" role="doc-endnote"><p><span class="citation" data-cites="Frankle_2018_The_lottery_ticket_hypothesis_Finding_sparse">Frankle &amp; Carbin (2018)</span>.<a href="#fnref181" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn182" role="doc-endnote"><p><span class="citation" data-cites="Bengio_2009_Learning_deep_architectures_for_AI">Bengio (2009)</span>.<a href="#fnref182" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn183" role="doc-endnote"><p><span class="citation" data-cites="Belkin_2019_Reconciling_modern_machine_learning_practice">Belkin, Hsu, Ma, &amp; Mandal (2019)</span>.<a href="#fnref183" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn184" role="doc-endnote"><p><span class="citation" data-cites="Muthukumar_2019_Harmless_interpolation_of_noisy_data_in_regression">Muthukumar, Vodrahalli, Subramanian, &amp; Sahai (2019)</span>.<a href="#fnref184" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn185" role="doc-endnote"><p><span class="citation" data-cites="Nakkiran_2019_Deep_double_descent_Where_bigger_models_and_more">Nakkiran, P. et al. (2019)</span>.<a href="#fnref185" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn186" role="doc-endnote"><p><span class="citation" data-cites="Chang_2020_Provable_benefits_of_overparameterization_in_model">Chang, Li, Oymak, &amp; Thrampoulidis (2020)</span>.<a href="#fnref186" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn187" role="doc-endnote"><p><span class="citation" data-cites="Holzmuller_2020_On_the_universality_of_the_double_descent_peak">Holzm&#xFC;ller (2020)</span>.<a href="#fnref187" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn188" role="doc-endnote"><p><span class="citation" data-cites="Dar_2021_A_farewell_to_the_bias_variance_tradeoff">Dar, Muthukumar, &amp; Baraniuk (2021)</span>.<a href="#fnref188" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn189" role="doc-endnote"><p><span class="citation" data-cites="Balestriero_2021_Learning_in_high_dimension_always_amounts">Balestriero, Pesenti, &amp; LeCun (2021)</span>.<a href="#fnref189" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn190" role="doc-endnote"><p><span class="citation" data-cites="Belkin_2021_Fit_without_fear_remarkable_mathematical">Belkin (2021)</span>.<a href="#fnref190" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn191" role="doc-endnote"><p><span class="citation" data-cites="Nagarajan_2021_Explaining_generalization_in_deep_learning">Nagarajan (2021)</span>.<a href="#fnref191" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn192" role="doc-endnote"><p><span class="citation" data-cites="Bach_2022_Learning_Theory_from_First_Principles">Bach (2022)</span>, p.&#xA0;225&#x2013;230.<a href="#fnref192" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn193" role="doc-endnote"><p><span class="citation" data-cites="Ghosh_2022_A_universal_trade_off_between_the_model_size_test">Ghosh &amp; Belkin (2022)</span>.<a href="#fnref193" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn194" role="doc-endnote"><p><span class="citation" data-cites="Singh_2022_Phenomenology_of_double_descent_in_finite_width">Singh, Lucchi, Hofmann, &amp; Sch&#xF6;lkopf (2022)</span>.<a href="#fnref194" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn195" role="doc-endnote"><p><span class="citation" data-cites="Hastie_2022_Surprises_in_high_dimensional_ridgeless_least">Hastie, Montanari, Rosset, &amp; Tibshirani (2022)</span>.<a href="#fnref195" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn196" role="doc-endnote"><p><span class="citation" data-cites="Bubeck_2023_A_universal_law_of_robustness_via_isoperimetry">Bubeck &amp; Sellke (2023)</span>.<a href="#fnref196" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn197" role="doc-endnote"><p><span class="citation" data-cites="Gamba_2022_Deep_double_descent_via_smooth_interpolation">Gamba, Englesson, Bj&#xF6;rkman, &amp; Azizpour (2022)</span>.<a href="#fnref197" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn198" role="doc-endnote"><p><span class="citation" data-cites="Schaeffer_2023_Double_descent_demystified_Identifying">Schaeffer, R. et al. (2023)</span>.<a href="#fnref198" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn199" role="doc-endnote"><p><span class="citation" data-cites="Yang_2023_Dropout_drops_double_descent">Yang &amp; Suzuki (2023)</span>.<a href="#fnref199" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn200" role="doc-endnote"><p><span class="citation" data-cites="Maddox_2023_Rethinking_parameter_counting_in_deep_models">Maddox, Benton, &amp; Wilson (2023)</span>.<a href="#fnref200" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn201" role="doc-endnote"><p><span class="citation" data-cites="Steinhardt_2022_More_is_different_for_AI">Steinhardt (2022)</span>.<a href="#fnref201" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn202" role="doc-endnote"><p><span class="citation" data-cites="Henighan_2023_Superposition_memorization_and_double_descent">Henighan, T. et al. (2023)</span>.<a href="#fnref202" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn203" role="doc-endnote"><p>Mishra, D. (2020). <a href="https://towardsdatascience.com/weight-decay-l2-regularization-90a9e17713cd">Weight Decay == L2 Regularization?</a><a href="#fnref203" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn204" role="doc-endnote"><p><span class="citation" data-cites="Chen_2020_A_group_theoretic_framework_for_data_augmentation">S. Chen, Dobriban, &amp; Lee (2020)</span>.<a href="#fnref204" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn205" role="doc-endnote"><p><span class="citation" data-cites="Chiley_2019_Online_normalization_for_training_neural_networks">Chiley, V. et al. (2019)</span>.<a href="#fnref205" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn206" role="doc-endnote"><p><span class="citation" data-cites="Kiani_2022_projUNN_efficient_method_for_training_deep">Kiani, Balestriero, Lecun, &amp; Lloyd (2022)</span>.<a href="#fnref206" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn207" role="doc-endnote"><p><span class="citation" data-cites="Fukushima_1982_Neocognitron_A_new_algorithm_for_pattern">Fukushima &amp; Miyake (1982)</span>.<a href="#fnref207" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn208" role="doc-endnote"><p><span class="citation" data-cites="LeCun_1989_Backpropagation_applied_to_handwritten_zip_code">LeCun, Y. et al. (1989)</span>.<a href="#fnref208" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn209" role="doc-endnote"><p><span class="citation" data-cites="LeCun_1998_Gradient_based_learning_applied_to_document">LeCun, Bottou, Bengio, &amp; Haffner (1998)</span>.<a href="#fnref209" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn210" role="doc-endnote"><p><span class="citation" data-cites="Ciresan_2012_Multi_column_deep_neural_network_for_traffic_sign">Ciresan, Meier, Masci, &amp; Schmidhuber (2012)</span>.<a href="#fnref210" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn211" role="doc-endnote"><p><span class="citation" data-cites="Krizhevsky_2012_ImageNet_classification_with_deep_convolutional">Krizhevsky, Sutskever, &amp; Hinton (2012)</span>.<a href="#fnref211" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn212" role="doc-endnote"><p><span class="citation" data-cites="Simonyan_2014_Very_deep_convolutional_networks_for_large_scale">Simonyan &amp; Zisserman (2014)</span>.<a href="#fnref212" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn213" role="doc-endnote"><p><span class="citation" data-cites="He_2015_Deep_residual_learning_for_image_recognition">He, Zhang, Ren, &amp; Sun (2015)</span>.<a href="#fnref213" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn214" role="doc-endnote"><p><span class="citation" data-cites="Haber_2017_Stable_architectures_for_deep_neural_networks">Haber &amp; Ruthotto (2017)</span>.<a href="#fnref214" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn215" role="doc-endnote"><p><span class="citation" data-cites="Howard_2017_MobileNets_Efficient_convolutional_neural">Howard, A.G. et al. (2017)</span>.<a href="#fnref215" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn216" role="doc-endnote"><p><span class="citation" data-cites="Chen_2018_Neural_ordinary_differential_equations">R. T. Q. Chen, Rubanova, Bettencourt, &amp; Duvenaud (2018)</span>.<a href="#fnref216" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn217" role="doc-endnote"><p><span class="citation" data-cites="Tan_2019_EfficientNet_Rethinking_model_scaling">Tan &amp; Le (2019)</span>.<a href="#fnref217" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn218" role="doc-endnote"><p><span class="citation" data-cites="Dosovitskiy_2020_An_image_is_worth_16x16_words_Transformers">Dosovitskiy, A. et al. (2020)</span>.<a href="#fnref218" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn219" role="doc-endnote"><p><span class="citation" data-cites="Tan_2021_EfficientNetV2_Smaller_models_and_faster_training">Tan &amp; Le (2021)</span>.<a href="#fnref219" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn220" role="doc-endnote"><p><span class="citation" data-cites="Liu_2021_Pay_attention_to_MLPs">H. Liu, Dai, So, &amp; Le (2021)</span>.<a href="#fnref220" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn221" role="doc-endnote"><p><span class="citation" data-cites="Dhariwal_2021_Diffusion_models_beat_GANs_on_image_synthesis">Dhariwal &amp; Nichol (2021)</span>.<a href="#fnref221" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn222" role="doc-endnote"><p><span class="citation" data-cites="Liu_2021_A_survey_of_visual_transformers">Liu, Y. et al. (2021)</span>.<a href="#fnref222" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn223" role="doc-endnote"><p><span class="citation" data-cites="Ingrosso_2022_Data_driven_emergence_of_convolutional_structure">Ingrosso &amp; Goldt (2022)</span>.<a href="#fnref223" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn224" role="doc-endnote"><p><span class="citation" data-cites="Park_2022_How_do_vision_transformers_work">Park &amp; Kim (2022)</span>.<a href="#fnref224" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn225" role="doc-endnote"><p><span class="citation" data-cites="Firth_1957_A_synopsis_of_linguistic_theory_1930_1955">Firth (1957)</span>.<a href="#fnref225" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn226" role="doc-endnote"><p><span class="citation" data-cites="Nirenburg_1996_Bar_Hillel_and_Machine_Translation_Then_and_Now">Nirenburg (1996)</span>.<a href="#fnref226" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn227" role="doc-endnote"><p><span class="citation" data-cites="Hutchins_2000_Yehoshua_Bar_Hillel_A_philosophers_contribution">Hutchins (2000)</span>.<a href="#fnref227" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn228" role="doc-endnote"><p><span class="citation" data-cites="Jurafsky_2022_Speech_and_Language_Processing_An_introduction">Jurafsky &amp; Martin (2022)</span>.<a href="#fnref228" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn229" role="doc-endnote"><p><span class="citation" data-cites="Liu_2023_Representation_Learning_for_Natural_Language">Z. Liu, Lin, &amp; Sun (2023)</span>.<a href="#fnref229" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn230" role="doc-endnote"><p><span class="citation" data-cites="Mikolov_2013_Efficient_estimation_of_word_representations">Mikolov, Chen, Corrado, &amp; Dean (2013)</span>, <span class="citation" data-cites="Mikolov_2013_Linguistic_regularities_in_continuous_space_word">Mikolov, Yih, &amp; Zweig (2013)</span>, and <span class="citation" data-cites="Mikolov_2013_Distributed_representations_of_words_and_phrases">Mikolov, T. et al. (2013)</span>.<a href="#fnref230" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn231" role="doc-endnote"><p><span class="citation" data-cites="Kun_2018_A_Programmers_Introduction_to_Mathematics">Kun (2018)</span>, p.&#xA0;176&#x2013;8.<a href="#fnref231" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn232" role="doc-endnote"><p><span class="citation" data-cites="Hochreiter_1997_Long_short_term_memory">Hochreiter &amp; Schmidhuber (1997)</span>.<a href="#fnref232" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn233" role="doc-endnote"><p><span class="citation" data-cites="Graves_2013_Generating_sequences_with_recurrent_neural">Graves (2013)</span>.<a href="#fnref233" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn234" role="doc-endnote"><p><span class="citation" data-cites="Sutskever_2014_Sequence_to_sequence_learning_with_neural">Sutskever, Vinyals, &amp; Le (2014)</span>, p.&#xA0;4.<a href="#fnref234" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn235" role="doc-endnote"><p><span class="citation" data-cites="Zhang_1998_Complete_anytime_beam_search">Zhang (1998)</span>.<a href="#fnref235" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn236" role="doc-endnote"><p><span class="citation" data-cites="Zhou_2005_Beam_stack_search_Integrating_backtracking">Zhou &amp; Hansen (2005)</span>.<a href="#fnref236" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn237" role="doc-endnote"><p><span class="citation" data-cites="Collobert_2019_A_fully_differentiable_beam_search_decoder">Collobert, Hannun, &amp; Synnaeve (2019)</span>.<a href="#fnref237" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn238" role="doc-endnote"><p><span class="citation" data-cites="Werbos_1990_Backpropagation_through_time_what_it_does_and_how">Werbos (1990)</span>.<a href="#fnref238" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn239" role="doc-endnote"><p><span class="citation" data-cites="Sutskever_2014_Sequence_to_sequence_learning_with_neural">Sutskever et al. (2014)</span>.<a href="#fnref239" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn240" role="doc-endnote"><p><span class="citation" data-cites="Bahdanau_2015_Neural_machine_translation_by_jointly_learning">Bahdanau, Cho, &amp; Bengio (2015)</span>.<a href="#fnref240" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn241" role="doc-endnote"><p><span class="citation" data-cites="Wu_2016_Googles_neural_machine_translation_system">Wu, Y. et al. (2016)</span>.<a href="#fnref241" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn242" role="doc-endnote"><p><span class="citation" data-cites="Stahlberg_2019_Neural_machine_translation_A_review">Stahlberg (2019)</span>.<a href="#fnref242" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn243" role="doc-endnote"><p><span class="citation" data-cites="Vaswani_2017_Attention_is_all_you_need">Vaswani, A. et al. (2017)</span>.<a href="#fnref243" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn244" role="doc-endnote"><p><span class="citation" data-cites="Devlin_2018_BERT_Pre_training_of_deep_bidirectional">Devlin, Chang, Lee, &amp; Toutanova (2018)</span>.<a href="#fnref244" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn245" role="doc-endnote"><p><span class="citation" data-cites="Liu_2019_RoBERTa_A_robustly_optimized_BERT_pretraining">Liu, Y. et al. (2019)</span>.<a href="#fnref245" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn246" role="doc-endnote"><p><span class="citation" data-cites="Raffel_2019_Exploring_the_limits_of_transfer_learning">Raffel, C. et al. (2019)</span>.<a href="#fnref246" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn247" role="doc-endnote"><p><span class="citation" data-cites="Lan_2019_ALBERT_A_lite_BERT_for_self_supervised_learning">Lan, Z. et al. (2019)</span>.<a href="#fnref247" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn248" role="doc-endnote"><p><span class="citation" data-cites="Lewis_2019_BART_Denoising_sequence_to_sequence_pre_training">Lewis, M. et al. (2019)</span>.<a href="#fnref248" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn249" role="doc-endnote"><p><span class="citation" data-cites="Radford_2018_Improving_language_understanding_by_generative">Radford, Narasimhan, Salimans, &amp; Sutskever (2018)</span>.<a href="#fnref249" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn250" role="doc-endnote"><p><span class="citation" data-cites="Radford_2019_Language_models_are_unsupervised_multitask">Radford, A. et al. (2019)</span>.<a href="#fnref250" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn251" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Language_models_are_few_shot_learners">Brown, T.B. et al. (2020)</span>.<a href="#fnref251" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn252" role="doc-endnote"><p><span class="citation" data-cites="Yang_2019_XLNet_Generalized_autoregressive_pretraining">Yang, Z. et al. (2019)</span>.<a href="#fnref252" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn253" role="doc-endnote"><p><span class="citation" data-cites="Zaheer_2020_Big_Bird_Transformers_for_longer_sequences">Zaheer, M. et al. (2020)</span>.<a href="#fnref253" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn254" role="doc-endnote"><p><span class="citation" data-cites="Edelman_2021_Inductive_biases_and_variable_creation_in_self">Edelman, Goel, Kakade, &amp; Zhang (2021)</span>.<a href="#fnref254" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn255" role="doc-endnote"><p><span class="citation" data-cites="Tay_2022_Efficient_transformers_A_survey">Tay, Dehghani, Bahri, &amp; Metzler (2022)</span>.<a href="#fnref255" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn256" role="doc-endnote"><p><span class="citation" data-cites="Phuong_2022_Formal_algorithms_for_transformers">Phuong &amp; Hutter (2022)</span>.<a href="#fnref256" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn257" role="doc-endnote"><p><span class="citation" data-cites="Chowdhery_2022_PaLM_Scaling_language_modeling_with_pathways">Chowdhery, A. et al. (2022)</span>.<a href="#fnref257" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn258" role="doc-endnote"><p><span class="citation" data-cites="Ouyang_2022_Training_language_models_to_follow_instructions">Ouyang, L. et al. (2022)</span>.<a href="#fnref258" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn259" role="doc-endnote"><p><span class="citation" data-cites="Wolfram_2023_What_is_ChatGPT_doing_and_why_does_it_work">Wolfram (2023)</span>.<a href="#fnref259" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn260" role="doc-endnote"><p><span class="citation" data-cites="OpenAI_2023_GPT_4_Technical_Report">OpenAI (2023)</span>.<a href="#fnref260" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn261" role="doc-endnote"><p><span class="citation" data-cites="Mohamadi_2023_ChatGPT_in_the_age_of_generative_AI_and_large">Mohamadi, S. et al. (2023)</span>.<a href="#fnref261" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn262" role="doc-endnote"><p><span class="citation" data-cites="Dao_2022_FlashAttention_Fast_and_memory_efficient_exact">Dao, T. et al. (2022)</span>.<a href="#fnref262" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn263" role="doc-endnote"><p><span class="citation" data-cites="Merrill_2022_The_parallelism_tradeoff_Limitations_of_log">Merrill &amp; Sabharwal (2022)</span>.<a href="#fnref263" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn264" role="doc-endnote"><p><span class="citation" data-cites="Gu_2021_Efficiently_modeling_long_sequences">Gu, Goel, &amp; R&#xE9; (2021)</span>.<a href="#fnref264" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn265" role="doc-endnote"><p><span class="citation" data-cites="Bulatov_2022_Recurrent_memory_transformer">Bulatov, Kuratov, &amp; Burtsev (2022)</span>.<a href="#fnref265" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn266" role="doc-endnote"><p><span class="citation" data-cites="Bulatov_2023_Scaling_transformer_to_1M_tokens_and_beyond">Bulatov, Kuratov, &amp; Burtsev (2023)</span>.<a href="#fnref266" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn267" role="doc-endnote"><p><span class="citation" data-cites="Bertsch_2023_Unlimiformer_Long_range_transformers">Bertsch, Alon, Neubig, &amp; Gormley (2023)</span>.<a href="#fnref267" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn268" role="doc-endnote"><p><span class="citation" data-cites="Mialon_2023_Augmented_Language_Models_a_Survey">Mialon, G. et al. (2023)</span>.<a href="#fnref268" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn269" role="doc-endnote"><p><span class="citation" data-cites="Peng_2023_RWKV_Reinventing_RNNs_for_the_Transformer_Era">Peng, B. et al. (2023)</span>.<a href="#fnref269" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn270" role="doc-endnote"><p><span class="citation" data-cites="Sun_2023_Retentive_network_A_successor_to_transformer">Sun, Y. et al. (2023)</span>.<a href="#fnref270" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn271" role="doc-endnote"><p><span class="citation" data-cites="Gu_2023_Mamba_Linear_time_sequence_modeling">Gu &amp; Dao (2023)</span>.<a href="#fnref271" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn272" role="doc-endnote"><p><span class="citation" data-cites="Wang_2023_BitNet_Scaling_1_bit_transformers_for_large">Wang, H. et al. (2023)</span>.<a href="#fnref272" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn273" role="doc-endnote"><p><span class="citation" data-cites="Ma_2024_The_era_of_1_bit_LLMs_All_large_language_models">Ma, S. et al. (2024)</span>.<a href="#fnref273" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn274" role="doc-endnote"><p><span class="citation" data-cites="Ma_2024_Megalodon_Efficient_LLM_pretraining_and_inference">Ma, X. et al. (2024)</span>.<a href="#fnref274" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn275" role="doc-endnote"><p><span class="citation" data-cites="Bhargava_2023_Whats_the_magic_word">Bhargava, Witkowski, Shah, &amp; Thomson (2023)</span>.<a href="#fnref275" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn276" role="doc-endnote"><p><span class="citation" data-cites="Hestness_2017_Deep_learning_scaling_is_predictable_empirically">Hestness, J. et al. (2017)</span>.<a href="#fnref276" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn277" role="doc-endnote"><p><span class="citation" data-cites="Church_2019_A_survey_of_25_years_of_evaluation">Church &amp; Hestness (2019)</span>.<a href="#fnref277" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn278" role="doc-endnote"><p><span class="citation" data-cites="Kaplan_2020_Scaling_laws_for_neural_language_models">Kaplan, J. et al. (2020)</span>.<a href="#fnref278" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn279" role="doc-endnote"><p><span class="citation" data-cites="Rae_2022_Scaling_language_models_Methods_analysis">Rae, J.W. et al. (2022)</span>.<a href="#fnref279" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn280" role="doc-endnote"><p><span class="citation" data-cites="Hoffmann_2022_Training_compute_optimal_large_language_models">Hoffmann, J. et al. (2022)</span>.<a href="#fnref280" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn281" role="doc-endnote"><p><span class="citation" data-cites="Muennighoff_2023_Scaling_data_constrained_language_models">Muennighoff, N. et al. (2023)</span>.<a href="#fnref281" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn282" role="doc-endnote"><p><span class="citation" data-cites="Mahowald_2023_Dissociating_language_and_thought_in_large">Mahowald, K. et al. (2023)</span>.<a href="#fnref282" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn283" role="doc-endnote"><p><span class="citation" data-cites="Kosinski_2023_Theory_of_mind_may_have_spontaneously_emerged">Kosinski (2023)</span>.<a href="#fnref283" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn284" role="doc-endnote"><p><span class="citation" data-cites="Watson_2019_The_explanation_game_A_formal_framework">Watson &amp; Floridi (2019)</span>.<a href="#fnref284" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn285" role="doc-endnote"><p><span class="citation" data-cites="Gurnee_2023_Finding_neurons_in_a_haystack_Case_studies">Gurnee, W. et al. (2023)</span>.<a href="#fnref285" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn286" role="doc-endnote"><p><span class="citation" data-cites="Meng_2023_Locating_and_editing_factual_associations_in_GPT">Meng, Bau, Andonian, &amp; Belinkov (2023)</span>.<a href="#fnref286" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn287" role="doc-endnote"><p><span class="citation" data-cites="McDougall_2023_Copy_suppression_Comprehensively_understanding">McDougall, C. et al. (2023)</span>.<a href="#fnref287" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn288" role="doc-endnote"><p><span class="citation" data-cites="Alain_2016_Understanding_intermediate_layers_using_linear">Alain &amp; Bengio (2016)</span>.<a href="#fnref288" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn289" role="doc-endnote"><p><span class="citation" data-cites="Belinkov_2022_Probing_classifiers_Promises_shortcomings">Belinkov (2022)</span>.<a href="#fnref289" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn290" role="doc-endnote"><p><span class="citation" data-cites="Gurnee_2023_Language_models_represent_space_and_time">Gurnee &amp; Tegmark (2023)</span>.<a href="#fnref290" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn291" role="doc-endnote"><p><span class="citation" data-cites="Sutton_2018_Reinforcement_Learning">Sutton &amp; Barto (2018)</span>.<a href="#fnref291" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn292" role="doc-endnote"><p><span class="citation" data-cites="Arulkumaran_2017_Deep_Reinforcement_Learning_A_Brief_Survey">Arulkumaran, Deisenroth, Brundage, &amp; Bharath (2017)</span>.<a href="#fnref292" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn293" role="doc-endnote"><p><span class="citation" data-cites="Cesa_Bianchi_2006_Prediction_Learning_and_Games">Cesa-Bianchi &amp; Lugosi (2006)</span>.<a href="#fnref293" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn294" role="doc-endnote"><p><span class="citation" data-cites="Bellman_1952_On_the_theory_of_dynamic_programming">Bellman (1952)</span>.<a href="#fnref294" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn295" role="doc-endnote"><p><span class="citation" data-cites="Mnih_2013_Playing_Atari_with_deep_reinforcement_learning">Mnih, V. et al. (2013)</span> and <span class="citation" data-cites="Mnih_2015_Human_level_control_through_deep_reinforcement">Mnih, V. et al. (2015)</span>.<a href="#fnref295" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn296" role="doc-endnote"><p><span class="citation" data-cites="Silver_2016_Mastering_the_game_of_Go_with_deep_neural_networks">Silver, D. et al. (2016)</span>.<a href="#fnref296" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn297" role="doc-endnote"><p><span class="citation" data-cites="Silver_2017_Mastering_the_game_of_Go_without_human_knowledge">Silver, D. et al. (2017b)</span>.<a href="#fnref297" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn298" role="doc-endnote"><p><span class="citation" data-cites="Silver_2017_Mastering_chess_and_shogi_by_self_play">Silver, D. et al. (2017a)</span>.<a href="#fnref298" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn299" role="doc-endnote"><p><span class="citation" data-cites="Hart_2000_A_simple_adaptive_procedure_leading_to_correlated">Hart &amp; Mas&#x2010;Colell (2000)</span>.<a href="#fnref299" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn300" role="doc-endnote"><p><span class="citation" data-cites="Roughgarden_2016_Twenty_Lectures_on_Algorithmic_Game_Theory">Roughgarden (2016)</span>.<a href="#fnref300" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn301" role="doc-endnote"><p><span class="citation" data-cites="Zinkevich_2007_Regret_minimization_in_games_with_incomplete">Zinkevich, Johanson, Bowling, &amp; Piccione (2007)</span>.<a href="#fnref301" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn302" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Equilibrium_finding_for_large_adversarial">N. Brown (2020)</span>, p.&#xA0;12.<a href="#fnref302" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn303" role="doc-endnote"><p><span class="citation" data-cites="Zinkevich_2007_Regret_minimization_in_games_with_incomplete">Zinkevich et al. (2007)</span>, p.&#xA0;4.<a href="#fnref303" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn304" role="doc-endnote"><p><span class="citation" data-cites="Tammelin_2014_Solving_large_imperfect_information_games_using">Tammelin (2014)</span>.<a href="#fnref304" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn305" role="doc-endnote"><p><span class="citation" data-cites="Tammelin_2015_Solving_heads_up_limit_texas_holdem">Tammelin, Burch, Johanson, &amp; Bowling (2015)</span>.<a href="#fnref305" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn306" role="doc-endnote"><p><span class="citation" data-cites="Burch_2019_Revisiting_CFR_and_alternating_updates">Burch, Moravcik, &amp; Schmid (2019)</span>.<a href="#fnref306" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn307" role="doc-endnote"><p><span class="citation" data-cites="Brown_2019_Solving_imperfect_information_games_via_discounted">N. Brown &amp; Sandholm (2019a)</span>.<a href="#fnref307" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn308" role="doc-endnote"><p><span class="citation" data-cites="Zinkevich_2007_Regret_minimization_in_games_with_incomplete">Zinkevich et al. (2007)</span> and <span class="citation" data-cites="Lanctot_2009_Monte_Carlo_sampling_for_regret_minimization">Lanctot, Waugh, Zinkevich, &amp; Bowling (2009)</span>.<a href="#fnref308" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn309" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Equilibrium_finding_for_large_adversarial">N. Brown (2020)</span>, p.&#xA0;6.<a href="#fnref309" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn310" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Equilibrium_finding_for_large_adversarial">N. Brown (2020)</span>, p.&#xA0;12.<a href="#fnref310" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn311" role="doc-endnote"><p><span class="citation" data-cites="Lanctot_2009_Monte_Carlo_sampling_for_regret_minimization">Lanctot et al. (2009)</span>.<a href="#fnref311" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn312" role="doc-endnote"><p><span class="citation" data-cites="Neller_2013_An_introduction_to_counterfactual_regret">Neller &amp; Lanctot (2013)</span>.<a href="#fnref312" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn313" role="doc-endnote"><p><span class="citation" data-cites="Burch_2012_Efficient_Monte_Carlo_counterfactual_regret">Burch, Lanctot, Szafron, &amp; Gibson (2012)</span>.<a href="#fnref313" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn314" role="doc-endnote"><p><span class="citation" data-cites="Johanson_2012_Efficient_Nash_equilibrium_approximation_through">Johanson, M. et al. (2012)</span>.<a href="#fnref314" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn315" role="doc-endnote"><p><span class="citation" data-cites="Schmid_2019_Variance_reduction_in_Monte_Carlo_counterfactual">Schmid, M. et al. (2019)</span>.<a href="#fnref315" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn316" role="doc-endnote"><p><span class="citation" data-cites="Li_2020_Regret_minimization_via_novel_vectorized_sampling">Li, H. et al. (2020)</span>.<a href="#fnref316" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn317" role="doc-endnote"><p><span class="citation" data-cites="Habara_2023_Convergence_analysis_and_acceleration">Habara, Fukuda, &amp; Yamashita (2023)</span>.<a href="#fnref317" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn318" role="doc-endnote"><p><span class="citation" data-cites="Lanctot_2013_Monte_Carlo_Sample_and_Regret_Minimization">Lanctot (2013)</span>.<a href="#fnref318" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn319" role="doc-endnote"><p><span class="citation" data-cites="Gibson_2014_Regret_minimization_in_games_and_the_development">Gibson (2014)</span>.<a href="#fnref319" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn320" role="doc-endnote"><p><span class="citation" data-cites="Johanson_2016_Robust_Strategies_and_Counter_Strategies_From">Johanson (2016)</span>.<a href="#fnref320" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn321" role="doc-endnote"><p><span class="citation" data-cites="Burch_2018_Time_and_Space_Why_imperfect_information_games">Burch (2018)</span>.<a href="#fnref321" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn322" role="doc-endnote"><p><span class="citation" data-cites="Lisy_2016_Equilibrium_approximation_quality_of_current_no">Lisy &amp; Bowling (2016)</span>, p.&#xA0;2.<a href="#fnref322" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn323" role="doc-endnote"><p>See NashConv exploitability defined in <span class="citation" data-cites="Lanctot_2017_A_unified_game_theoretic_approach_to_multiagent">Lanctot, M. et al. (2017)</span>.<a href="#fnref323" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn324" role="doc-endnote"><p><span class="citation" data-cites="Timbers_2020_Approximate_exploitability_Learning_a_best">Timbers (2020)</span>, p.&#xA0;3.<a href="#fnref324" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn325" role="doc-endnote"><p><span class="citation" data-cites="Johanson_2011_Accelerating_best_response_calculation_in_large">Johanson, Waugh, Bowling, &amp; Zinkevich (2011)</span>.<a href="#fnref325" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn326" role="doc-endnote"><p><span class="citation" data-cites="Ponsen_2011_Computing_approximate_Nash_equilibria_and_robust">Ponsen, De Jong, &amp; Lanctot (2011)</span>.<a href="#fnref326" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn327" role="doc-endnote"><p><span class="citation" data-cites="Lisy_2016_Equilibrium_approximation_quality_of_current_no">Lisy &amp; Bowling (2016)</span>.<a href="#fnref327" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn328" role="doc-endnote"><p><span class="citation" data-cites="Timbers_2020_Approximate_exploitability_Learning_a_best">Timbers (2020)</span>.<a href="#fnref328" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn329" role="doc-endnote"><p><span class="citation" data-cites="Kuhn_1950_A_simplified_two_person_poker">Kuhn (1950)</span>.<a href="#fnref329" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn330" role="doc-endnote"><p><span class="citation" data-cites="Southey_2012_Bayes_bluff_Opponent_modelling_in_poker">Southey, F. et al. (2012)</span>.<a href="#fnref330" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn331" role="doc-endnote"><p><span class="citation" data-cites="Billings_2002_The_challenge_of_poker">Billings, Davidson, Schaeffer, &amp; Szafron (2002)</span>.<a href="#fnref331" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn332" role="doc-endnote"><p><span class="citation" data-cites="Billings_2003_Approximating_game_theoretic_optimal_strategies">Billings, D. et al. (2003)</span>.<a href="#fnref332" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn333" role="doc-endnote"><p><span class="citation" data-cites="Johanson_2013_Measuring_the_size_of_large_no_limit_poker_games">Johanson (2013)</span>.<a href="#fnref333" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn334" role="doc-endnote"><p><span class="citation" data-cites="Bowling_2015_Heads_up_limit_holdem_poker_is_solved">Bowling, Burch, Johanson, &amp; Tammelin (2015)</span>.<a href="#fnref334" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn335" role="doc-endnote"><p><span class="citation" data-cites="Heinrich_2016_Deep_reinforcement_learning_from_self_play">Heinrich &amp; Silver (2016)</span>.<a href="#fnref335" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn336" role="doc-endnote"><p><span class="citation" data-cites="Moravcik_2017_DeepStack_Expert_level_artificial_intelligence">Moravcik, M. et al. (2017)</span>.<a href="#fnref336" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn337" role="doc-endnote"><p><span class="citation" data-cites="Brown_2018_Superhuman_AI_for_heads_up_no_limit_poker">N. Brown &amp; Sandholm (2018)</span>.<a href="#fnref337" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn338" role="doc-endnote"><p><span class="citation" data-cites="Brown_2019_Solving_imperfect_information_games_via_discounted">N. Brown &amp; Sandholm (2019a)</span>.<a href="#fnref338" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn339" role="doc-endnote"><p><span class="citation" data-cites="Brown_2019_Deep_counterfactual_regret_minimization">N. Brown, Lerer, Gross, &amp; Sandholm (2019)</span>.<a href="#fnref339" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn340" role="doc-endnote"><p><span class="citation" data-cites="Brown_2019_Superhuman_AI_for_multiplayer_poker">N. Brown &amp; Sandholm (2019b)</span>.<a href="#fnref340" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn341" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Combining_deep_reinforcement_learning_and_search">N. Brown, Bakhtin, Lerer, &amp; Gong (2020)</span>.<a href="#fnref341" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn342" role="doc-endnote"><p><span class="citation" data-cites="Brown_2020_Equilibrium_finding_for_large_adversarial">N. Brown (2020)</span>.<a href="#fnref342" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn343" role="doc-endnote"><p><span class="citation" data-cites="Schmid_2021_Player_of_games">Schmid, M. et al. (2021)</span>.<a href="#fnref343" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn344" role="doc-endnote"><p><span class="citation" data-cites="Kovarik_2022_Rethinking_formal_models_of_partially_observable">Kovarik, V. et al. (2022)</span>.<a href="#fnref344" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn345" role="doc-endnote"><p><span class="citation" data-cites="Spears_2018_Deep_learning_A_guide_for_practitioners">Spears, B.K. et al. (2018)</span>.<a href="#fnref345" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn346" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2021_Machine_learning">Cranmer, Seljak, &amp; Terao (2021)</span>.<a href="#fnref346" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn347" role="doc-endnote"><p><span class="citation" data-cites="Cilibrasi_2005_Clustering_by_compression">Cilibrasi &amp; Vitanyi (2005)</span>.<a href="#fnref347" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn348" role="doc-endnote"><p><span class="citation" data-cites="Hutter_2007_Universal_Algorithmic_Intelligence_A_mathematical">Hutter (2007)</span>.<a href="#fnref348" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn349" role="doc-endnote"><p><span class="citation" data-cites="Rathmanner_2011_A_philosophical_treatise_of_universal_induction">Rathmanner &amp; Hutter (2011)</span>.<a href="#fnref349" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn350" role="doc-endnote"><p><span class="citation" data-cites="Wolpert_1995_No_free_lunch_theorems_for_search">Wolpert &amp; Macready (1995)</span>.<a href="#fnref350" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn351" role="doc-endnote"><p><span class="citation" data-cites="Wolpert_1996_The_lack_of_a_priori_distinctions_between_learning">Wolpert (1996)</span>.<a href="#fnref351" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn352" role="doc-endnote"><p><span class="citation" data-cites="Wolpert_1997_No_free_lunch_theorems_for_optimization">Wolpert &amp; Macready (1997)</span>.<a href="#fnref352" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn353" role="doc-endnote"><p><span class="citation" data-cites="Shalev_Shwarz_2014_Understanding_Machine_Learning_From_Theory">Shalev-Shwarz &amp; Ben-David (2014)</span>, p.&#xA0;60&#x2013;66.<a href="#fnref353" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn354" role="doc-endnote"><p><span class="citation" data-cites="McDermott_2019_When_and_why_metaheuristics_researchers_can_ignore">McDermott (2019)</span>.<a href="#fnref354" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn355" role="doc-endnote"><p><span class="citation" data-cites="Wolpert_2007_Physical_limits_of_inference">Wolpert (2007)</span>.<a href="#fnref355" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn356" role="doc-endnote"><p><span class="citation" data-cites="Wolpert_2020_Noisy_deductive_reasoning_How_humans_construct">Wolpert &amp; Kinney (2020)</span>.<a href="#fnref356" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn357" role="doc-endnote"><p><span class="citation" data-cites="Mitchell_1980_The_need_for_biases_in_learning_generalizations">Mitchell (1980)</span>.<a href="#fnref357" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn358" role="doc-endnote"><p><span class="citation" data-cites="Roberts_2021_Why_is_AI_hard_and_physics_simple">Roberts (2021)</span>.<a href="#fnref358" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn359" role="doc-endnote"><p><span class="citation" data-cites="Goldreich_1997_On_universal_learning_algorithms">Goldreich &amp; Ron (1997)</span>.<a href="#fnref359" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn360" role="doc-endnote"><p><span class="citation" data-cites="Joyce_2017_A_review_of_no_free_lunch_theorems_and_their">Joyce &amp; Herrmann (2017)</span>.<a href="#fnref360" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn361" role="doc-endnote"><p><span class="citation" data-cites="Lauc_2020_Machine_learning_and_the_philosophical_problems">Lauc (2020)</span>.<a href="#fnref361" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn362" role="doc-endnote"><p><span class="citation" data-cites="Nakkiran_2021_Turing_universal_learners_with_optimal_scaling">Nakkiran (2021)</span>.<a href="#fnref362" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn363" role="doc-endnote"><p><span class="citation" data-cites="Bousquet_2021_A_theory_of_universal_learning">Bousquet, O. et al. (2021)</span>.<a href="#fnref363" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn364" role="doc-endnote"><p><span class="citation" data-cites="Andrews_2023_The_devil_in_the_data_Machine_learning">Andrews (2023)</span>.<a href="#fnref364" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn365" role="doc-endnote"><p><span class="citation" data-cites="Raissi_2017_Physics_informed_deep_learning_Part_I_Data">Raissi, Perdikaris, &amp; Karniadakis (2017a)</span>, p.&#xA0;2.<a href="#fnref365" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn366" role="doc-endnote"><p><span class="citation" data-cites="Roberts_2021_Why_is_AI_hard_and_physics_simple">Roberts (2021)</span>, p.&#xA0;7.<a href="#fnref366" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn367" role="doc-endnote"><p><span class="citation" data-cites="Dennett_1991_Real_patterns">Dennett (1991)</span>, p.&#xA0;TODO.<a href="#fnref367" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn368" role="doc-endnote"><p><span class="citation" data-cites="Minsky_1969_Perceptrons_An_Introduction_to_Computational">Minsky &amp; Papert (1969)</span>.<a href="#fnref368" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn369" role="doc-endnote"><p><span class="citation" data-cites="Hornik_1989_Multilayer_feedforward_networks_are_universal">Hornik, Stinchcombe, &amp; White (1989)</span>.<a href="#fnref369" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn370" role="doc-endnote"><p><span class="citation" data-cites="Lu_2017_The_expressive_power_of_neural_networks_A_view">Lu, Z. et al. (2017)</span>.<a href="#fnref370" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn371" role="doc-endnote"><p><span class="citation" data-cites="Lin_2018_ResNet_with_one_neuron_hidden_layers_is">Lin &amp; Jegelka (2018)</span>.<a href="#fnref371" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn372" role="doc-endnote"><p><span class="citation" data-cites="Ismailov_2020_A_three_layer_neural_network_can_represent_any">Ismailov (2020)</span>.<a href="#fnref372" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn373" role="doc-endnote"><p><span class="citation" data-cites="Bishop_2006_Pattern_Recognition_and_Machine_Learning">Bishop (2006)</span>, p.&#xA0;230.<a href="#fnref373" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn374" role="doc-endnote"><p><span class="citation" data-cites="Opper_1996_Statistical_mechanics_of_generalization">Opper &amp; Kinzel (1996)</span>.<a href="#fnref374" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn375" role="doc-endnote"><p><span class="citation" data-cites="Opper_2001_Learning_to_generalize">Opper (2001)</span>.<a href="#fnref375" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn376" role="doc-endnote"><p><span class="citation" data-cites="Bahri_2020_Statistical_mechanics_of_deep_learning">Bahri, Y. et al. (2020)</span>.<a href="#fnref376" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn377" role="doc-endnote"><p><span class="citation" data-cites="Halverson_2020_Neural_networks_and_quantum_field_theory">Halverson, Maiti, &amp; Stoner (2020)</span>.<a href="#fnref377" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn378" role="doc-endnote"><p><span class="citation" data-cites="Canatar_2020_Spectral_bias_and_task_model_alignment_explain">Canatar, Bordelon, &amp; Pehlevan (2020)</span>.<a href="#fnref378" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn379" role="doc-endnote"><p><span class="citation" data-cites="Roberts_2021_The_Principles_of_Deep_Learning_Theory">Roberts, Yaida, &amp; Hanin (2021)</span>.<a href="#fnref379" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn380" role="doc-endnote"><p><span class="citation" data-cites="Cantwell_2022_Approximate_sampling_and_estimation_of_partition">Cantwell (2022)</span>.<a href="#fnref380" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn381" role="doc-endnote"><p><span class="citation" data-cites="Dinan_2023_Effective_theory_of_transformers_at_initialization">Dinan, Yaida, &amp; Zhang (2023)</span>.<a href="#fnref381" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn382" role="doc-endnote"><p><span class="citation" data-cites="Sohl_Dickstein_2020_Two_equalities_expressing_the_determinant">Sohl-Dickstein (2020)</span>.<a href="#fnref382" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn383" role="doc-endnote"><p><span class="citation" data-cites="Aifer_2023_Thermodynamic_linear_algebra">Aifer, M. et al. (2023)</span>.<a href="#fnref383" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn384" role="doc-endnote"><p><span class="citation" data-cites="Geshkovski_2023_A_mathematical_perspective_on_Transformers">Geshkovski, Letrouit, Polyanskiy, &amp; Rigollet (2023)</span>.<a href="#fnref384" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn385" role="doc-endnote"><p><span class="citation" data-cites="Cohen_2016_Group_equivariant_convolutional_networks">Cohen &amp; Welling (2016)</span>.<a href="#fnref385" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn386" role="doc-endnote"><p><span class="citation" data-cites="Cohen_2019_Gauge_equivariant_convolutional_networks">Cohen, Weiler, Kicanaoglu, &amp; Welling (2019)</span>.<a href="#fnref386" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn387" role="doc-endnote"><p><span class="citation" data-cites="Fuchs_2020_SE3_Transformers_3D_roto_translation">Fuchs, Worrall, Fischer, &amp; Welling (2020)</span>.<a href="#fnref387" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn388" role="doc-endnote"><p><span class="citation" data-cites="Bogatskiy_2023_Explainable_equivariant_neural_networks">Bogatskiy, A. et al. (2023)</span>.<a href="#fnref388" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn389" role="doc-endnote"><p><span class="citation" data-cites="Marchetti_2023_Harmonics_of_learning_Universal_fourier_features">Marchetti, Hillar, Kragic, &amp; Sanborn (2023)</span>.<a href="#fnref389" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn390" role="doc-endnote"><p><span class="citation" data-cites="Smith_2019_A_gentle_introduction_to_information_geometry">Smith (2019)</span>.<a href="#fnref390" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn391" role="doc-endnote"><p><span class="citation" data-cites="Nielsen_2020_An_elementary_introduction_to_information">Nielsen (2020)</span>.<a href="#fnref391" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn392" role="doc-endnote"><p><span class="citation" data-cites="Amari_1998_Natural_gradient_works_efficiently_in_learning">Amari (1998)</span>.<a href="#fnref392" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn393" role="doc-endnote"><p><span class="citation" data-cites="Amari_2016_Information_Geometry_and_Its_Applications">Amari (2016)</span>.<a href="#fnref393" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn394" role="doc-endnote"><p><span class="citation" data-cites="Balasubramanian_1996_A_geometric_formulation_of_Occams_razor">Balasubramanian (1996a)</span>.<a href="#fnref394" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn395" role="doc-endnote"><p><span class="citation" data-cites="Balasubramanian_1996_Statistical_inference_Occams_razor">Balasubramanian (1996b)</span>.<a href="#fnref395" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn396" role="doc-endnote"><p><span class="citation" data-cites="Calin_2014_Geometric_Modeling_in_Probability_and_Statistics">Calin &amp; Udriste (2014)</span>.<a href="#fnref396" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn397" role="doc-endnote"><p><span class="citation" data-cites="deCarvalho_2019_On_the_geometry_of_Bayesian_inference">de Carvalho, Page, &amp; Barney (2019)</span>.<a href="#fnref397" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn398" role="doc-endnote"><p><span class="citation" data-cites="Lei_2018_Geometric_understanding_of_deep_learning">Lei, Luo, Yau, &amp; Gu (2018)</span>.<a href="#fnref398" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn399" role="doc-endnote"><p><span class="citation" data-cites="Gao_2020_An_information_geometric_distance_on_the_space">Gao &amp; Chaudhari (2020)</span>.<a href="#fnref399" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn400" role="doc-endnote"><p><span class="citation" data-cites="Bronstein_2021_Geometric_deep_learning_Grids_groups_graphs">Bronstein, Bruna, Cohen, &amp; Velickovic (2021)</span>.<a href="#fnref400" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn401" role="doc-endnote"><p><span class="citation" data-cites="Fefferman_2016_Testing_the_manifold_hypothesis">Fefferman, Mitter, &amp; Narayanan (2016)</span>.<a href="#fnref401" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn402" role="doc-endnote"><p><span class="citation" data-cites="Raissi_2017_Physics_informed_deep_learning_Part_I_Data">Raissi et al. (2017a)</span> and <span class="citation" data-cites="Raissi_2017_Physics_informed_deep_learning_Part_II_Data">Raissi, Perdikaris, &amp; Karniadakis (2017b)</span>.<a href="#fnref402" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn403" role="doc-endnote"><p><span class="citation" data-cites="Karniadakis_2021_Physics_informed_machine_learning">Karniadakis, G.E. et al. (2021)</span>.<a href="#fnref403" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn404" role="doc-endnote"><p><span class="citation" data-cites="Howard_2021_Foundations_of_a_fast_data_driven_machine">Howard, Mandt, Whiteson, &amp; Yang (2021)</span>.<a href="#fnref404" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn405" role="doc-endnote"><p><span class="citation" data-cites="Thuerey_2021_Physics_based_deep_learning">Thuerey, N. et al. (2021)</span>.<a href="#fnref405" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn406" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2015_Approximating_likelihood_ratios_with_calibrated">Cranmer, Pavez, &amp; Louppe (2015)</span>.<a href="#fnref406" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn407" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2019_The_frontier_of_simulation_based_inference">Cranmer, Brehmer, &amp; Louppe (2019)</span>.<a href="#fnref407" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn408" role="doc-endnote"><p><span class="citation" data-cites="Baydin_2019_Etalumis_Bringing_probabilistic_programming">Baydin, A.G. et al. (2019)</span>.<a href="#fnref408" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn409" role="doc-endnote"><p><span class="citation" data-cites="Anderson_2008_The_End_of_Theory_The_data_deluge_makes">Anderson (2008)</span>.<a href="#fnref409" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn410" role="doc-endnote"><p><span class="citation" data-cites="Asch_2018_Big_data_and_extreme_scale_computing_Pathways">Asch, M. et al. (2018)</span>.<a href="#fnref410" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn411" role="doc-endnote"><p><span class="citation" data-cites="DAgnolo_2019_Learning_New_Physics_from_a_Machine">D&#x2019;Agnolo &amp; Wulzer (2019)</span>.<a href="#fnref411" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn412" role="doc-endnote"><p><span class="citation" data-cites="Krenn_2022_On_scientific_understanding_with_artificial">Krenn, M. et al. (2022)</span>.<a href="#fnref412" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn413" role="doc-endnote"><p><span class="citation" data-cites="Udrescu_2020_Symbolic_pregression_Discovering_physical_laws">Udrescu &amp; Tegmark (2020)</span>.<a href="#fnref413" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn414" role="doc-endnote"><p><span class="citation" data-cites="Cranmer_2020_Discovering_symbolic_models_from_deep_learning">Cranmer, M. et al. (2020)</span>.<a href="#fnref414" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn415" role="doc-endnote"><p><span class="citation" data-cites="Liu_2022_AI_Poincare_2_Machine_learning_conservation_laws">Z. Liu, Madhavan, &amp; Tegmark (2022)</span>.<a href="#fnref415" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn416" role="doc-endnote"><p><span class="citation" data-cites="Asch_2018_Big_data_and_extreme_scale_computing_Pathways">Asch, M. et al. (2018)</span>.<a href="#fnref416" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn417" role="doc-endnote"><p><span class="citation" data-cites="Korb_2001_Machine_learning_as_philosophy_of_science">Korb (2001)</span>.<a href="#fnref417" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn418" role="doc-endnote"><p><span class="citation" data-cites="Williamson_2009_The_philosophy_of_science_and_its_relation">Williamson (2009)</span>.<a href="#fnref418" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn419" role="doc-endnote"><p><span class="citation" data-cites="Bensusan_2000_Is_machine_learning_experimental_philosophy">Bensusan (2000)</span>.<a href="#fnref419" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn420" role="doc-endnote"><p><span class="citation" data-cites="Perone_2018_NLP_word_representations_and_the_Wittgenstein">Perone (2018)</span>.<a href="#fnref420" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn421" role="doc-endnote"><p><span class="citation" data-cites="Tenney_2019_What_do_you_learn_from_context">Tenney, I. et al. (2019)</span>.<a href="#fnref421" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn422" role="doc-endnote"><p><span class="citation" data-cites="Nissim_2019_Fair_is_better_than_sensational_Man_is_to_doctor">Nissim, Noord, &amp; Goot (2019)</span>.<a href="#fnref422" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn423" role="doc-endnote"><p><span class="citation" data-cites="Skelac_2020_Meaning_as_use_From_Wittgenstein_to_Googles">Skelac &amp; Jandric (2020)</span>.<a href="#fnref423" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn424" role="doc-endnote"><p><span class="citation" data-cites="Patel_2022_Mapping_language_models_to_grounded_conceptual">Patel &amp; Pavlick (2022)</span>.<a href="#fnref424" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn425" role="doc-endnote"><p><span class="citation" data-cites="Lovering_2022_Unit_testing_for_concepts_in_neural_networks">Lovering &amp; Pavlick (2022)</span>.<a href="#fnref425" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn426" role="doc-endnote"><p><span class="citation" data-cites="Wittgenstein_2009_Philosophical_Investigations">Wittgenstein (2009)</span>, &#xA7;43.<a href="#fnref426" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn427" role="doc-endnote"><p><span class="citation" data-cites="Wittgenstein_2009_Philosophical_Investigations">Wittgenstein (2009)</span>, &#xA7;340.<a href="#fnref427" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn428" role="doc-endnote"><p><span class="citation" data-cites="Piantadosi_2023_Modern_language_models_refute_Chomskys_approach">Piantadosi (2023)</span>, p.&#xA0;15.<a href="#fnref428" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn429" role="doc-endnote"><p><span class="citation" data-cites="Wasserman_2003_All_of_Statistics_A_Concise_Course_in_Statistical">Wasserman (2003)</span>.<a href="#fnref429" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
<li id="fn430" role="doc-endnote"><p><span class="citation" data-cites="Savage_1954_The_Foundations_of_Statistics">Savage (1954)</span>.<a href="#fnref430" class="footnote-back" role="doc-backlink">&#x21A9;&#xFE0E;</a></p></li>
</ol>
</section>

</div> <!-- end pagecontainer -->
</div> <!-- end mainbody -->

<div class="nav">
<ul>
    <li> <a href="./">&#8634;&nbsp;Contents</a> </li>
    <li> <a href="#site_header">&#8613;&nbsp;Top</a> </li>
    <li> <a href="./scientific-method.html">&#8612;&nbsp;Previous</a> </li>
    <li> <a href="./scientific-realism.html">&#8614;&nbsp;Next</a> </li>
</ul>
</div>

<!--
<div id="afterbody">
</div>
-->

<div id="site_footer">
  <div class="signature">
    <p><i>Ryan Reece</i></p>
    <p><a href="https://twitter.com/RyanDavidReece">@RyanDavidReece</a><br/><img class="email" src="img/my-email-alt-blue.png" alt="my email address"/></p>
    <p>Mon May 20, 2024</p>
  </div>
  <div class="license">
    <p>&#xA9; 2014-2024 <a href="http://rreece.github.io/">Ryan Reece</a>. All rights reserved.</p>
    <p>Made with <a href="https://github.com/rreece/markdown-memo">markdown-memo</a>.</p>
  </div>
  <div style="clear:both;"></div>
</div>

<!-- disqus stuff -->
<div id="disqus_stuff">
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'ryans-outline-of-philosophy';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript><p>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="nofollow">comments powered by Disqus.</a></p></noscript>

<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES * * */
    var disqus_shortname = 'ryans-outline-of-philosophy';
    
    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function () {
        var s = document.createElement('script'); s.async = true;
        s.type = 'text/javascript';
        s.src = '//' + disqus_shortname + '.disqus.com/count.js';
        (document.getElementsByTagName('HEAD')[0] || document.getElementsByTagName('BODY')[0]).appendChild(s);
    }());
</script>
</div> <!-- end disqus_stuff -->

</body>
</html>
